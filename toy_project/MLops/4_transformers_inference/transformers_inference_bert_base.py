# -*- coding: utf-8 -*-
"""transformers inference bert base.ipynb

Automatically generated by Colaboratory.
"""

!pip install transformers
!pip install onnxruntime-gpu

# base inference code
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained("bert-base-uncased")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

encoded_input = tokenizer(" ".join(["hello"]*510), return_tensors='pt')
output = model(**encoded_input)

# base inference code performance
import time

start = time.time()
for _ in range(10):
  encoded_input = tokenizer(" ".join(["hello"]*510), return_tensors='pt')
  output = model(**encoded_input)
end = time.time()

print("cpu:", end-start)

# performance with fast tokenizer
from transformers import BertTokenizerFast

fast_tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")

start = time.time()
for _ in range(10):
  encoded_input = fast_tokenizer(" ".join(["hello"]*510), return_tensors='pt')
  output = model(**encoded_input)
end = time.time()

print("cpu + fast tokenizer:", end-start)

# serialization with onnx
! python -m transformers.onnx --model=bert-base-uncased onnx

# model loading with onnx 
import onnxruntime as ort

session = ort.InferenceSession("onnx/model.onnx", providers=['CPUExecutionProvider']) # using cpu

start = time.time()
for _ in range(10):
  encoded_input = fast_tokenizer(" ".join(["hello"]*510), return_tensors='np')
  temp = session.run(None, input_feed=dict(encoded_input))
end = time.time()

print("cpu + fast tokenizer + onnx:", end - start)

# model initializing with gpu
model_gpu = AutoModel.from_pretrained("bert-base-uncased").to("cuda")

# first infernece for initializing GPU
encoded_input = fast_tokenizer(" ".join(["hello"]*510), return_tensors='pt').to("cuda")
output = model_gpu(**encoded_input)

# onnx gpu
session = ort.InferenceSession("onnx/model.onnx", providers=['CUDAExecutionProvider'])

# first inference for initializing gpu
encoded_input = fast_tokenizer(" ".join(["hello"]*510), return_tensors='np')
temp = session.run(None, input_feed=dict(encoded_input))

### max token length ###

# torch model without fast tokenizer
start = time.time()
for _ in range(10):
  encoded_input = tokenizer(" ".join(["hello"]*510), return_tensors='pt').to("cuda")
  output = model_gpu(**encoded_input)
end = time.time()
print("gpu base:", end-start)

# torch model
start = time.time()
for _ in range(10):
  encoded_input = fast_tokenizer(" ".join(["hello"]*510), return_tensors='pt').to("cuda")
  output = model_gpu(**encoded_input)
end = time.time()
print("gpu + fast tokenizer:", end-start)

# onnx
start = time.time()
for _ in range(10):
  encoded_input = fast_tokenizer(" ".join(["hello"]*510), return_tensors='np')
  temp = session.run(None, input_feed=dict(encoded_input))
end = time.time()

print("gpu + fast tokenizer + onnx:", end - start)

### short token length ###

# torch model
start = time.time()
for _ in range(10):
  encoded_input = fast_tokenizer("hello", return_tensors='pt').to("cuda")
  output = model_gpu(**encoded_input)
end = time.time()
print("short token length, gpu + fast tokenizer:", end-start)

# onnx model
start = time.time()
for _ in range(10):
  encoded_input = fast_tokenizer("hello", return_tensors='np')
  temp = session.run(None, input_feed=dict(encoded_input))
end = time.time()

print("short token length, gpu + fast tokenizer + onnx:", end - start)
