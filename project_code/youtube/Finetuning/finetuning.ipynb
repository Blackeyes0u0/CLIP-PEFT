{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom dataset, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Science': 0,\n",
       " 'News': 1,\n",
       " 'Food': 2,\n",
       " 'Blog': 3,\n",
       " 'Tech': 4,\n",
       " 'Informative': 5,\n",
       " 'Comedy,Entertainment': 6,\n",
       " 'Entertainment': 7,\n",
       " 'Automobile': 8,\n",
       " 'Tech,Informative': 9,\n",
       " 'Automobile,Comedy': 10,\n",
       " 'VideoGames': 11,\n",
       " 'Food,Entertainment': 12,\n",
       " 'Blog,Comedy': 13,\n",
       " 'Comedy,Informative': 14,\n",
       " 'Tech,Comedy': 15,\n",
       " 'Comedy': 16,\n",
       " 'Blog,Science': 17,\n",
       " 'Blog,Entertainment': 18,\n",
       " 'Entertainment,Comedy': 19,\n",
       " 'Tech,News': 20,\n",
       " 'Entertainment,Blog': 21}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "label = [\"Science\",\n",
    "        \"News\",\n",
    "        \"Food\",\n",
    "        \"Blog\",\n",
    "        \"Tech\",\n",
    "        \"Informative\",\n",
    "        \"Comedy,Entertainment\",\n",
    "        \"Entertainment\",\n",
    "        \"Automobile\",\n",
    "        \"Tech,Informative\",\n",
    "        \"Automobile,Comedy\",\n",
    "        \"VideoGames\",\n",
    "        \"Food,Entertainment\",\n",
    "        \"Blog,Comedy\",\n",
    "        \"Comedy,Informative\",\n",
    "        \"Tech,Comedy\",\n",
    "        \"Comedy\",\n",
    "        \"Blog,Science\",\n",
    "        \"Blog,Entertainment\",\n",
    "        \"Entertainment,Comedy\",\n",
    "        \"Tech,News\",\n",
    "        \"Entertainment,Blog\"]\n",
    "\n",
    "label_dict = {}\n",
    "\n",
    "for i,name in enumerate(label):\n",
    "    label_dict[name] = i\n",
    "    \n",
    "label_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data set\n",
    "\n",
    "## 목표\n",
    "\n",
    "```python\n",
    "## 목표\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file, names=['file_name', 'label'])\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "0\n",
      "0\n",
      "3d6DsjIBzJ4\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "import torch\n",
    "import clip\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import os\n",
    "\n",
    "class YoutubeDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,\n",
    "                batch_size=16,\n",
    "                valid_size=0.1,\n",
    "                data_dir = '/Users/juniverse/Desktop/pointcloud/VectorUniverse/Data/thumbnails/images/',\n",
    "                transform = None):\n",
    "  \n",
    "        self.transform = transform\n",
    "        # load the dataset\n",
    "        train_dataset = datasets.ImageFolder(\n",
    "            root=data_dir, transform=self.transform,\n",
    "        )\n",
    "\n",
    "        num_train = len(train_dataset) # 2303\n",
    "        \n",
    "        data = pd.read_csv('/Users/juniverse/Desktop/pointcloud/VectorUniverse/Data/Youtube_thumbnails/metadata.csv')\n",
    "        \n",
    "        text_list=[]\n",
    "        id_list = [] \n",
    "        \n",
    "        for i in range(num_train):\n",
    "            img_id_dir =train_dataset.imgs[i][0].split('.jpg')[0].split('/')[-1]\n",
    "            # 3d6DsjIBzJ4\n",
    "            id_list.append(img_id_dir)\n",
    "            x_text = data[data['Id']==img_id_dir]['Title'].values[0]\n",
    "            # 그것에 해당하는 타이틀 아이디.\n",
    "            text_list.append(x_text)\n",
    "\n",
    "        ch2cat = {}\n",
    "        for ch in train_dataset.classes: #channels:\n",
    "            ch2cat[ch] = data[data['Channel']==ch]['Category'].values[0]\n",
    "            # 3Blue1Brown': 'Science',\n",
    "            \n",
    "        id2ch = {}\n",
    "        for k,v in train_dataset.class_to_idx.items():\n",
    "            id2ch[v] = k\n",
    "            # {0: '3Blue1Brown',\n",
    "        \n",
    "        id2cat = {}\n",
    "        for ch,v in train_dataset.class_to_idx.items():\n",
    "            id2cat[v] =  data[data['Channel']==ch]['Category'].values[0]\n",
    "            # {0: 'Science',\n",
    "\n",
    "        id2cat_id = {}\n",
    "        for ch,v in train_dataset.class_to_idx.items():\n",
    "            id2cat_id[v] =  label_dict[data[data['Channel']==ch]['Category'].values[0]]             \n",
    "            # {0: '0',\n",
    "        self.id_list = id_list\n",
    "        self.text_samples = text_list\n",
    "        self.tokenizer = clip.tokenize\n",
    "        self.num_train = num_train\n",
    "        self.train_dataset = train_dataset\n",
    "        self.ch2cat = ch2cat\n",
    "        self.id2ch = id2ch\n",
    "        self.id2cat = id2cat\n",
    "        self.id2cat_id = id2cat_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        # return len(self.text_samples)\n",
    "        return self.num_train\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        image =self.train_dataset[i][0]\n",
    "        text = self.tokenizer(self.text_samples[i])\n",
    "        data_id = self.id_list[i] # id 번호\n",
    "        channel_id = self.train_dataset[i][1] # 채널 번호 index \n",
    "        channel_text = self.id2ch[channel_id] # 채널 이름\n",
    "        category_id = self.id2cat_id[channel_id]\n",
    "        category_text = self.id2cat[channel_id]    # 채널 카테고리\n",
    "        \n",
    "        return image,text,channel_id ,category_id,data_id\n",
    "\n",
    "    \n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225],\n",
    ")\n",
    "\n",
    "# define transforms\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    normalize])\n",
    "\n",
    "dataset = YoutubeDataset(transform=train_transform)#text_list)\n",
    "\n",
    "data = pd.read_csv('/Users/juniverse/Desktop/pointcloud/VectorUniverse/Data/Youtube_thumbnails/metadata.csv')\n",
    "for i in dataset:\n",
    "    print(i[0].shape)\n",
    "    print(i[1].shape)\n",
    "    print(i[2])\n",
    "    print(i[3])\n",
    "    print(i[4])    \n",
    "    x = i[0].permute(1,2,0).cpu().numpy()\n",
    "    plt.imshow(x)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Channel</th>\n",
       "      <th>Category</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3d6DsjIBzJ4</td>\n",
       "      <td>3Blue1Brown</td>\n",
       "      <td>Science</td>\n",
       "      <td>Taylor series | Chapter 11, Essence of calculus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id      Channel Category  \\\n",
       "28  3d6DsjIBzJ4  3Blue1Brown  Science   \n",
       "\n",
       "                                              Title  \n",
       "28  Taylor series | Chapter 11, Essence of calculus  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['Id']==i[4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1842, 461)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the train dataset into train and validation sets\n",
    "train_dataset, valid_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "len(train_dataset),len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 77])\n",
      "69\n",
      "1\n",
      "F2xv4fba65U\n"
     ]
    }
   ],
   "source": [
    "for i in train_dataset:\n",
    "    print(i[0].shape)\n",
    "    print(i[1].shape)\n",
    "    print(i[2])\n",
    "    print(i[3])\n",
    "    print(i[4])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the train dataset into train and validation sets\n",
    "\n",
    "\n",
    "# based on https://gist.github.com/srikarplus/8bdb5bedf0ca25e894e39ea78fce2f39\n",
    "def get_train_valid_loader(train_dataset, valid_dataset, \n",
    "                           batch_size,\n",
    "                           random_seed=2130,\n",
    "                           shuffle=True,\n",
    "                           num_workers=4,\n",
    "                           pin_memory=True):\n",
    "    \"\"\"\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    - data_dir: path directory to the dataset.\n",
    "    - batch_size: how many samples per batch to load.\n",
    "    - augment: whether to apply the data augmentation scheme\n",
    "      mentioned in the paper. Only applied on the train split.\n",
    "    - random_seed: fix seed for reproducibility.\n",
    "    - valid_size: percentage split of the training set used for\n",
    "      the validation set. Should be a float in the range [0, 1].\n",
    "    - shuffle: whether to shuffle the train/validation indices.\n",
    "    - show_sample: plot 9x9 sample grid of the dataset.\n",
    "    - num_workers: number of subprocesses to use when loading the dataset.\n",
    "    - pin_memory: whether to copy tensors into CUDA pinned memory. Set it to\n",
    "      True if using GPU.\n",
    "    Returns\n",
    "    -------\n",
    "    - train_loader: training set iterator.\n",
    "    - valid_loader: validation set iterator.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    if shuffle:\n",
    "      np.random.seed(random_seed) \n",
    "    \n",
    "    train_dataset, valid_dataset       \n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,num_workers=0, collate_fn=None, pin_memory=True)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True,num_workers=0, collate_fn=None, pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "    # return (img_train_loader, img_valid_loader, train_dataset.class_to_idx)\n",
    "    return (train_dataloader, valid_dataloader)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    batch_size = 16\n",
    "    train_loader,valid_loader = get_train_valid_loader(train_dataset, valid_dataset,  batch_size)\n",
    "    # print(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 224, 224])\n",
      "torch.Size([16, 1, 77])\n",
      "tensor([35, 62, 40, 86, 35, 68, 34, 33, 26, 83, 37, 42, 52, 89, 19,  7])\n",
      "tensor([ 5, 11,  0,  8,  5,  4, 14,  1,  4,  5, 15, 11,  2, 13,  0,  2])\n",
      "('9x2NaGkl6BI', 'g8utYlNlb9A', 'tlTKTTt47WE', 'RurHZGQtbmg', 'yS3k_Inm45Q', 'L8tdPYqFV7E', 'MQEEJ57Gsow', 'Honkq3fKFoA', 'cuHDQhDhvPE', 'TPpoJGYlW54', 'M5v1nXiUaOI', 'fbjYkPKRm-8', 'HpRQx020GDE', '1lJhcmuMeBA', 'cPouwOKMS2s', 'u6TFP_r2oA8')\n",
      "torch.Size([16, 3, 224, 224])\n",
      "torch.Size([16, 1, 77])\n",
      "tensor([68,  9, 75, 26, 30, 88, 80, 23, 45, 74, 57, 75, 32, 14, 15, 18])\n",
      "tensor([ 4,  7,  8,  4,  4, 11, 10,  0, 17,  6,  0,  8, 13,  8,  9,  8])\n",
      "('qfS8-Qvvmfk', '3tvegr-O9vI', '1-XqGieWOv8', 'njdJeu95p6s', 'kAEL0zTfbU0', 'OnnxKjvAYLs', 'RKm160Qcnqs', '06w3-l1AzFk', 'yiMlzXMr0KU', '8GxqvnQyaxs', 'NblR01hHK6U', 'k3uW6HTAvRQ', 'SiZ4VkqhGT8', 'sStxk4mv_DY', 'cX1nCBVNRqc', 'A_Aw1auPWe0')\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "train_loader,valid_loader = get_train_valid_loader(train_dataset, valid_dataset,  batch_size)\n",
    "for img,txt,ch,cat,id in train_loader:\n",
    "    print(img.shape)\n",
    "    print(txt.shape)\n",
    "    print(ch)\n",
    "    print(cat)\n",
    "    print(id)\n",
    "    break\n",
    "\n",
    "for img,txt,ch,cat,id in valid_loader:\n",
    "    print(img.shape)\n",
    "    print(txt.shape)\n",
    "    print(ch)\n",
    "    print(cat)\n",
    "    print(id)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 조금더 간단한 dataset을 준비했다.\n",
    "### preprocessed image와 tokenize된 title의 쌍 데이터셋이다.\n",
    "\n",
    "## dataset 준비\n",
    "## image, title list 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['l0IefQRFVfE', 'pYy5-KowMj0', '015wj1C5wK0', 'VnIsuYVlSls', 'C0f2pu0EVSg', '3kJPxsELVAk', 'MGGhz29kYYA', 'gtXpRZUAc1o', 'qNxQRgsF2NA', 'bdUVehx6UBU', 'WTxTBFU7Sdk', 'pF2mNC4EXfA', 'gKZPPYWTtvc', 'dojhP_1W5tA', 'iokVvwcut5o', '6L-BogO6HwU', 'cvHkteWF5GM', 'y1XbPl1XB44', 'BDd-5qCiL-A', 'AfjrpjqdQxI', 'ZAYZWgGxKO4', '0EhcAzhauRM', '5HInaJxFxWs', 'I6w5AN_Wcak', 'oMA7o8olHzY', 'Gy4QHTgikbk', '3xH1vBuDbBA', 'yiMlzXMr0KU', 'RoVnb_TlJ8A', 'A0Lf8eP41vs', 'tj1D9KAYV1E', 'ipRvjS7q1DI', 'RYSvCb14JjU', 'u3Mqvex6tIE', 'dLyt7H7gyhI', 'kEIvicUmegs', 'UNbgIxL3PyM', '52GCI7rqV4A', 'KoihlAl7ugQ', 'Lwq2WfpY-ss', 'YLWjn0TFH3U', 'mTztlpAcips', '1GfuKF4uENc', 'N5SrDhe4bE0', 'X2jakwIVLbY', '_bggaA5AURA', 'NGbFtTYQpus', 'z2qNEOnlxjA', 'uyU-lKE0Go8', 'oAADFdfa-G4', 'Jl0IDXkYKbk', 'P09pTe54uKM', 'AQWeO5wMekQ', 'RyLXLFaspj0', 'mqZS-hEPE6k', '39ii9LvnDoE', '0xPwHR8_D2w', 'REhLHpdLCE0', 'PDkC1eebWbU', 'qpipLfMiaYU', 'Ev3__YJJH3M', 'YNaBoKLX52M', 'JzWtoP1Xhc0', '5pAnk9MKHzo', 'IbgPYS3RnTA', 'Z4BKrSazQh0', 'FWYsvw_U3Zw', 'd25HklopoSs', '4VTOplLl2BM', 'uw-VK2URZZQ', 'JoGjIRVY9t4', 'ugElBfp0j98', '_TM3lnF58Hc', 'NQcwgZbgLj4', '4GihAbGZPwQ', '4H5--Qy3h9Q', 'kcUFSapjVqk', 'UkUskA-stM8', 'Pck5JcnkYYs', '-mgqMpWKEBk', 'OhV3wb_FVEE', 'Yaui2pL26ZQ', 'y6GNrpcXtqM', 'yyRwM0QqZCs', 'kidmmP_HGXU', 'ithpKqYOvd0', 'cciw0MyXMyI', 'Tr1FHd_n2_0', 'pFEB0chiuJA', 'a_TSR_v07m0', 'LEZCxxKp0hM', 'BHiWygziyso', 'MFVXsnq230c', 'hFZFjoX2cGg', 'DPZzrlFCD_I', 'P_6my53IlxY', 'wM5NHC97JBw', '6qZWMNW7GmE', 'Kou7ur5xt_4', '8HEfIJlcFbs', 'T1KRQ3RcvXA', '57MKxz4pJKE', 'VrKW58MS12g', 'vXBfwgwT1nQ', 'vePc5V4h_kg', '-k-V3ESHcfA', 'DTvS9lvRxZ8', 'Q6vVuj3iggE', 'xoxhDk-hwuo', 'My4RA5I0FKs', 'h4T_LlK1VE4', 'lv8wqnk_TsA', 'tk_ZlWJ3qJI', 'S32y9aYEzzo', 'W4DnuQOtA8E', 'ugRc5jx80yg', 'nsnyl8llfH4', 'd_k0f8VGxtw', 'GvzKAnzWTrw', 'lIXl8eSzsFA', '4x04M8YwOBw', 'NvaoIoULjrM', 'Wk-isKUjWBQ', 'Lgy0_uB3sLI', 'p0-ldVORQyE', 'Lv4yoQKJ12s', 'Xz2z1LXbxzE', 'XKH5Gd92k74', 'EwmDdMzzDjY', '_BL1y6v974Y', 'kaQzEvpmDIo', '610rMnK9HCo', 'tP7VdxVY6UQ', 'YlYoAiMKGC4', 'RiTf1SOKa1s', 'I1AeAY8nn_s', '6Lj_gFyI1Qs', 'u3XSfVPFM74', 'rlK7JIAz9WY', 'nhQPBSa2w9k', 'KTeXDOC_fRg', 'nrB3HqVbB3o', 'kNZV3Bry5UU', 'UBPg5ftCMv8', 'h-StyszRCzc', 'O5BJVO3PDeQ', 'GmULc5VANsw', 'P1FUMdHU29c', 'Y7nymZEXjf8', '3aZuj_SDqDo', '2rRIqrWuYy4', 'nzFykQv6Q08', 'fp2EZbbuMa0', 'ROaj3bCpZEM', 'vvXRcwcIJh4', 'QWhUvm8SunY', 'QynNpzqYt0Y', 'kjyeCdd-dl8', 'HsLup7yy-6I', 'Qh9KBwqGxTI', 'HiTqIyx6tBU', 'vZlmUG00OrI', '9XN57BhyZwk', '1ZgxeX2dCnQ', '9NKG9lR34dM', 'YcuofODMCMU', 't95aLo7R1NM', 'SvjwXhCNZcU', 'MQEEJ57Gsow', '_p4h3jwJob0', '-wmfpoq_Y0E', 'vw9zyxm860Q', 'IXgX_r-x-XE', 'mtnVU4BU39E', 'tLExSqNNMPc', 'SkY1DiyCqTU', 'wwH3rA8Evow', 'AHuj4DCueO4', 'j5gy6QLq8wg', 'faJM9iQZMbQ', 'ueTa3dytTuY', '-tBy2jemw4s', 'qKFep4ekvx0', 'fso4yjbgceY', '5VDnj8IEWNc', 'TwPHIABVdg8', 'RuAQo97K-zE', 'l3ifRE7BQiM', 'aVT0BvWZcLg', 'M400lV0GNV4', 'nfpWAqK0YZE', 'HEaaln5jDl8', 'u240yK8_fVM', '_IOZbJ7PCPk', 'tORfGTrr1ww', '3LS83dDqoGU', '0GCuvcTI090', 'U7Q3sKNuZA0', '5VmCvChcGzU', 'icwENiwT1zI', 'yWwfNmIIc_Q', 'fmOP68S5v7c', '8ehoj-y3b1s', 'yQl-RYrbRas', 'HqYlqWvlPTA', 'VR0xCO7BFgo', 'fqHJ05IO-cA', '8Sm99OVG5A8', 'O25-_eEdxaw', 'IVQ3yH-Zusg', 'nbE8B7zggUg', 'jA-DBKQsHp0', 'TfPz27jsFiU', '-dmiS_6YrGU', 'RfRGIhrWWFI', 'IHMFDxLcMYs', 'dtfEzDAlL5k', 'mvmwqx5vjps', 'fihVzPl7Dys', 'IEtPy0o8i0M', 'O9ZlqWp7620', 'j7Q5vP3R-2M', 'noT8kkdl8hE', 'cPouwOKMS2s', 'qXkM0ItZ_X8', 'cnn_5YMpo3Q', '0VgAHq8xXWs', 'XW_qIqLhPkI', 'RK78IKPzeNc', '0Kvw2BPKjz0', 'TWzezWq7FkA', 'Vmb1tqYqyII', '_YTG9vTkhGs', '2h6k6eg_RmM', 'lQYzT49hyKo', '4timuAnUnG4', 'JF5HCUmINE4', 'ugm3hm6nxkI', '32zzZE658ec', 'Y54dfdonqgM', 'GRo32Ug22HY', '8GxqvnQyaxs', 'a7RoP1LKMeM', 'gO8N3L_aERg', 'ST-653teD1U', 'w4jI97DiHF8', 'Qc35afiM2f4', 'bqgVB6Vl-lI', 'lC5lsemxaJo', 'AeZ6a1A0-ow', 'fx9-cMoEv7k', 'Xnk4seEHmgw', 'mfokPqeSNcw', '1Rf9-Ej2xPw', '7iPyz6Yqwl4', 'cLNyF1Zw5tg', 'ClzJkv3dpY8', 'bqAhJcSQQG4', 'scIizw2asro', '6A8W77m-ZTw', 'eMBFhaxUIew', 'jDdKtWnFXFo', 'fVDOB9mby_w', 'PmoHSczX8pU', 'j_pC0-Jfsq0', 'gYgveD5B-V0', 'aztdMwYrF4k', 'XfkzEBmx9nc', 'k70xBg8en-4', 'IiVDtNjORbY', 'EB_XX_IM8io', 'ty4PhRWt1hU', 'LYpKwt4Za_4', 'VNM7Z7hir_I', 'DiaDblUd-lw', 'lWctcNBs2qQ', 'ffVbnPjl86A', '48RFno9AQ9A', 'ZI8QZ2WN-Xo', 'Rd_BRT6_TPk', 'Auh74d_OG8Y', 'l6T63M1XiDg', '2WKkrT4TvBo', 'OpHYkNlsxPs', 'YkiR1KjIw7k', '1Za8BtLgKv8', '6CnuNPwx4pI', 'mgCMRai9K8I', 'w6YLj12s1e4', '02X3ZLUsojU', 'DfgIyD_ea4g', 'IKyaNyYS5Is', '7e_yVm-vcPM', 'OwNr_WCpzRc', 'fuB6-qNwk7k', 'maYuTnci4ys', 'GEjya4OPqNs', 'oQ0a0eAPT7s', 'jEO8CTpEzbg', 'eBQQa65X6F0', '9zMOrX3MM-s', 'yOwZLfjmysY', '94b2pr9Prog', '73ysn2YtTTE', 'BYbSGBi1bBA', 'L-jMOTYWCgU', 'F0LNbWM-Hno', 'zoEvVmU5eC4', 'TqYrvexML-U', 'KU6vIDpNaxw', 'Ei3IuH8IZYI', 'DX1ifYxzr3M', 'UEwagWXt_tc', 'kAEL0zTfbU0', 'OH0fyooKmns', 'n38Qxi7TVWo', 'm-_yImAhNj0', 'uqfw79PTwTw', 'w1DABg8pSMY', 'NkBmPIm3JDk', 'nhXbdSaCddw', 'wVgWeOEw3Ag', '-8tk0uk17gc', 'P9FXWmwvri4', 'E0WT1HtB-Sc', '3TitHNx0l3w', 'L5hksM_R59M', 'S_0geClYYJI', 'HkPatmQFLXU', 'ADb60W8jTsM', 'Y3DEpThhyeY', 'P69xdkqBJno', 'F576tDLa10E', 'kU-M44tWm64', 'Honkq3fKFoA', 'CoDpjqZpAh0', 'CH4TaEFtcvc', 'yPsYOnGPOZk', 'UCq_cDPeY1k', 'l554kV12Wuo', 'gCG52EeOv38', 'j9n5rlgnbjI', 'Zczmz4850wM', 'hxv1-twWRGk', '7box15ryvx8', 'IZpiUSU4VOI', '4uInE0P4Io0', 'gwqEgvSkT_c', 'jpQqr3JQs84', 'AhxpnCW1mp8', '1UFPWzVucSc', 'FljP5JViUOg', 'aMoZbJ39rwg', 'D8aQpYrfIBQ', 'LdZaqnAOthE', 'xy1gghJRcfI', 'mE2mevd7-7A', 'WiUBM8bbDS4', 'It0o9T5OS_4', 'qQHWOA_ikHQ', '7JBCHECtS58', 'IABIU27z7_k', 'g8utYlNlb9A', 'ADsp3UaLhl0', 'glidaMrZrdY', 'Xu1Qo-WFQFo', 'JQ6TUnmaApY', 'EsL5ajBEspE', 'F7Aqz3dAGfU', 'ZDiL_9EYzy0', '4pNGfRvE-vA', '3ArNQp2dZto', 'Cr_-xnWnEbk', 'jXRHb4sCM8c', 'Q9SZlypyK-4', '_CXdl9H28uE', 'qeNhy2pAQVU', 'S0EHWgR6yVU', '8GNInLhL2zA', 'Ie_m_eBevUI', '5W07i-V_MVE', 'sTrrussrG8A', 't-lMIGV-dUI', 'S9VeNB5ili8', 'Ntmhe4EUgpE', 'nX4d2mxwCS8', 'ck-89phIXsM', 'FjtRbz1LE4Y', 'zO9fZaFfNAs', 'AR7bFfOFsno', 'DMu9Bna2PDk', 'qSOVBiEotaw', '7W194GQ6fHI', 'AbzCt9GWZjo', 'r7qliVpGEk0', 'zL3UHF5SlEU', 'Fgkf7VGtZfY', 'QqZBC60vuhM', 'AYA9twQXBq8', 'ToQ8PWYnu04', 'ZRuSS0iiFyo', 'vDBzi0n9Fxg', '8A7woRoVwyM', 'WG-IShOxrLg', 'W2Cv5hZfOmk', 'a9UZBg1pMS4', 'HkB9VJdu27M', 'Gye8ukFO-9s', 'TVfIr_uM5zk', 'xoKwbbnlxi0', 'SUnobHHAKxo', 'eWISIF0k8Mc', '0sr1Xeocuuc', 'dAfdPFWiF9U', '-xfra2_tVF8', 'mwahLAUzf1s', 'VFns39RXPrU', 'CO5E-VoR3sA', '9ugC3TXSKoE', 'Cn1MugPalaA', 'XolBNaavdAQ', 'UA3jI71gy8E', 'kJcpTSNWXdQ', '7mmlmxamw_k', '8kJacGNg_9w', 'cZFhjMQrVts', 'tqqiZdqBNI0', 'Ni5tgO4Bh3Y', 'FozCkl1xj-w', '4-u_I8o0VEk', '2yYsmzeVHaM', 'BvEKky9N1tQ', 'g93tfwqxgiI', '_O_kWL-YhZE', '-9kzxE3PfE8', 'zIOFGCbwJSs', 'rafnUWedZws', 'RrRvFBYfNtU', '59v360lNqgQ', 'przFVvL-gxs', '19JYOVPmwDM', 'Oh2oF-eZTD8', 'lfyRNCFt2jU', '-BRSus9eU10', 'XCuz0pqBKb0', 'l34boCGuYZU', 'nNDtwhUPG18', 'kD80DmquP5M', 'nV0PAyP0laA', '8o-VGnFaStg', 'tw6vPc6j1V8', 't2X3wlvoShg', 'uHYmvUA0Y9c', 'Km_UKeV7Ym4', 'MjiZrDrGCcc', 'UqaNKNgSxYI', 'HgFzP5m1k_c', 'ZYPtgjKJFOs', 'USuh4mYfFYQ', 'Y8RigxxiilI', 'M-VQlqE9M1M', 'Sh3Fl_Rhcw4', 'LqfJJ60JcKo', 'CLUy9DH7V2M', 'pwIhn9Ljeqo', 'xetnJ_gNRBQ', 'fb7T1v_VHpE', 'mUP0tx7Ib2w', 'CbUjuwhQPKs', 'Pqn-OH-jbrs', 'I2O7blSSzpI', '3GyrXqGCszE', '6oY8HIWBS-Y', 'TwNPKb9ol8Y', 'YopcNJcQdEo', 'RQdxHi4_Pvc', 'K4rCzA8fS84', 'AgMcqNnqatw', 'e4RavVnBvTM', 'kd2KEHvK-q8', 'Anfs5idQeJI', 'y-yzL5zJVno', 'ryg077wBvsM', 'G2CuMoe3TWU', 'j2umt3WZU8Q', 'SpeSpA3e56A', 'QiFTTQyKc3w', 'G4xBNFD1p14', 'oKbEuUjbGL0', 'fuFQzAKbrtM', 'Ej5Ap4B9q9I', 'tl-Kmo2UTHE', 'qwI8bALatug', 'H0cL6X0NMk8', 'Os3jAjpzUaU', 'cEdfD7j-LnU', 'IcXI5yzip-s', 'jzRSDvEn3ms', 'HR3KVCUVXkI', 'L90DibXzunI', 'rE5hgHDIbcE', 'a6Vc9XYwdNw', 'G2JWvtGusTw', 'E2Hp6k-PcMo', 'lZmZMISnW7w', 'Pb3SRG1Nvv4', 'NRjKjgdjCXI', 'r5_clNmB3uo', 'NaUnif8Aon0', 'JoadRyVIcTI', 'OoH8G4ZJqOE', 'pACgVf6rSiE', 'PmJbLFAHoe4', '0SnTzvCH6lc', 'J-LqLU7CxeA', 'hC4eog5cY9Q', 'wZsaXxtfuRg', 'GESBI9pWWmI', 'swoAnLQ38NI', '9a0xIzp-fbs', 'et8Cn9q_A-g', 'Mf76yyTY7Ss', 'u8cg5hhHJlA', 'N6cfFnBNELs', 'KH0SQbW-iaY', 'jG8Feqn6Z0E', 'gW1CShFy0NI', 'FnSr820S2Mk', 'yGPfKkjDIts', 'U4nBnuv9n9o', 'FH-_Mtkugbs', 'K8Z9Kqhrh5c', 'J1ugR-VVE1I', 'jr9XRmWNpfw', '9E3FAKBlZh4', 'A9tlDIhpMHo', '_J60fQr0GWo', 'jFNHkjsPqE0', '0ssucYoYtYE', 'IDtHjIe13aU', 'A0MYr3IqZAY', 'rS_qpJvOc7k', 'v_NRSoVncJQ', 'zjzGlXD6bsQ', 'w3jN7kgTVsI', '_DiGQRaaHvg', 'pbRohIxAUS4', 'kLtpcxtk4HI', 'qIxP8OgAg44', 'mJJBMuBVsvw', 'c6N-zitUQ3M', 'V12C6xuMwmE', 'BHdezYIgLDE', 'GqkoykvCV1I', 'ei43IyseG9Y', 'RigmjdQQE6g', 'PcS4rEE8dIU', 'FXIlAed9_xc', '85GSILNh6U4', '111rmlKjx1Q', '4-XDxCb92X4', 'iBy6XRHPODQ', 'rA1Kjvxu28w', 'CmI2dXo14sY', 'rRSOt9SpBMI', 'IsGWMxZhlOY', 'iCd5Po4fPb0', 'whJgPudKgE0', 'uJa584PPm-E', 'PlI3gjkcPz0', 'pJbC7W2fMvQ', 'UT9a7EtxQOM', 'nUSI5uUocho', 'uxhgrd6HWVw', 'LHx63zUMLpM', 'xlPJfeNzBzE', '0U_Mpg59FSw', 'wkPxBltQzzg', '29ZP8ADQJ1Y', '-gal8E_dgE8', 'h5st8mluVUo', 'Bhos_P4a-JQ', '4G6Y6glM_hg', 'F3RGIBExjfE', '39opX6-QJJI', 'AAtoia2BbjE', '48DD3kekP3E', 'eHSvv8eSsvc', 'Uk3-FxTF-BU', 'd7qqu9HC7V0', 'ePN1J_c4ulw', 'GTW8IplsKmM', 'U9DyHthJ6LA', 'fBljahaLMXs', 'enmLybSqmgc', 'lMbseuQR2G8', '8Lvrikv6oPs', 'tYRS0aXaBC8', 'SR5kfWXfxto', '-6BvA4U1dLI', 'gWVHses2GCY', '8Od-IMIGjfg', 'Da8-QfGemgo', 'iS68LmqFVeA', 'kjNU-dbG3ek', 'GDEVOCTW4qk', 'GVLAAMZeraQ', 'JeUFrZtKkn8', 'mmlyHMLeo1o', '_sZH-psg9yE', 'YDr4ITrp7YI', 'sGCKnuKH5dQ', 'FCXfezz4BB8', '3WQHDUYk310', 'CdDXbvBFXLY', 'cnt_rMA1pe0', 'aydFCQiUW44', 'b9bUUHsn-Pc', 'iL4irerdGdU', 'iXbvS1CrqdA', 'zyVZ73CBR7Y', 'LQsjNmkqUOc', 'GnD7IQ48IQE', 'UQGoVB_zMYQ', '8w_kHIAkucA', 'GVTRJa4B4fY', 'iEiiu75dRvs', 'xlvxoNzUxfM', '45eWEO0gRHI', 'nI0BfXFjI1I', 'mvW7fNpEmUE', 'wYALykLb5oY', 'jA14r2ujQ7s', 'XKehq1w4vfg', 'ysEN5RaKOlA', 'FLcRb289uEM', 'FboXxLxg8eo', '9qkzDLos7fk', 'OdkzB13xjL0', 'P3aKRdUyr0s', 'cX1nCBVNRqc', 'pGzssFNtWXw', 's6dMWzZKjTs', 'cJONS7sqi0o', 'kixAljyfdqU', 'JDmKLXVFJzk', 'vNaEBbFbvcY', 'jW55cViXu6s', 'p-MNSLsjjdo', 'tJevBNQsKtU', 'klpDHn8viX8', 'EzZGPCyrpSU', 'NblR01hHK6U', 'msVuCEs8Ydo', 'Ttwl_zH_DZ8', 'K8gV05nS7mc', '94ed4v_T6YM', 'KePNhUJ2reI', '7UwigY4SjKY', 'YycAzdtUIko', 'ztninkgZ0ws', 'dzKWfw68M5U', 'Xo232kyTsO0', 'rckrnYw5sOA', 'gJ5KV3rzuag', '-q7EvLhOK08', 'u4RNGRyzt10', 'Aj6Kc1mvsdo', 'AwwIFcdUFrE', 'tafGL02EUOA', 'mht-1c4wc0Q', '8ORLN_KwAgs', 'gqibxjObZ-I', '5KrRqeZbmrs', 'iI48g7YTZFA', 'Tfb99MZlqKY', 'hhSYpPSnfww', 'rk95JDlI9es', 'va4P3RKW7T0', 'UTHKgVPZ-is', 's9X9qymgshE', 'lex6USTugUU', '_uX9adalziQ', 'Strwla1ELdA', 'sz3aopl4OhI', 'GxLwDHjIp4c', 'V_m7Ik9NZWk', 'LFOUK8OCopM', 'YIafrsAWS1Y', 'TJvHHgGerJg', '15AGWA9bTjo', 'bibPGJBDILQ', 'lz7UgBP-m7w', 'r9cLluDKIhk', 'QRpDIdPFUXg', 'qeAfdT1Ihpg', 'BfzXLjK6Pno', 'tMNxgi_0eKA', 'AGjxX4gvaqk', 'NICir0cWsEY', 'yCdbHiVPYwI', '282lGw6q-SY', 'VXtVrNdD3YA', 'bGHM6DSAgiA', 'ndsaoMFz9J4', '9TjfkXmwbTs', 'FsdWNU--qsU', '1Q4MKOW8FZk', 'dlSw0OicMII', 'qKlr8NlaQcM', 'MujRLvZ61jE', 'aO4dTgt47No', 'GlZtlTon7_I', 'N9K2p54k9GA', '6Sl-1X58ObY', 'YT1NhLTwwEg', 'p2tAqCl5Qig', 'cNsUbtV001E', 's5GfhGFVCFE', 'iOztnsBPrAA', 'YUyuIcy_Lg4', 'Zz8MCVJb0_k', 'BB_4BnQe-qc', 'I4Q3YDezqcM', '60wLvPWXCCc', 'jItnCGRsMjw', 'mNTBC7Qe-R0', 'FDgHVNbCcEU', '66sNL_2ztnw', 'BQw3v31s-Y8', 'TwIvUbOhcKE', '5YBwDNfOaxU', 'y_bbX_Ch1Z8', 'RRkaWQJFkBQ', '15V0gUXUPko', 'MrnCDKB1hE0', 'dcrY59nGxBg', 'Lh6Ob1HFC6k', '-28pKoQdMl8', 'abqMLqHwqpo', '06xFhUHFnx8', 'sI5Ftm1-jik', 'At0advb9_fA', 'im-PLK7ePhQ', 'otjvUz7qKXc', 'xjW-isgOijs', 'jW3_txSfIAQ', 'DZ9wk6aVjT0', 'lT3vGaOLWqE', 'MCW5HUkrr-o', 'dje7uhyW23o', '06w3-l1AzFk', 'CkGVMWK10qU', 'WM25pUsrODk', 'RtlYi1yLTVQ', 'XDf2nhfxVzg', 'g6hQcdjcI9Y', 'DOMs7mYm_zs', 'uuhkVTJ6A_c', 'HoyMs7XXSbw', 'Hy6vddbQa8Q', '9vncG0IP9qU', 'aO3JgPUJ6iQ', 'eLAwG7CjF_k', 'NXCtgI2lkFw', 'Op8KgCcoqn8', '_dSp_f0f9gE', 'eKFrZNXB29M', 'U5EDORs8Jkk', 'S_doxRtYd8o', 'YGgUrj10HdM', 'sWy1qmMoToM', '-NPmBkYb_mY', 'vECmNK0xHNk', 'iLIAMt0wXzQ', 'W8sGTh7ZpoY', 'KbgvSi35n6o', '63qn9w-a2ok', 'PI4EszMsYy4', 'em-pVICrnqM', '5LI2nYhGhYM', 'rsXQInxxzBU', 'Cw7g8ixbomU', 'zhPRtCW5sRk', 'F-xa6Rk5rzg', 'LuM1IXl66B8', 'ou7KSmfC3lA', 'Xmrn2IuSW-Q', 'ZjS6rLKnU0c', 'ZDxLa6P6exc', 'n9dlwebb04Y', 'nLy_jEbuY-U', 'KXZxhmbbHk4', 'dv6UaHZxUys', 'feNbGXiHgdg', 's9omkhY9DpU', 'fbjYkPKRm-8', 'w2gbRe8bOsY', 'YWjTWl0oNLk', 'YkJxWtTPyb0', 'p2q02Bxtqds', 'wm5omDCENPo', 'jFpKVbsI8BY', 'ThItRZqFKX4', '-XTnL4Mang0', 'CaNDeyYP98A', 'H5XWyPgUCm4', 'DPsri_-tIXk', 'ohwbklm3w_E', 'nduMTX86Zl0', '3Awwne_NpJQ', 'puMYeBRTsHs', 'ld6qrjYwi_4', 'rPcYW9dTAUw', 'y0zZqHOZq7M', 'nMUuSGRIk9I', '0cJH8_0M-bE', 'KZHiIVMIbF4', '1gTWfV1QQ-s', 'CgqudioHmTE', 'VRnEpEcWieU', 'oZcKT37EsMs', 'Pv_5VTrHyqY', 'cvLbjQ11GAM', 'SvQ7rmr3mow', 'voIlMkwvybI', 'pv8s9dGQ0l4', 'EudiumXdN7c', 'c7i3rprLMJw', 'JpLJY0s9a2A', 'YMRfZHIGhMg', 'ddzRE69bIZ8', 'cQfNoSHlud0', 'XYFLK8gXMRU', 'p7PwQPdWw4Y', 'YJlOmF0HK9w', '6m3SJMz2LRU', '2JOqNqoaQ70', 'oxH-0n-SpYQ', 'Fte3ey3L-48', 'iaEcZW0eyEg', '3DkOxyBghN8', 'JeswElZRJ_Y', 'H8On7sHjsng', 'k82iqndlfbA', 'ZIQiK1-5zY4', 'PCRx78Zhj7s', 'dzmATptvbFQ', 'uo0jY_lRpp8', 'fZhRZIqJy-c', '623AC6a6org', 'QQmFClkyoPY', 'EmcMG4uxiHk', '5GiBolhyjcQ', 'eMKjfbIPAtM', 's4mGOrbEKbs', 'Ze0kq-ROeaU', 'XNYlEq46giM', 'LOQe63mBxWc', 'EgK0CQdTXSI', 'jEEUmfW0qFQ', 'gDmcL3LBEo4', 'USkHZ5zfcgs', 'dAaw3g08K4A', 'eCxPAdcV1y8', '1lJhcmuMeBA', 'dCGS067s0zo', 'Cyskqnp1j64', '3tyYsfl4DlA', 'ZJy1ajvMU1k', 'Yw-FSUEc8Pc', 'y5gnlPTXGB4', 'x5nEb-7UKZI', 'dQ4ZAZJphx4', '7t6m0k_Ki9g', 'p6ff-ShY5Bw', 'TCR2hJcW11k', 'WK01gDq8d6A', 'jwu2y9x5OlM', '9biIOtEYeHc', 'iXwfBJYCTc4', 'dwt8LetFRSw', 'QqLgmequ7Bk', 'JWdTvyy6-mU', 'If2rE7Sagyw', 'a40VeD8YxiU', '9i4SKHbhbqk', 'v191Y8AUk6w', 'V4gyraWEwKo', 'B1Kaq0_FqGs', '6p5x0nxtqVs', '4NnJ-7Y6qwo', 'MpTodVRR73E', 'thBg1Uo5LIc', '88aDJFdUjH4', '2OSrvzNW9FE', 'OxGsU8oIWjY', 'pTn6Ewhb27k', 'qevIIQHrJZg', 'mXVGIb3bzHI', 'kTXTPe3wahc', 'GcdB5bFwio4', 'xdJwG_9kF8s', '97t7Xj_iBv0', 'vKA4w2O61Xo', 'ZMByI4s-D-Y', 'k5s1cMNTmGs', 't-_VPRCtiUg', 'AeJ9q45PfD0', 'bHIhgxav9LY', 'uxPdPpi5W4o', 'BZbChKzedEk', 'ovJcsL7vyrk', 'TRL7o2kPqw0', 'j5v8D-alAKE', '_k9egfWvb7Y', 'HeQX2HjkcNo', '094y1Z2wpJg', 'AaZ_RSt0KP8', 'GeyDf4ooPdo', '2awbKQ2DLRE', 'wCrtk-pyP0I', '9cNmUNHSBac', '48sCx-wBs34', '4Wrc4fHSCpw', 'n4bucphC9r4', 'ct-CdyT4HkM', '6R8n6FbXVc8', 'V4Uuxg6jmbo', 'Z5FoX3ju2J0', 'A1LUHmUcIsk', 'M6XiFKB7j0w', 'VzHtsoBwBqI', 'bcdJgjNDsto', 'vzSfnduxNDg', 'NHiqsrYkcuk', '-XbX4eQlwgs', '3tvegr-O9vI', '2GZrPR5Ajrs', 'G4Sn91t1V4g', '7fgCfC3bM0U', 'b81Cr97ANrk', 'slmYnJdDQ1U', 'GTXP2FzQS94', 'QtVL76gh09U', 'HuEREfPjT5Y', 'L3MtFGWRXAA', 'RYuipbRGu5s', 'L6EwZWWRA-U', 'pWUzkxtoJ2I', '43uTBB574g4', 'wduZHtRbSkY', 'iN8zmmCRqUA', 'goxvS5pxLDo', '8NLE8d4GW-Y', 'ZLuxpDxhHYA', 'LiEFyZGcK5c', '0viKv1RDhak', 'HLedllETYD4', 'IF59QNrF1VQ', '2Sj2fd6b_Cg', 'ii50SWTJSoA', '9ANWQPD4Ak0', 'nbBHdhFs1q0', 'QgONIEV7Rh0', 'XAtzfJcuHjY', 'PIotJjzDyHM', 'LqUxOfIdxxo', 'fA5Z2e86Hns', '-VRx0GRo-Ws', 'YiM9oosSvNg', 'iVv_xiIx22Q', 'YiEj9mrqTN0', 's4X-CSFTUfo', 'p6OkpHNtvE4', 'fxk3ZTusSqI', 'eoFgUT82pTc', 'Zn0GBm1ltIk', 'ce28TIoa4ao', 'VqP-2ftKIYI', '-9JQndAAvLE', 'MqarV06DDWo', '895AKrvz_AI', 'LOhu1AmuOGo', 'PYzhuFS8YkE', 'lrCwmpjR77U', 'z9rfPuCuDmc', 'A_Aw1auPWe0', 'MnmrEMbDdyA', 'q1UytQTIECk', 'WvSjrHzKd8E', 'WBxqiUhadyY', 'q9ZcPtvTKpY', 'LjZlAzr8MuQ', 'daMJS6dk80o', 'VuafXmP254E', 'b1gIqVsRoaw', 'gKMxgXcnS6g', 'Viz-GXRBj0E', 'P7pUT0gbZZQ', 'kvCHcrzLL8I', '8kef1wXcQYw', 'nNmRDRfnX6U', 'obUnEqsTmLg', 'j1LDtdlDyjU', 'eEvjPwb8y4U', 'EM4odIQZVgw', 'ZWHg5V0JliE', 'H7M74iEonn0', 'WLSNPkf8RCU', '1J2YOLQM2Yc', 'L0H6xYwMQnk', 'sVF_SlzxBJ4', 'eJt_afGN3IQ', 'UQt46gvYO40', 'nVZhV7M3mNE', 'Mwt35SEeR9w', '9x2NaGkl6BI', 'BemHqUqcpI8', 'yS3k_Inm45Q', 'Xfg861hO-Ag', 'sv0iN5J-9mk', 'Kjp3PTeXBew', '7a5gM7t8TZo', 'dmbpagijVkk', '5BS4JXzq50w', '_QWZvU7VCn8', 'kzJLVWnzLqA', 'mfT-UZCA6Tg', 'gBFMGtpfcXw', 'UEob1L_tsn0', 'stRtOHjh6lY', 'GNlQjzje2gw', 'iRngp25a8wM', 'QLFRGj-PPNI', 'oFF134_iokI', 'TUJWx7vo44c', 'iq_6dwYr6YE', '6x2dzepmU8o', 'lIHWgKIXaJU', 'WLex9xvQycQ', 'WGcqICKfi9Q', 'UIL9CiUDHp0', '9gxC3mhjCEU', 'fXUsRIcwtSo', '8vW6qokJ22o', '_aAZOeftU74', 'Y-orMndwuSE', 'icLK1J0n9I0', 'yuXGpUR7fXA', 'QmMHP6JxU4s', '7Q17ubqLfaM', 'iiADhChRriM', 'CVClHLwv-4I', '1PnVor36_40', 'mbsmsi7l3r4', 'p3qvj9hO_Bo', 'R8rmfD9Y5-c', 'fYq5PXgSsbE', 'hQAHSlTtcmY', 'jx5jmI0UlXU', 'In0nB0ABaUk', 'h33Srr5J9nY', 'y17RuWkWdn8', 'iX_QyjdctsQ', 'O6P86uwfdR0', 'V_Kr9OSfDeU', 'EumXak7TyQ0', '-RCnNyD0L-s', 'DvlyzDZDEq4', 'j59qQ7YWLxw', 'cuEtnrL9-H0', 'DUg2SWWK18I', 'g2nMKzhkvxw', 'JnFh2NoAM4s', 'fNcJuPIZ2WE', 'YeFzkC2awTM', 'DHvZLI7Db8E', 'pKcJ-0bAHB4', 'zyC3CWDlEzY', 'RKm160Qcnqs', 'YtTOQSRMRcM', '7NZ9X9A2efA', 'LO0PgyPWE3o', 'dJfSS0ZXYdo', 'Vyau3VUVVtI', 'mkpCzp0CmjY', 'eKBGcQfp1DI', 'LSFX9vrwJf8', 'DrUVMdkb4_k', 'PrqYohBV58o', 'OETj9aTYO2Q', 'aFdBcYN3sNw', 'qqQzU-q8S2o', 'jV51BGIzkwU', 'bf7q8lWEd-o', 'I7g08nwEmyY', '_jPaYnaKVDk', 'xnWKz7Cthkk', 'ROOeGPrC1Do', 'cDoRmT0iRic', 'rKtFJxGzJX8', 'kNTnrpL1Uw0', 'daQcxVqQJsI', 'RXqSedWSu2k', 'QQh56geU0X8', 'JhdRv3KNHXA', 'Cln0J87vulU', 'BiDvLshi9CY', 'EQmjXM4VK2U', 'QLq6GEiHqR8', 'fo2Rb9h788s', 'tsxmyL7TUJg', '8qRZcXIODNU', 'b1reY72ktEc', 'Cqbleas1mmo', 'dHSQAEam2yc', 'W12vb_Crf00', '_uk_6vfqwTA', 'ewLpXw6uN28', '26EivpCPHnQ', 'ATlila3e9dM', 'Dd1JUTA7Ijc', 'I79TpDe3t2g', 'rtYC2jx1LM0', 'BXpu6tbFCsI', 'gzALIXcY4pg', 'AAGIi62-sAU', 'Jl3K63Rbygw', 'mY3SEMTROas', 'OIYy32RuHao', 'sV6uuMAnJUE', 'Mun1dKkc_As', 'zqllxbPWKNI', 'VjINuQX4hbM', 'wcueSXGueJs', 'w08XDXjJhsQ', 'yJS9HPeyu6A', 'kFEka86Lrrc', 'ERUugjLmwuY', '7LjC-nUPrNg', 'hJTy_8Dtbsc', 's6y7zf9Lx-w', 'vgZLPGT5llg', 'LnA7Au-DLUM', 'jIzXsLTf998', 'YfmHi9U9bwM', 'li94gCM6zIs', 'JZ2fRE_nkak', 'oiTmN3NyV0o', 'rRRUrhomU3o', 'fTgm36y884c', 'rutcoSAy2ow', 'dbSL7vmt27Y', 'ey6drwA_xvE', 'KGAj8IhnR3c', 'p28wMbunulQ', 'K8O3bpR4mdQ', '605DPLl8Gqg', 'R-s2mZvskT4', '9j_5wGwn5P8', 'sI7E8J1sGvw', '79_IsTV8X-c', 'GXDvZVYon8M', 'w9nM5WRD-qw', 'I2rNDGh3Rjg', '1vPL9rR0p3Q', 'Cg9JUpalyYc', '2oysDxhHEEk', 'A1S4pwewhyc', 'Q0k7ijqsGpQ', 'K5MrpawcrWQ', 'S-8U4lSEq8A', 'rQt8eGchAXs', 'Z3aNyMLKMG8', 'MGDT0LYJmxc', 'RN8yoi-e2yc', 'V2wgm1jIqho', 'HpRQx020GDE', '2HBmrx__Ock', 'db8e1yYw6to', 'cIR72bmWEEk', 'aDQZUxsrgeI', 'ygrgkc0rYd8', 'YoIMqgxs1mI', 't36jHEyK9C8', 'it7v4ATUEhc', 'L5z-IjLwCH8', 'yJJmKFqqLTI', 'YfBVm1hh5cM', '20-WnF0XwHA', 'I-VmHtzFnBY', 'u8bdtAUpvlA', 'NB4MhB6vIWQ', 'IeVY8u6Y-z4', 'a3Oz3Vz312Y', 'UFpGIi2OjX0', 'rk1fl8RbgyA', 'SorWwznGdCU', 'eQyriT0j36g', 'sgu2mQ65kVc', 'Fokzk5kQR6o', '7Alory4yIyo', 'xKcx7vXZcHc', 'XoXg6DZmJmA', 'HGzlV4ggudM', '4cxWyFlCLhs', 'OK6_wnN0rew', 'jeESbGPl_Dw', 'N10nZr5SRtg', '6VHdJhSVwTU', 'D_2G7grRmoU', 'UckZ5xF1O3s', 'I_1FPzRtLMA', 'HkLibqNtjZk', 'UsOcXG2v3EE', 'vyC7LUZTJQI', 'rGVtTYsv_i8', 'd8uNE7lAQxg', 'FP60G0DbVxc', 'VpNX324owaQ', 'n30DARxk9A4', 'Wxs0hzzTWjo', 'xgHIvDHqUDA', 'wev_gievHmo', 'zOIYAHCeKII', '_9qJ3rwwTDs', 'Ez1GO58inuw', 'l-qNOrlYT7Y', '9tbxDgcv74c', 'PaErPyEnDvk', 'pP44EPBMb8A', 'ulCdoCfw-bY', 'zQGOcOUBi6s', '3mnSDifDSxQ', 'h6fcK_fRYaI', '1-NxodiGPCU', 'UjtOGPJ0URM', '4_aOIA-vyBo', 'oakWgLqCwUc', '7_e0CA_nhaE', 'Da-2h2B4faU', 'BtN-goy9VOY', 'JyECrGp-Sw8', '5iPH-br_eJQ', 'jAhjPd4uNFY', 'uqKGREZs6-w', 'yWO-cvGETRQ', 'JhHMJCUmq28', 'tlTKTTt47WE', 'ZL4yYHdDSWs', 'n3Xv_g3g-mA', 'p_8yK2kmxoo', '1AElONvi9WQ', 'dGiQaabX3_o', '9P6rdqiybaw', 'YI3tsmFsrOg', 'e-P5IFTqB98', 'J5HtSy5bATk', 'oTZ84U-K_5k', 'mr9kK0_7x08', 'vtqtyyGZvXM', 'X1b3C2081-Q', 'kFz9afj8lu0', '_CTUs_2hq6Y', 'x4yF3a3Zn4Y', 't9R7xx0joOU', 'MevKTPN4ozw', 'Sx6dAx7dnXg', 'ZLyDvABxGF0', 'QtMzV73NAgk', 'MrUhzYdcX6w', '5R0_FJ4r73s', 'ZsxQxS0AdBY', 'Oy8zSYKkczI', 'ROkXM3csNWY', 'PTpoj4f25dk', 'l0DoQYGZt8M', 'cH66LWWluVE', 'uBEaG6n0XDs', '0PrUr3bQdwM', 'Gzy_nCkn88U', 'UdfSrJvqY_E', 'tF0dCNxfYHk', 'snWvNkJCCs8', 'k_B3a2cCqo0', 'eS_rEzKdzBA', 'GpMP6Nh3FvU', 'KQMZexRllNs', 'NMDZcbkgpJA', '_OzpODjTISc', 'CwwZFxH1W4c', 'q-1HikE4b54', 'q9XmXad_wmo', 'NCPNTsKBZS0', 'XfcvX0P1b5g', 'fMWnaR5uJxQ', '6PtiY-ev2qA', '_Kav2K1DVWo', '9na8UBoncy4', 'pl20NdwAs3k', '7vQyrlW8ahw', '1R97tphpD_M', 'OA6F6liLnvc', 'HnE60IVm51E', 'DBkONFkdWSM', '_28b0P_6VvY', 'vboQOQwifwg', 'xEeyLDTAkK8', '1VZkiQUzITU', '1ci3l7pnfdk', '-LKVUarhtvE', 'iRYZjOuUnlU', 'QWveXdj6oZU', 'veMFCFyOwFI', 'lxMWSmKieuc', 'BShvYeyMm_Y', 'rFxu7NEoKC8', 'XwNUmnDu1r8', 'a_wuykzfFzE', '3R3cvbLsbAk', 'NAN1kt4SG9E', '9AEMKudv5p0', 'MQyxG4vTyZ8', 'HdsWYOZ8iqM', 'UygPcBCFRrA', 'QMbHLF_zwjs', 'TPpoJGYlW54', 'hLrFyjGZ9NU', 'TVe5XPU10Zc', 'J0KHiiTtt4w', 'kIID5FDi2JQ', 't-osG0F2MZM', 'eXTiiz99p9o', '4oqfodY2Lz0', 'n3IYmdy6d4Y', 'KnMKCHqXLow', 'H2tuKiiznsY', 'luTPMHC7zHY', 'SGNwG_MjslI', '_r9Ndb6N_y0', 'qWAagS_MANg', 'IeM8OReKzfQ', 'Jpd_CUX2o98', 'u6TFP_r2oA8', 'ppi0khS0s_8', 'Ew-3-8itpjc', 'J3xLG-whfFk', 'XY-hOqcPGCY', 'BriBDiBxaMY', 'xmRaWhae5e8', 'LrRzZXuE9qA', 'fuOnYDEIjUA', 'RrkL9e2w7gQ', 'qOsp6IMqnn0', '2J6bJnwr1rw', 'H2nY7Tu1Qgs', 'fEMxFSjmo5Q', 'oidnwPIeqsI', 'TudhYwBrgYg', 'HzUIUTatyZI', '1Gdl-A1DvpA', 'UPS4nxI0b9g', 'VyaxDWLe6A4', 'hc3TEaT3WHA', '4nqJiBRNQuw', 'WrOzwoMKzH4', 'g1GFJxVeH9c', '-pAOuR8s03Q', 'Zb5IH57SorQ', 'I2cS5Fv5xIQ', '5FWfg__wKSY', 'WQJuGeqdbn4', 'eUTtt14G31c', 'kL1aqfnIr2Y', 'BzpIB5TJ7LI', 'CA-eyG7lUv8', 'dOyJqGtP-wU', 'DFtBjc1dz7w', 'Aa-rfmbUG24', 'QDajL441mZc', 'SVaD8rouJn0', 'u843KNE-exo', 'oDqGAUvWKkU', 'fY9UhIxitYM', 'vs3sVrm_W4o', 'AOIi9SjJvgU', 'GeDJAKvcZ9o', 'cdiFhv3gsyo', '6B22Uy7SBe4', 's6mMvBeEPT4', '_qIRtFE6aIc', 'NzJuDo5ots0', 'TtzAcNZYflc', 'Baa_Db8RfxM', 'UFyIMBlFyos', 'KSiZrKvHlVQ', 'Gj5lJWy0TYk', 'elKXLQTOc-s', 'BDPCrGjAKXk', 'giNAjqgCAaA', 'Ook-J58HnvM', 'iEjvVcdCDB4', 'C0UyT5y6DHQ', 'tV8OhAc3Zyc', 'GIdzIBDNt2s', 'UYsaY4ga4aU', 'xkiSfeGtYw4', 'obng-x8oAk4', 'wRccvEsIybA', 'q8f4uxCGWJA', 'MOuV-0f66Y4', 'leDLtQHFzKE', 'JUBSQXmm-3k', 'Rd8PFcqUxA4', 'XpMhtAqOPKA', 'ZxKqoIOiW7s', 'C2EfChhlEYo', 'xosZKMYZ5JY', 'TZt6GDXP2OQ', 'pPyfwf-Vw0A', 'MCMY2kMVPa4', 'AiwOQRhKF1E', '2fAv4kWLp8w', '1zx8-CBtXIs', 'Wzg8hPq9FtQ', 'bkGMkhJ_BwY', 'y-jdvMU1EKo', 'EWOHA0sJEnM', '0lWSYxPJYLo', 'QItug8-BuSc', 'OkU0OECQuKE', 'ci92HvCeR40', 'CFUu71dMpqA', 'qju9ULFiW8E', 'BziZfoUXtJI', '7ZiHjo9sGbk', 'sdkIvUu1eZg', 'ru2rDHzCfNM', 'Za6WWHSWAM4', '9mhOJggoa_Q', 'zASl44yR1M0', 'wZLvLDknpPs', 'L_6SAsypMNM', 'lf8daKt3UlY', 'PV1MlxtpIJs', 'zFiw3_uXyyQ', '0ZgXj-OiT0s', 'z4kW6CUq5F0', '-gZLTLmZBGM', 'm8MQTE5uOoU', 'xiKWUL4iEGQ', 'kbulCM90w8w', 'Lh6ETOmV3eM', '84sHN6_MyMo', 'mRcRtmG0Zy4', 'bmFpNbscp5Q', 'GE2W93FBdo8', 'lfHuTtoKQs4', 'sZ7Jlq9mQrY', 'A4orCB71BgY', 'd3nP2mbjB_M', 'yg3yHYQi02I', 'OYCvbBijvQM', 'mLUTB5TwM4o', 'P1EalbUQfiU', 'IxYg1IDrJUs', '8ye7Vh-_b50', 'fJGxc0x7hzA', 'tbm-izZisbw', 'qLvpLGFacjQ', 'bCu0Z71QRF0', 'l2ojCqhqGZ8', 'm62NJftSS7g', 'cpx4ZdTMmbE', 'RY_rbooeMcI', '8I-6_H0haAw', 'xKg0C6g8UDg', 'rUely4qsxaQ', 'ZH7V2tU3iFc', 'YOwO5iM4QP8', 'RurHZGQtbmg', 'JyfeBHAfpZQ', 'KUqg8XtAHnc', 'PZFLM2DVQHs', 'A4g2pdjxnY8', '_igf9kSG60A', 'Dmk0XJH4aTc', '3kUUgQ0AjZc', 'nFCrqZE69xw', '2TeWWWrLNqI', 'QrdwpKzNge0', 'P4FD3rs3brI', '9vrqKAiRDcM', 'FSs_JYwnAdI', 'gAkwW2tuIqE', 'vqs_0W-MSB0', 'ENrzD9HAZK4', 'ZtyMdRzvi0w', 'nhBVL41-_Cw', 'Mus_vwhTCq0', 'njdJeu95p6s', 'u21W_tfPVrY', 'hdHjjBS4cs8', 'U_gANjtv28g', '3yqDxhR2XxE', 'vn3tm0quoqE', 's9F8pu5KfyM', 'B1t4Fjlomi8', 'ahCwqrYpIuM', 'Sh6lK57Cuk4', 'goy4lZfDtCE', 'cuHDQhDhvPE', 'ifTF3ags0XI', '4duqI8WyfqE', 'Qhaz36TZG5Y', 'ecl-eCbYFPM', '9kRgVxULbag', 'MoOvPtI5M4k', 'PeACT6GHBh8', 'KhQL58CYyRQ', 'b_yCj6fXtlA', 'xw6xB7f2LEs', 'y4mYPMvEbcQ', 'VT128ElBWkM', 'CwF8DYf5dDc', 'INq3d89XxFI', 'Tgp2cZd1sB0', 'MU1_QlN3Z98', 'h-h9cpm1vD8', 'U6Crdz7Uo7c', '8Sn286tPtDE', 'BGIuPAG2VrM', 'Q11jwThxBBI', 'oDYB4ZZLkmY', 'z8aM_P0qT3o', 'iPlFdIpgioI', 'sPUKxtLHPKQ', 'wTziIhu8yvU', 'vrGf4nJWVOU', 'Xh6rc-t47rQ', 'SiZ4VkqhGT8', '5vfZqByi808', 'eRjnPwoxSWU', 'i520aRuZXLo', 'rXAeb7Pmzjs', 'Zx-SY1a_8qk', 'LTikuFFr7JA', 'aIYBZIM10DA', 'DAUOurZIVfI', 'bglWRBB6ZM8', '855Am6ovK7s', 'FRlI2SQ0Ueg', 'Wul0SIpKaIg', 'T9lpyl0ibiY', 'Wutl6OP04jU', 'smkyorC5qwc', '5cathmZFeXs', 'ti2Nokoq1J4', 'zTJa_SwHcTE', 'faI8kacPGbQ', 'kEQACtNXl6g', 'BInk1AoBHrY', 'UCA1A5GqCdQ', 'FPs_lU01KoI', 'AW_Bdf_jGaA', 'prd2RfhF1tM', 'm6pJd6O_NT0', 'kiSm0Nuqomg', '0yqC4BYZWnM', 'LKXvdyNz6L8', '6VixqvOcK8E', 'RwTAgom_VX8', 't17O2AKa2FU', 'DDo4CA13LbY', 'JqhCAORmsaE', '1TieKn0W4o0', '9WSBQne_8FE', 'IVsMSPNVHR0', 'aJukS3HjYWY', 'jsAWnYewsMA', 'kqCNV2Xu6tI', '_W_l79lNtA4', 'r2kXXcla8Bk', 'vo_Rz1xPOXI', '4AwkK8BjjF4', 'lgc7uqWcp-w', 'G3gqFdFXoC0', 'KIv4s6nzvIA', 'FQWX8R1KkY8', '1MBn9uWLJ60', '9yRerlN3XJw', 'IfNQblPxuQk', '65E-U_xd4VQ', 'qGYw1MeNjy8', 'VNRuzCohElk', 'mgKFTvRwicg', 'HA2J71KQV24', 'OVCJ3sOrXeA', 'mKz7quQlx1U', '-ny03ousWJo', 'CiRBQ3frG6U', 'ww_FFS8yjlc', 'cykdSb7xqI4', '65vK_gbsgwI', 'qd9oEhlyIKY', 's36hrId0rOo', 'TvfaGnMNDo0', 'R1Nq92M3kfI', 'QJ4YYocbBwA', '-E_TWPCkqy4', 'JUcIAaZ2LWQ', 'Xtu2WXehZe4', 'exqELqiqSmM', 'XvVCc1Ts0MA', 'OZ_xhkg32nk', 'ElQdUHSXkME', '48oOpnKxXtk', '-cPdImejxEQ', '4GCO9k1FKaE', 'vLaErMfgJ8Q', 'FDng_dnHKFg', 'vebNxWGotC8', 'wmW4K1xkvoQ', 'oHLMla-75QA', 'AdZ9kGwqkTM', '4kEil-H7490', '8eq2vGEEbB4', 'jzYEmIycjtQ', 'UOc7xdRhAAY', 'L0rCE9fE6T4', 'saANxD0cqy0', '6L8yUY-doNc', 'uUU3jW7Y9Ak', 'IrdYueB9pY4', 'ZiWlthrtneU', 'RGw6fXprV9U', 'LL3kVtc-4vY', 'Y0HfmYBlF8g', 'RS7gyZJg5nc', 'FMtayizdFiw', 'v-mWK_kcZMs', 'x1H-323d838', 'j-PrAczOGb0', 'vHuFizITMdA', 'zFZ5jQ0yuNA', 'SIfIZqXaR0g', 'txm7eu8KxVI', 'uMBeXHnWhsE', 'WA-q4pXd0Pk', 'BqLH-nTZEOc', 'n0wvDwSnzcw', '-_qpzFlpgpo', '37Kn-kIsVu8', 'OfI-XpYAqM0', 'oY8tz1paj6o', '2BZGjycR7YM', 'dPIaEWd8zf4', 'araeHtN_3Lk', '_M_Yhlh2PTo', '_RKYQdUUttc', 'SzkHeDPTLOg', 'PGqZkOveLb8', 'KrMvtS7DAlM', '4hCy4-G37jA', 'tXqQs7oMwmg', 'N1uHX_WQsEY', 's_Sf4wfwvzI', 'r-7UBVzjO8Y', '0Xi2rurVAvs', 'FcX0pkXSKl4', 'ObQhpDQzfXE', 'TjP0UgI8CTg', '0SmVHslppDQ', '7Fjn4GRw8qE', 'E9-x3jRSQvQ', 'Y80ZTbjtz5c', '5NX-U36VDis', '6ghOAKzQMGY', 'KM0ZV1X9o7o', 'HtJa9jJdzac', 'G_ZxvxtUwus', '16q8_32M03k', 'Q1UXZs-yWDc', '-naamtpQ8lw', 'VmzQ2hdfQcs', '5pP8TLwO_Ks', 'X3XkG6GL_DM', 'ko9GJKliHpM', 'GajbZY0l7oY', 'gZuDMKXWU_E', 'bs40BtGpuCA', 'EFh6bJIHl2Y', '2iWUUcW08ac', 'IugcIAAZJ2M', 'FBbBWifAdCg', '-8pCMoL_b_s', 'mH7vFc0bUpU', '5356zt0JiDY', 'vLQeb8LWyh0', 'rP86Bby9Y9M', 'ts0d7I6m7GE', 'gis6urgkFoI', 'ywZevdHW5bQ', 'Z0xDAUXPc8c', 'dW7n2UP60bk', 'ogfyJICT9aI', 'GViBkwbyd0Q', 'sxt4YCIsn2I', 'auLmekEsaak', 'i1iqVGORUck', 'RNXSGzTBhXI', 'qjlpBiQjzPg', 'AWkvzycD5PE', 'DRQnZzW_FZU', 'WXbidC6q5-Y', '3TovcF1j3bE', 'OT-qAQLGkGo', 'hqi0114mwtY', 'gShRBsahzXg', 'L8tdPYqFV7E', 'f38sotYHqtA', '8i67R5bnb14', '2LOkI3Xyd_E', 'ooBQIsDTe_M', '_RlxCc0M6Ko', 'AYaRzp--xyk', 'YXQpgAAeLM4', 'qfS8-Qvvmfk', 'lpcpsCY4Mco', 'uxRf7KS3abo', 'kZC12U6EhTc', 'EiKK04Ht8QI', 'xC-c7E5PK0Y', 'X34ZmkeZDos', 'Rgx8dpiPwpA', 'gvAF9mboxFM', 'sW9npZVpiMI', 'pvimAM_SLic', '-UdWguw90g4', 'Pb3AAfz5Yjg', '0KmUoTfGa34', '1fPWr0d5zBE', 'K7s1IYVfvSA', 'hWFDujYzvbI', 'fB7nyxXaczY', 'vT3GUKuAzIs', 'pKO9UjSeLew', 'Hv6EMd8dlQk', '0oBi8OmjLIg', 'TLysAkFM4cA', 'M5v1nXiUaOI', 'Ck0ozfJV9-g', 'uHt01D6rOLI', '4FMdbwDdHlw', 'HTXTVfBCeSY', 'OTfp2_SwxHk', 'ANQcBnQSQOU', 'z1c0w5cYm18', 'lBMAdwBLYgs', 'ugwwFmp4bIA', 'XU8_A4Kb2wo', 'uQJ7Map2qwM', '5ZYw4MIUgk0', 'bqNzbkIHYF8', 'zlGEsR1DQPw', 'xqf7SfJUepE', 'Uil9bh8kJgg', 'jBZRZ9--rUs', 'kD8f0ixy4u4', '2JyUD79ky8s', 'zFk_JjsTMKc', 'ficIQF23rqg', 'eEBXYh_ND-E', 'B6KkwjGecmo', 'BJPc49z57bU', 'z8XF2TByVbI', 'rJmsCDWKCnc', 'Q9BVIwyUr9k', 'zNV2TOF2PC0', 'AtwgGc-An6U', 'OnnxKjvAYLs', 'yl3MiR7XXAM', 'JSOcG2Svf4U', '8qLIEjsBUF4', 'c3wKkZULCHg', 'nxnZe6wjjqU', 'YO4Pm-a7oZ4', 'ViJrqqBo4J0', 'xJZlhECK0Ps', 'vac6mJE8rqo', 'lKdY1OoCZ4U', 'wgzmxJPting', 'AAfwfZr0EHo', 'l6oF3_x4WQs', '18oAkxwYQh0', 'Ops5qBV5GhQ', 'Iv4J3MDhe8M', 'NqVLbDJxfbw', 'lRt56Y9JMEg', 'eeGx5zc90hQ', '9-41nLtleX0', 'lxNUFWs2VBw', 'cIkyO-L3HIw', 'QCI2KYhC8vk', 'QEqVInxyL5w', 'ct8U06Wonhk', 'ug_sryfoPMc', 'vPdkgaHnZuM', 'j7GfAbZ0xWM', 'IWSGFLazkjE', 'qunNYOC-toQ', '0txXeFdn1PM', 'ZOFJ06mLAJA', 'Fefm-ro61Rk', 'AGcgQEzCCrI', 'yD64OsqrZ_g', '9ElPvR3kupM', 'R-NIThUYa_0', '2PFyt2Ku1eg', 'DpXtRbggpQM', 'Px3T14RczDY', 'P_aDQDtcd-8', '803JDTJJ8iQ', '_uBH-oZwceE', '57yCeKge8LI', '3BjaDogb6kc', 'Ec9s-t75lMw', 'tCtal_f8nJI', 'GBUJ5O7zvKA', 'c25eRhYDXa8', 'b6EX-ePnXe8', 'KHSdObeUdxI', 'CEHMfdqwWXA', 'CLUWDLKAF1M', 'Vr6XYTeivPs', '-nSHiHO6QJI', 'ip7gRmrN8TA', '7S0EshM9lM4', 'RmAYo5E52sI', 'EgI_KAkSyCw', 'a2A5Ld-QWnU', 'lA77zsJ31nA', '464puoD09dM', 'gElfIo6uw4g', 'tccTiVeQKA4', 'Q0o3Ci71W6U', '0lrIqGphJx8', 'QP_9rFbvY_Q', 'Sep0Otx4qO4', 'VYLJLxKBhSU', 'udwPo6vLfSU', 'tLcnJEMnlTs', 'EwAkH19MB-I', 'UMdI5JwoI2I', 'TX1sRxCrduA', '3KPYIaks1UY', 'U1wEO-pHizQ', 'b-VCzLiyFxc', '4F84WapAH7M', 'IkWxMG-JDk4', '0e71KWwE5Fk', 'GoVLhUxhdSw', 'yJqsPBWbtjk', 'gQHC7a9JEQQ', 'uPjJ_VAubRs', 'ZqNDkSqL4TA', 'ubTJI_UphPk', 'ByPrDPbdRhc', 'BJP9o4BEziI', 'Zs0qpASAtCA', 'DLjJwW1lFxI', 'cwdbLu_x0gY', 'Qxn2b2LW6bQ', 'BgVJDd5vHfw', 'yIfrEMhtu2o', 'w4xm9NHNUf8', '0ZWGeidvrJw', '6AxCoMbiZfk', 'G5wcCPw5FEg', 'sVEY5AL5zzk', 'a8HoZlhenZ4', '3heSgyr8_cs', '2eoo3XQrHB8', 'ioI0EIUerTQ', 'TGZsBfUQ3J4', 'bP9lPl0LuR0', 'zVdodVA3qTk', 'z1AHfXrbdbg', 'F6dZvCWe94Y', 'IXrLriaYLe8', 'Gb8LNB9Zka4', 'UeUHp1GU644', '57URnhbFDQk', 'rhwY04UAPmk', '6QXt1AhlYqs', 'W9ME-WBlkeM', 'XUtatDjX_i4', 'tIgL4h2S270', 'xMy2O1RVkMI', 'ZVPsNDonA84', 'rJLtT0QXoPo', '4iC9Qi3y9q8', '0w4OTD4L0GQ', '-ybiXR2WCFQ', 'ziUMJB3id8I', 'IvMQe3v-HwA', 'q6yHoSvrTss', '_8Nb_zgWZZ4', 'sp7Ofgpfxoc', 'A4_auMe1HsY', '7y9CLWJvia8', 'P6ODTQKhaXk', 'UOgvbS4GkF0', 'QtplRk6BdyA', 'lN-rVlMIJZs', 'ZE3bT2Z-8eE', '2zE-J1rK1NQ', 'OeHjN4oWVfk', 'pMPomCWUraQ', 'wTXRMBoc4Qs', 'HSOuLJtDRFA', 'QDjci1ODoBs', 'VjzgbZL12VI', 'atCA7JWOi_0', 'a3o_ZKWi-OU', '9EvbqxBUG_c', 'dDagv6SA8nw', 'CCkPhYiKbJU', 'lG2dXobAXLI', 'PUBnqS1qcvk', 'lPwlfbeQOBg', 'DTBu4tigSDo', 'kR3HFEeC3_E', '6rjYXEYpuDo', 'IoaIQSIHuQs', 'GeG3st-2wCE', '7yub7hDEfzI', '2ea1FrrEJD0', 'VkDEUeqQUXY', 'Kd0Fnx0QYhI', 'a1plyHeIVmE', 'KqZCFEThVcA', 'V2UDvKojIjI', 'AeB3kUk5FBI', 'fPcd2egOQnI', 'v5y_D875HMQ', 'H9p4-pzoz-Y', 'thx9hVgrXhQ', 'pZo0r45xxC0', 'vng1eJyoL5w', '6TfI7Z2L7Xw', 'ipJb4mB5bMA', 'm-Ob3aq-wrg', 'IVhoIcvACR4', 'EoDd0hRqs6Y', 'TJM-Qo0VZbs', 'UZzxBbBby1I', '9M8FzipwYtk', 'bYs_zn2pTZo', 'CfmzxyvKZ1w', 'ciW3cGluA10', 'btaEDH775ko', 'QlaeirHJpns', 'kAgTZSJ4jo8', 'mW-C7WE1xOs', 'NEoOJ0uRskY', '0OGZMfVQROY', '8A3jiM2FNR8', '7OeeHz0uNdM', 'v4amCfVbA_c', 'qJVhGXSZO9Y', '3-0-81q-fGo', 'nV5fXPVeZoQ', '2FPrJxTvgdQ', '3QIWolLM9i8', 'mBbOF8LVCj4', 'F2xv4fba65U', 'NzDhm808oU4', 'RskvZgc_s9g', 'LoBJOkhtDQQ', '4DMEekDsN2M', 'YVDJqipoohc', 'QtUgq2Q1ivA', 'sfMUn1U3aSk', 'Y4Zdx97A63s', '1s5iz6ml-qA', 'uN84HhcdIws', 'Ym1NLvmcl0k', '4b-dannQQ0Q', 'COD9hcTpGWQ', 'x2YLS80Nmls', '3GwU4GKzLXY', 'JNvpcGV1frQ', 'e8_6NEnS41A', 'TXdKrtmexWU', 'IweS2CPSnbI', 'Eg08rJGKjtA', 'VQLi2GYVULc', 'cqvVL8IurMw', 'oilZ1hNZPRM', 't7GhemGAZXs', '6DLU4Gih0gY', '43diGN3pR8U', 'NDjNX3nEfYo', 'o1zIgTwENPg', 'F5pgG1M_h_U', 'gxAaO2rsdIs', '3d6DsjIBzJ4', 'XFDM1ip5HdU', 'rB83DpBJQsE', 'M64HUIJFTZM', 'WUvTyaaNkzM', 'HEfHFsfGXjs', 'kYB8IZa5AuE', 'GNcFjFmqEc8', 'r6sGWTCMz2k', 'spUNpyF58BY', 'd-o3eB9sfls', 'fNk_zzaMoSs', 'aircAruvnKk', 'IHZwWFHWa-w', 'zwAD6dRSVyI', 'Ilg3gGewQ5U', 'jsYwFizhncE', 'bBC-nXj3Ng4', 'p_di4Zn4wz4', 'k7RM-ot2NWY', 'EK32jo7i5LQ', 'sD0NjbwqlYw', 'd4EgbgTm0Bg', 'QJYmyhnaaek', 'OkmNXy7er84', 'gB9n2gHsHN4', 'CfW845LNObM', 'PFDu9oVAE-g', 'Kas0tIxDvrg', 'uuSwnbb65f4', 'ssDdqq_9TzI', 'KwmvsHtoLgU', 't_hBKGbf_c0', 'DonD68byO0o', 'eopL9kmMhvw', 'yVjnidcT2ts', 'sM82m4NpdmM', 'bqpRijthHOQ', 'AVaPxM78mDo', 'ZbJjwoXn2O8', 'VMb-su4DZTQ', 'JiEO6F8i0eU', '9mGtJ2p9Zas', '9qLuGDEtcnc', 'asX-pWFvS-o', 'cSsIyt4kbCU', 'j0YdqFPbSpc', '62oDCqb8XWU', 'fsCeZQ4cjGk', 'FGgGLLhbf78', 'SrLZgP-OR6s', 'DDgnb9iAWtY', 'BURAxRI2DQo', '7KqvkMQVQhc', 'XkhMw2KIp_4', '2HbqRMhLmxE', 'XhulR_kJf7Y', 'apg09bolUIc', '5BBhNkywMJY', 'Bbi0kEaTgio', '8Vj9avZxuwI', 'QjuWqBzO7qY', 'AmEEYTtIfMU', 'sStxk4mv_DY', 'n4vusY2-rkQ', 'Bb5VfiFy0kY', 'PNhuDCVIydw', '7VX090Fi63E', 'u6UeJXkzDW8', 'PRSoRkM8GcM', 'u-TNsO33eqQ', 'LfR4QfmwTAc', 'axI5Luw0Eos', '2nYF46P7B2c', 'Z5x22ef7A-w', 's--5ft5YiHg', '3aQRO29ZzbE', 'UEJbKLZ7RmM', 'vUdSUDObwVc', 'w0EmwhiTD74', 'Quvby2Me5CM', 'XUhXLsrZiE0', 'CFXfSBR5Q9w', 'o690DovjDAc', 'QcWELyTbdP4', 'rXHTXdlWw1w', 'iI1o2hNy2hE', 'Pctzq_pT2MU', 'hNWz9o6CSyM', '3dEfc9LL9bQ', 'qXUJSCZiU48', 'OX31kZbAXsA', 'vorkmWa7He8', 'tFmb5h1lN-w', '9-NU7yOSElE', 'eQ_8F4nzyiw', 'VWHlPH23P-w', 'so4FwjfQ7YI', 'M3bezYerYxQ', 'nDTeSb0TzbQ', 'G2VxwFEA8xM', 'Toft6fMvByA', 'gIEGN5rV2JY', '4IFexZ5apzw', 'sCfmM71NOTo', 'Kpnd4OVf7oo', '7iwgyzX-76g', 'wNo7qoLRtkQ', 'Wnptqz_177E', '7CZq5SkIiu0', 'OcUv-TzVWUc', 'oUoO3Ik8aIk', 'M8NFhjj0bUk', 'sZs4NDC4yrI', 'v7MYOpFONCU', '9zNPFcn3pAw', 'GyamlGlrERQ', 'hu5N9OEK-7Q', 'aKQPSZbPKjI', 'EjnG3Tmo4UE', 'lMAkC4vM5aE', 'SXsQeGVk3TA', '1-XqGieWOv8', 'k3uW6HTAvRQ', 'ITVVgoGZMUI', 'oRAuXHcAUiA', 'C07Dfg18Oi4', 'in57Yq2Z-WI', 'zPeEPwQcF1o', 'qIruO8iKZsQ', 'yYHph58TC_4', 'sgWZ316ObRs', 'O-kn9GVOzWc', 'DPthR7Z0um0', 'XRNv2yXqmaU', 'jHbyQ_AQP8c', 'fCn8zs912OE', 'CPBJgpK0Ulc', '7uiv6tKtoKg', 'PEikGKDVsCc', 'JTvcpdfGUtQ', 'Xc4xYacTu-E', 'ixQbCXLUUj8', 'evQsOFQju08', 'sHCHEykUxP4', 's86-Z-CbaHA', 'Dd7dQh8u4Hc', '4fuHzC9aTik', 'VNqNnUJVcVs', 'GDrBIKOR01c', 'R3unPcJDbCc', 'K0-GxoJ_Pcg', 'SrU9YDoXE88', 'L45Q1_psDqk', 'IJhgZBn-LHg', 'cn1kw3_mI70', '-378LN_fUqE', '5WL8Omm7b-4', 'X-Wmy_L5o9w', 'yPr9Xb_oRXk', 'I6Oku6zex-k', 'nb-xbTV-22A', 'rRgQ067tyNg', 'sxpxDdLFSBo', 'nS7xHq3LNfU', 'ppxNZX4MDd8', 'Nb7WLJrW_PU', 'Uhhw6Jw295o', 'SQUNZTdwGoc', 'O9gg0pz7rlc', 'Cr8N3xwvonY', '529h0a2RMt4', 'ygPBddnvhwY', 'S5HHOjOmWag', 'NHQKH-nc8JY', 'TPLnnSHSeLE', '-6Up7CG5d6M', '6Ock5HVWPlk', 'Cvo1sjervSU', 'fVt1fWsM73k', '7XoGwFNUwiM', 'RKpvSSzdRCg', '8pD7MXQfdqU', 'HlBYdiXdUa8', 'HhrbbYD5ScI', 'ntfProwNYkE', 'TLnUJzueBOQ', '5Kod1q39ddE', 'YrpoK7Askck', 'WqBZVlgORbY', '3-jv7doUI8o', '7uG9PGqaWeo', 'ZgyU0LyWZ9M', '-qv7k2_lc0M', 'IkTZ8BVAlFE', 'ztRSm_SJP58', 'ir_OzWzbnC0', '9-GRzu6zbS0', '5pKt4gaErvU', 'nlD9JYP8u5E', 'NQNAdgo91JE', 'vBzPsY3D58k', 'leL_bsHEZdM', 'izh-j8KUYjs', 'bxHEZU0AFcQ', 'j-2ZxldMO-M', 'SxwMTAy1gP0', 'vNIe_UwHPU0', 'UCW_UH-k-ec', 'IHfiMoJUDVQ', '07mBfR8erMY', '__d2FMCtgi4', '0z-oJTZ1b5Q', 'rxhKrtb3XsE', '_uScCrpAW4U', 'fvu5FxKuqdQ', '7pOXunRYJIw', 'kxLoycj4pJY', 'cqidD7kVnxY', '_fTC_Ud_k3U', 'M2w3NZzPwOM', 'K9_b9X2EA3E', 'MFzDaBzBlL0', '_FlV6pgwlrk', 'QJ9lNRAjTQM', 'Q_F9CxSmGOM', 'D0B7F5UbTOQ', 'I8yJkIuvPvM', 'RXXMJAU6vY8', 'QxZ_iPldGtI', 'wplX6t4S7p4', 'JT0wx27J9xs', 'p1PgNbgWSyY', '0i5rycLJ3D8', 'o0fG_lnVhHw', '-53kaP6dZeI', '4I-p8vjQ95s', 'AfaIM7Ybwj4', 'YXy3emGbxHg', 'PYY9VLOSLxk', 'pKmM1N0MoC4', 'k6zVmxp6dOM', 'MGVraepNj04', 'kuYylDsN6KQ', 'QgydTdThoeA', 'nYHDj2sB-rc', 'BQTwvbWAx8A', '5JcZsNxT2BM', 'MmHiZQRaso0', 'aOgBWl_kHYY', 'Yb5D9Q4gZH8', 'el6No1wNKf0', '-A3IlRATIsI', '0zMSMJZc9EA', 'X2bu3UjCxj0', 'hhBQ-sHYEtc', 'jFS9rRTXue8', 'K_NRBkJtNAI', 'DF-yBAkLFfw', 'lfGeJHksykI', '7KwzVus9xds', 'n1hR9rpvS74', 'nV-wPx3fRWE', 'Wnv_JJIIxKk', 'ORmUUhh6O-8', 'rZizVUKe4sQ', '3TflpIllQHY', 'YQDDm9HLkV4', 'lBYC4_Lccjw', 'bs0SWXbty18', 'A-vX1AGBGsc', 's1ax8Tx_Jz0', 'QxGVgXf_LNk', 'C680oxL__ck', '6wsFjjhZPJI', 'bTrV5v7GLcQ', 'erQ_9yEz0ls', 'ha4tRQwKIUg', 'yXWw0_UfSFg', 'NkE0AMGzpJY', '_qAJMXfL6o0', 'ayXxwJJId_c', 'D9lVNzyhYnc', '2eFSU7TFOnk', 'r7zJ8srwwjk', 'Dc2ZRmuH5OM', 'zxYjTTXc-J8', 'F4Y3Pkn95GI', 'IYVjOfoU3uI', '9bqk6ZUsKyA', 'YSoJPA8-oHc', 'GLoeAJUcz38', '0e3GPea1Tyg', 'fMfipiV_17o', 'HBMmK1c44sE']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "img_dir = '/Users/juniverse/Desktop/pointcloud/VectorUniverse/Data/thumbnails/images/'\n",
    "data = pd.read_csv('/Users/juniverse/Desktop/pointcloud/VectorUniverse/Data/Youtube_thumbnails/metadata.csv')\n",
    "# 이미지 파일 목록 생성\n",
    "img_list = []\n",
    "id_list = []\n",
    "for file in os.listdir(img_dir):\n",
    "    ch_dir = os.path.join(img_dir,file)\n",
    "    for images in os.listdir(ch_dir):\n",
    "        # print(images)\n",
    "        if images.endswith('.jpg'):\n",
    "            img_list.append(os.path.join(ch_dir, images))\n",
    "            img_id = images.split('.jpg')[0]\n",
    "            id_list.append(img_id)\n",
    "\n",
    "# 이미지 파일 목록 출력\n",
    "# print(img_list)\n",
    "print(id_list) # 2303 \n",
    "\n",
    "text_list = []\n",
    "cat_list = []\n",
    "ch_list = []\n",
    "\n",
    "for i in id_list:\n",
    "    text_list.append(data[data[\"Id\"]==i]['Title'].values[0])\n",
    "    cat_list.append(data[data[\"Id\"]==i]['Category'].values[0])\n",
    "    ch_list.append(data[data[\"Id\"]==i]['Channel'].values[0])\n",
    "   \n",
    "# 아래 다섯가지로 만들 수 있음. 이미지가 부족해서 이미지를 기준으로 만듬. \n",
    "# img_list = []\n",
    "# id_list = []\n",
    "# text_list = []\n",
    "# cat_list = []\n",
    "# ch_list = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import clip\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# with open(json_path, 'r') as f:\n",
    "#     input_data = []\n",
    "#     for line in f:\n",
    "#         obj = json.loads(line)\n",
    "#         input_data.append(obj)\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "class image_title_dataset(Dataset):\n",
    "    def __init__(self, list_image_path,list_txt):\n",
    "        # Initialize image paths and corresponding texts\n",
    "        self.image_path = list_image_path\n",
    "        # Tokenize text using CLIP's tokenizera\n",
    "        self.title  = clip.tokenize(list_txt)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Preprocess image using CLIP's preprocessing function\n",
    "        image = preprocess(Image.open(self.image_path[idx]))\n",
    "        title = self.title[idx]\n",
    "        return image, title\n",
    "    \n",
    "\n",
    "youtube_dataset = image_title_dataset(img_list,text_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train, valid split\n",
    "\n",
    "이걸 sampler로 만들어서 나누는 동중에 문제가 생겨서 dataset에서 나눠주기로했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1842, 461)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the train dataset into train and validation sets\n",
    "train_dataset, valid_dataset = train_test_split(youtube_dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "len(train_dataset),len(valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 224, 224])\n",
      "torch.Size([16, 77])\n",
      "torch.Size([16, 3, 224, 224])\n",
      "torch.Size([16, 77])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "shuffle = True\n",
    "random_seed = 2130\n",
    "\n",
    "if shuffle:\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,num_workers=0, collate_fn=None, pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True,num_workers=0, collate_fn=None, pin_memory=True)\n",
    "\n",
    "for img,labels in train_dataloader:\n",
    "    print(img.shape)\n",
    "    print(labels.shape)\n",
    "    break\n",
    "\n",
    "for img,labels in valid_dataloader:\n",
    "    print(img.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 아래와 같이 sampler를 이용해서 dataloader 단에서 나눠주는것은 실패했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이방법은 실패\n",
    "# len(train_idx),len(valid_idx)\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     train_dataset,  \n",
    "#     sampler=train_sampler,\n",
    "#     num_workers=4,\n",
    "#     pin_memory =True,\n",
    "#     drop_last =True,\n",
    "#     persistent_workers =True\n",
    "#     )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "num_train = len(train_dataset)          # 50,000\n",
    "indices = list( range(num_train) )  # 0부터 50,000까지의 숫자가 순서대로 나열된 리스트\n",
    "np.random.shuffle(indices)          # 0부터 50,000까지의 숫자가 순서없이 뒤섞인 리스트가 됨\n",
    "split = int( np.floor( 0.2 * num_train ))  # 10,000\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,  \n",
    "    sampler=train_sampler,\n",
    "    num_workers=4,\n",
    "    pin_memory =True,\n",
    "    drop_last =True,\n",
    "    persistent_workers =True\n",
    "    )\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,  \n",
    "    sampler=valid_sampler,\n",
    "    num_workers=2,\n",
    "    pin_memory =True,\n",
    "    drop_last =True,\n",
    "    persistent_workers =True\n",
    "    )\n",
    "\n",
    "for img,txt in train_loader:\n",
    "    print(img.shape)\n",
    "    print(txt.shape)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import clip\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# with open(json_path, 'r') as f:\n",
    "#     input_data = []\n",
    "#     for line in f:\n",
    "#         obj = json.loads(line)\n",
    "#         input_data.append(obj)\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "class image_title_dataset():\n",
    "    def __init__(self, list_image_path,list_txt):\n",
    "\n",
    "            \n",
    "            \n",
    "        # Initialize image paths and corresponding texts\n",
    "        self.image_path = list_image_path\n",
    "        # Tokenize text using CLIP's tokenizer\n",
    "        self.title  = clip.tokenize(list_txt)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Preprocess image using CLIP's preprocessing function\n",
    "        image = preprocess(Image.open(self.image_path[idx]))\n",
    "        title = self.title[idx]\n",
    "        return image, title\n",
    "    \n",
    "\n",
    "# train_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 224, 224])\n",
      "torch.Size([16, 77])\n"
     ]
    }
   ],
   "source": [
    "for img,labels in train_dataloader:\n",
    "    print(img.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out device : cpu\n",
      "Epoch :  0\n",
      "i batch :  0   !!loss : tensor(0.1969, grad_fn=<DivBackward0>)\n",
      "i batch :  1   !!loss : tensor(0.2307, grad_fn=<DivBackward0>)\n",
      "i batch :  2   !!loss : tensor(0.6662, grad_fn=<DivBackward0>)\n",
      "i batch :  3   !!loss : tensor(0.0368, grad_fn=<DivBackward0>)\n",
      "i batch :  4   !!loss : tensor(0.5990, grad_fn=<DivBackward0>)\n",
      "i batch :  5   !!loss : tensor(1.1483, grad_fn=<DivBackward0>)\n",
      "i batch :  6   !!loss : tensor(0.4925, grad_fn=<DivBackward0>)\n",
      "i batch :  7   !!loss : tensor(1.0365, grad_fn=<DivBackward0>)\n",
      "i batch :  8   !!loss : tensor(0.6151, grad_fn=<DivBackward0>)\n",
      "i batch :  9   !!loss : tensor(1.3992, grad_fn=<DivBackward0>)\n",
      "i batch :  10   !!loss : tensor(0.7352, grad_fn=<DivBackward0>)\n",
      "i batch :  11   !!loss : tensor(1.6047, grad_fn=<DivBackward0>)\n",
      "i batch :  12   !!loss : tensor(1.2360, grad_fn=<DivBackward0>)\n",
      "i batch :  13   !!loss : tensor(0.9194, grad_fn=<DivBackward0>)\n",
      "i batch :  14   !!loss : tensor(0.8263, grad_fn=<DivBackward0>)\n",
      "i batch :  15   !!loss : tensor(0.6325, grad_fn=<DivBackward0>)\n",
      "i batch :  16   !!loss : tensor(1.0424, grad_fn=<DivBackward0>)\n",
      "i batch :  17   !!loss : tensor(1.2923, grad_fn=<DivBackward0>)\n",
      "i batch :  18   !!loss : tensor(1.7209, grad_fn=<DivBackward0>)\n",
      "i batch :  19   !!loss : tensor(0.7870, grad_fn=<DivBackward0>)\n",
      "i batch :  20   !!loss : tensor(1.2564, grad_fn=<DivBackward0>)\n",
      "i batch :  21   !!loss : tensor(1.4327, grad_fn=<DivBackward0>)\n",
      "i batch :  22   !!loss : tensor(1.0106, grad_fn=<DivBackward0>)\n",
      "i batch :  23   !!loss : tensor(1.8304, grad_fn=<DivBackward0>)\n",
      "i batch :  24   !!loss : tensor(1.5315, grad_fn=<DivBackward0>)\n",
      "i batch :  25   !!loss : tensor(1.2918, grad_fn=<DivBackward0>)\n",
      "i batch :  26   !!loss : tensor(0.7844, grad_fn=<DivBackward0>)\n",
      "i batch :  27   !!loss : tensor(0.8372, grad_fn=<DivBackward0>)\n",
      "i batch :  28   !!loss : tensor(0.8518, grad_fn=<DivBackward0>)\n",
      "i batch :  29   !!loss : tensor(1.3718, grad_fn=<DivBackward0>)\n",
      "i batch :  30   !!loss : tensor(1.3886, grad_fn=<DivBackward0>)\n",
      "i batch :  31   !!loss : tensor(0.9616, grad_fn=<DivBackward0>)\n",
      "i batch :  32   !!loss : tensor(1.3836, grad_fn=<DivBackward0>)\n",
      "i batch :  33   !!loss : tensor(1.1400, grad_fn=<DivBackward0>)\n",
      "i batch :  34   !!loss : tensor(0.5783, grad_fn=<DivBackward0>)\n",
      "i batch :  35   !!loss : tensor(1.0437, grad_fn=<DivBackward0>)\n",
      "i batch :  36   !!loss : tensor(1.4936, grad_fn=<DivBackward0>)\n",
      "i batch :  37   !!loss : tensor(1.1436, grad_fn=<DivBackward0>)\n",
      "i batch :  38   !!loss : tensor(0.8658, grad_fn=<DivBackward0>)\n",
      "i batch :  39   !!loss : tensor(0.6877, grad_fn=<DivBackward0>)\n",
      "i batch :  40   !!loss : tensor(0.7631, grad_fn=<DivBackward0>)\n",
      "i batch :  41   !!loss : tensor(1.0609, grad_fn=<DivBackward0>)\n",
      "i batch :  42   !!loss : tensor(0.6785, grad_fn=<DivBackward0>)\n",
      "i batch :  43   !!loss : tensor(0.9088, grad_fn=<DivBackward0>)\n",
      "i batch :  44   !!loss : tensor(0.6432, grad_fn=<DivBackward0>)\n",
      "i batch :  45   !!loss : tensor(1.2491, grad_fn=<DivBackward0>)\n",
      "i batch :  46   !!loss : tensor(0.7105, grad_fn=<DivBackward0>)\n",
      "i batch :  47   !!loss : tensor(0.5242, grad_fn=<DivBackward0>)\n",
      "i batch :  48   !!loss : tensor(0.8334, grad_fn=<DivBackward0>)\n",
      "i batch :  49   !!loss : tensor(0.6343, grad_fn=<DivBackward0>)\n",
      "i batch :  50   !!loss : tensor(0.8116, grad_fn=<DivBackward0>)\n",
      "i batch :  51   !!loss : tensor(1.6690, grad_fn=<DivBackward0>)\n",
      "i batch :  52   !!loss : tensor(1.1239, grad_fn=<DivBackward0>)\n",
      "i batch :  53   !!loss : tensor(1.0273, grad_fn=<DivBackward0>)\n",
      "i batch :  54   !!loss : tensor(0.5133, grad_fn=<DivBackward0>)\n",
      "i batch :  55   !!loss : tensor(0.5885, grad_fn=<DivBackward0>)\n",
      "i batch :  56   !!loss : tensor(1.5732, grad_fn=<DivBackward0>)\n",
      "i batch :  57   !!loss : tensor(1.0163, grad_fn=<DivBackward0>)\n",
      "i batch :  58   !!loss : tensor(1.4190, grad_fn=<DivBackward0>)\n",
      "i batch :  59   !!loss : tensor(1.1253, grad_fn=<DivBackward0>)\n",
      "i batch :  60   !!loss : tensor(1.1130, grad_fn=<DivBackward0>)\n",
      "i batch :  61   !!loss : tensor(1.5805, grad_fn=<DivBackward0>)\n",
      "i batch :  62   !!loss : tensor(1.3738, grad_fn=<DivBackward0>)\n",
      "i batch :  63   !!loss : tensor(1.2362, grad_fn=<DivBackward0>)\n",
      "i batch :  64   !!loss : tensor(1.7859, grad_fn=<DivBackward0>)\n",
      "i batch :  65   !!loss : tensor(0.6401, grad_fn=<DivBackward0>)\n",
      "i batch :  66   !!loss : tensor(1.0704, grad_fn=<DivBackward0>)\n",
      "i batch :  67   !!loss : tensor(0.8975, grad_fn=<DivBackward0>)\n",
      "i batch :  68   !!loss : tensor(0.7569, grad_fn=<DivBackward0>)\n",
      "i batch :  69   !!loss : tensor(0.2177, grad_fn=<DivBackward0>)\n",
      "i batch :  70   !!loss : tensor(1.3442, grad_fn=<DivBackward0>)\n",
      "i batch :  71   !!loss : tensor(1.2347, grad_fn=<DivBackward0>)\n",
      "i batch :  72   !!loss : tensor(0.6853, grad_fn=<DivBackward0>)\n",
      "i batch :  73   !!loss : tensor(0.9613, grad_fn=<DivBackward0>)\n",
      "i batch :  74   !!loss : tensor(1.0313, grad_fn=<DivBackward0>)\n",
      "i batch :  75   !!loss : tensor(1.8238, grad_fn=<DivBackward0>)\n",
      "i batch :  76   !!loss : tensor(1.3513, grad_fn=<DivBackward0>)\n",
      "i batch :  77   !!loss : tensor(1.4400, grad_fn=<DivBackward0>)\n",
      "i batch :  78   !!loss : tensor(0.9666, grad_fn=<DivBackward0>)\n",
      "i batch :  79   !!loss : tensor(1.4081, grad_fn=<DivBackward0>)\n",
      "i batch :  80   !!loss : tensor(0.9882, grad_fn=<DivBackward0>)\n",
      "i batch :  81   !!loss : tensor(0.7400, grad_fn=<DivBackward0>)\n",
      "i batch :  82   !!loss : tensor(0.8250, grad_fn=<DivBackward0>)\n",
      "i batch :  83   !!loss : tensor(1.0176, grad_fn=<DivBackward0>)\n",
      "i batch :  84   !!loss : tensor(1.0000, grad_fn=<DivBackward0>)\n",
      "i batch :  85   !!loss : tensor(1.8662, grad_fn=<DivBackward0>)\n",
      "i batch :  86   !!loss : tensor(0.8430, grad_fn=<DivBackward0>)\n",
      "i batch :  87   !!loss : tensor(1.2639, grad_fn=<DivBackward0>)\n",
      "i batch :  88   !!loss : tensor(1.0664, grad_fn=<DivBackward0>)\n",
      "i batch :  89   !!loss : tensor(1.0666, grad_fn=<DivBackward0>)\n",
      "i batch :  90   !!loss : tensor(0.9536, grad_fn=<DivBackward0>)\n",
      "i batch :  91   !!loss : tensor(1.1724, grad_fn=<DivBackward0>)\n",
      "i batch :  92   !!loss : tensor(1.2049, grad_fn=<DivBackward0>)\n",
      "i batch :  93   !!loss : tensor(1.0264, grad_fn=<DivBackward0>)\n",
      "i batch :  94   !!loss : tensor(1.6213, grad_fn=<DivBackward0>)\n",
      "i batch :  95   !!loss : tensor(1.1595, grad_fn=<DivBackward0>)\n",
      "i batch :  96   !!loss : tensor(0.7923, grad_fn=<DivBackward0>)\n",
      "i batch :  97   !!loss : tensor(1.7505, grad_fn=<DivBackward0>)\n",
      "i batch :  98   !!loss : tensor(0.8861, grad_fn=<DivBackward0>)\n",
      "i batch :  99   !!loss : tensor(0.9377, grad_fn=<DivBackward0>)\n",
      "i batch :  100   !!loss : tensor(2.2116, grad_fn=<DivBackward0>)\n",
      "i batch :  101   !!loss : tensor(1.3306, grad_fn=<DivBackward0>)\n",
      "i batch :  102   !!loss : tensor(1.2680, grad_fn=<DivBackward0>)\n",
      "i batch :  103   !!loss : tensor(1.5861, grad_fn=<DivBackward0>)\n",
      "i batch :  104   !!loss : tensor(1.2210, grad_fn=<DivBackward0>)\n",
      "i batch :  105   !!loss : tensor(1.2040, grad_fn=<DivBackward0>)\n",
      "i batch :  106   !!loss : tensor(1.2233, grad_fn=<DivBackward0>)\n",
      "i batch :  107   !!loss : tensor(0.9733, grad_fn=<DivBackward0>)\n",
      "i batch :  108   !!loss : tensor(1.3089, grad_fn=<DivBackward0>)\n",
      "i batch :  109   !!loss : tensor(1.5189, grad_fn=<DivBackward0>)\n",
      "i batch :  110   !!loss : tensor(1.1614, grad_fn=<DivBackward0>)\n",
      "i batch :  111   !!loss : tensor(1.0492, grad_fn=<DivBackward0>)\n",
      "i batch :  112   !!loss : tensor(0.9003, grad_fn=<DivBackward0>)\n",
      "i batch :  113   !!loss : tensor(1.1945, grad_fn=<DivBackward0>)\n",
      "i batch :  114   !!loss : tensor(0.9920, grad_fn=<DivBackward0>)\n",
      "i batch :  115   !!loss : tensor(0.0172, grad_fn=<DivBackward0>)\n",
      "Epoch :  1\n",
      "i batch :  0   !!loss : tensor(0.5206, grad_fn=<DivBackward0>)\n",
      "i batch :  1   !!loss : tensor(0.4177, grad_fn=<DivBackward0>)\n",
      "i batch :  2   !!loss : tensor(0.5346, grad_fn=<DivBackward0>)\n",
      "i batch :  3   !!loss : tensor(0.2834, grad_fn=<DivBackward0>)\n",
      "i batch :  4   !!loss : tensor(0.6490, grad_fn=<DivBackward0>)\n",
      "i batch :  5   !!loss : tensor(0.2430, grad_fn=<DivBackward0>)\n",
      "i batch :  6   !!loss : tensor(0.3967, grad_fn=<DivBackward0>)\n",
      "i batch :  7   !!loss : tensor(0.3215, grad_fn=<DivBackward0>)\n",
      "i batch :  8   !!loss : tensor(0.3623, grad_fn=<DivBackward0>)\n",
      "i batch :  9   !!loss : tensor(0.2762, grad_fn=<DivBackward0>)\n",
      "i batch :  10   !!loss : tensor(0.8764, grad_fn=<DivBackward0>)\n",
      "i batch :  11   !!loss : tensor(0.2414, grad_fn=<DivBackward0>)\n",
      "i batch :  12   !!loss : tensor(0.4108, grad_fn=<DivBackward0>)\n",
      "i batch :  13   !!loss : tensor(0.4725, grad_fn=<DivBackward0>)\n",
      "i batch :  14   !!loss : tensor(0.5913, grad_fn=<DivBackward0>)\n",
      "i batch :  15   !!loss : tensor(0.3683, grad_fn=<DivBackward0>)\n",
      "i batch :  16   !!loss : tensor(0.5929, grad_fn=<DivBackward0>)\n",
      "i batch :  17   !!loss : tensor(0.2707, grad_fn=<DivBackward0>)\n",
      "i batch :  18   !!loss : tensor(0.9583, grad_fn=<DivBackward0>)\n",
      "i batch :  19   !!loss : tensor(0.5366, grad_fn=<DivBackward0>)\n",
      "i batch :  20   !!loss : tensor(0.8135, grad_fn=<DivBackward0>)\n",
      "i batch :  21   !!loss : tensor(0.2750, grad_fn=<DivBackward0>)\n",
      "i batch :  22   !!loss : tensor(0.9196, grad_fn=<DivBackward0>)\n",
      "i batch :  23   !!loss : tensor(0.8994, grad_fn=<DivBackward0>)\n",
      "i batch :  24   !!loss : tensor(0.6917, grad_fn=<DivBackward0>)\n",
      "i batch :  25   !!loss : tensor(0.3351, grad_fn=<DivBackward0>)\n",
      "i batch :  26   !!loss : tensor(0.5781, grad_fn=<DivBackward0>)\n",
      "i batch :  27   !!loss : tensor(1.4856, grad_fn=<DivBackward0>)\n",
      "i batch :  28   !!loss : tensor(0.8124, grad_fn=<DivBackward0>)\n",
      "i batch :  29   !!loss : tensor(0.5219, grad_fn=<DivBackward0>)\n",
      "i batch :  30   !!loss : tensor(0.8746, grad_fn=<DivBackward0>)\n",
      "i batch :  31   !!loss : tensor(1.6806, grad_fn=<DivBackward0>)\n",
      "i batch :  32   !!loss : tensor(0.8287, grad_fn=<DivBackward0>)\n",
      "i batch :  33   !!loss : tensor(0.7119, grad_fn=<DivBackward0>)\n",
      "i batch :  34   !!loss : tensor(1.0363, grad_fn=<DivBackward0>)\n",
      "i batch :  35   !!loss : tensor(1.2578, grad_fn=<DivBackward0>)\n",
      "i batch :  36   !!loss : tensor(0.6471, grad_fn=<DivBackward0>)\n",
      "i batch :  37   !!loss : tensor(0.8021, grad_fn=<DivBackward0>)\n",
      "i batch :  38   !!loss : tensor(0.7099, grad_fn=<DivBackward0>)\n",
      "i batch :  39   !!loss : tensor(0.6597, grad_fn=<DivBackward0>)\n",
      "i batch :  40   !!loss : tensor(1.6860, grad_fn=<DivBackward0>)\n",
      "i batch :  41   !!loss : tensor(0.5454, grad_fn=<DivBackward0>)\n",
      "i batch :  42   !!loss : tensor(1.3699, grad_fn=<DivBackward0>)\n",
      "i batch :  43   !!loss : tensor(1.0185, grad_fn=<DivBackward0>)\n",
      "i batch :  44   !!loss : tensor(0.6442, grad_fn=<DivBackward0>)\n",
      "i batch :  45   !!loss : tensor(1.5004, grad_fn=<DivBackward0>)\n",
      "i batch :  46   !!loss : tensor(0.6554, grad_fn=<DivBackward0>)\n",
      "i batch :  47   !!loss : tensor(0.9292, grad_fn=<DivBackward0>)\n",
      "i batch :  48   !!loss : tensor(0.6211, grad_fn=<DivBackward0>)\n",
      "i batch :  49   !!loss : tensor(1.0774, grad_fn=<DivBackward0>)\n",
      "i batch :  50   !!loss : tensor(1.6993, grad_fn=<DivBackward0>)\n",
      "i batch :  51   !!loss : tensor(1.6070, grad_fn=<DivBackward0>)\n",
      "i batch :  52   !!loss : tensor(1.3697, grad_fn=<DivBackward0>)\n",
      "i batch :  53   !!loss : tensor(1.3246, grad_fn=<DivBackward0>)\n",
      "i batch :  54   !!loss : tensor(0.5446, grad_fn=<DivBackward0>)\n",
      "i batch :  55   !!loss : tensor(1.1926, grad_fn=<DivBackward0>)\n",
      "i batch :  56   !!loss : tensor(1.1296, grad_fn=<DivBackward0>)\n",
      "i batch :  57   !!loss : tensor(1.1218, grad_fn=<DivBackward0>)\n",
      "i batch :  58   !!loss : tensor(0.5208, grad_fn=<DivBackward0>)\n",
      "i batch :  59   !!loss : tensor(0.9594, grad_fn=<DivBackward0>)\n",
      "i batch :  60   !!loss : tensor(1.4560, grad_fn=<DivBackward0>)\n",
      "i batch :  61   !!loss : tensor(0.7282, grad_fn=<DivBackward0>)\n",
      "i batch :  62   !!loss : tensor(1.3271, grad_fn=<DivBackward0>)\n",
      "i batch :  63   !!loss : tensor(0.9589, grad_fn=<DivBackward0>)\n",
      "i batch :  64   !!loss : tensor(1.6028, grad_fn=<DivBackward0>)\n",
      "i batch :  65   !!loss : tensor(1.1313, grad_fn=<DivBackward0>)\n",
      "i batch :  66   !!loss : tensor(1.6167, grad_fn=<DivBackward0>)\n",
      "i batch :  67   !!loss : tensor(1.7890, grad_fn=<DivBackward0>)\n",
      "i batch :  68   !!loss : tensor(1.1529, grad_fn=<DivBackward0>)\n",
      "i batch :  69   !!loss : tensor(1.1070, grad_fn=<DivBackward0>)\n",
      "i batch :  70   !!loss : tensor(1.7886, grad_fn=<DivBackward0>)\n",
      "i batch :  71   !!loss : tensor(1.1432, grad_fn=<DivBackward0>)\n",
      "i batch :  72   !!loss : tensor(1.3873, grad_fn=<DivBackward0>)\n",
      "i batch :  73   !!loss : tensor(1.7829, grad_fn=<DivBackward0>)\n",
      "i batch :  74   !!loss : tensor(1.0105, grad_fn=<DivBackward0>)\n",
      "i batch :  75   !!loss : tensor(1.1756, grad_fn=<DivBackward0>)\n",
      "i batch :  76   !!loss : tensor(1.3693, grad_fn=<DivBackward0>)\n",
      "i batch :  77   !!loss : tensor(1.6227, grad_fn=<DivBackward0>)\n",
      "i batch :  78   !!loss : tensor(1.5037, grad_fn=<DivBackward0>)\n",
      "i batch :  79   !!loss : tensor(0.9629, grad_fn=<DivBackward0>)\n",
      "i batch :  80   !!loss : tensor(1.2224, grad_fn=<DivBackward0>)\n",
      "i batch :  81   !!loss : tensor(2.0531, grad_fn=<DivBackward0>)\n",
      "i batch :  82   !!loss : tensor(1.5954, grad_fn=<DivBackward0>)\n",
      "i batch :  83   !!loss : tensor(1.2116, grad_fn=<DivBackward0>)\n",
      "i batch :  84   !!loss : tensor(1.4277, grad_fn=<DivBackward0>)\n",
      "i batch :  85   !!loss : tensor(1.3426, grad_fn=<DivBackward0>)\n",
      "i batch :  86   !!loss : tensor(1.4449, grad_fn=<DivBackward0>)\n",
      "i batch :  87   !!loss : tensor(1.2073, grad_fn=<DivBackward0>)\n",
      "i batch :  88   !!loss : tensor(1.3318, grad_fn=<DivBackward0>)\n",
      "i batch :  89   !!loss : tensor(1.9124, grad_fn=<DivBackward0>)\n",
      "i batch :  90   !!loss : tensor(1.0625, grad_fn=<DivBackward0>)\n",
      "i batch :  91   !!loss : tensor(1.3320, grad_fn=<DivBackward0>)\n",
      "i batch :  92   !!loss : tensor(1.4799, grad_fn=<DivBackward0>)\n",
      "i batch :  93   !!loss : tensor(1.1418, grad_fn=<DivBackward0>)\n",
      "i batch :  94   !!loss : tensor(1.9894, grad_fn=<DivBackward0>)\n",
      "i batch :  95   !!loss : tensor(1.2658, grad_fn=<DivBackward0>)\n",
      "i batch :  96   !!loss : tensor(1.4396, grad_fn=<DivBackward0>)\n",
      "i batch :  97   !!loss : tensor(1.0713, grad_fn=<DivBackward0>)\n",
      "i batch :  98   !!loss : tensor(1.3691, grad_fn=<DivBackward0>)\n",
      "i batch :  99   !!loss : tensor(1.3794, grad_fn=<DivBackward0>)\n",
      "i batch :  100   !!loss : tensor(1.2491, grad_fn=<DivBackward0>)\n",
      "i batch :  101   !!loss : tensor(1.3652, grad_fn=<DivBackward0>)\n",
      "i batch :  102   !!loss : tensor(1.7709, grad_fn=<DivBackward0>)\n",
      "i batch :  103   !!loss : tensor(0.8577, grad_fn=<DivBackward0>)\n",
      "i batch :  104   !!loss : tensor(1.3299, grad_fn=<DivBackward0>)\n",
      "i batch :  105   !!loss : tensor(1.3449, grad_fn=<DivBackward0>)\n",
      "i batch :  106   !!loss : tensor(1.6947, grad_fn=<DivBackward0>)\n",
      "i batch :  107   !!loss : tensor(1.1082, grad_fn=<DivBackward0>)\n",
      "i batch :  108   !!loss : tensor(1.5457, grad_fn=<DivBackward0>)\n",
      "i batch :  109   !!loss : tensor(1.5702, grad_fn=<DivBackward0>)\n",
      "i batch :  110   !!loss : tensor(1.2928, grad_fn=<DivBackward0>)\n",
      "i batch :  111   !!loss : tensor(1.6442, grad_fn=<DivBackward0>)\n",
      "i batch :  112   !!loss : tensor(1.1814, grad_fn=<DivBackward0>)\n",
      "i batch :  113   !!loss : tensor(2.4327, grad_fn=<DivBackward0>)\n",
      "i batch :  114   !!loss : tensor(1.8952, grad_fn=<DivBackward0>)\n",
      "i batch :  115   !!loss : tensor(0.1073, grad_fn=<DivBackward0>)\n",
      "Epoch :  2\n",
      "i batch :  0   !!loss : tensor(0.7488, grad_fn=<DivBackward0>)\n",
      "i batch :  1   !!loss : tensor(0.8670, grad_fn=<DivBackward0>)\n",
      "i batch :  2   !!loss : tensor(1.5373, grad_fn=<DivBackward0>)\n",
      "i batch :  3   !!loss : tensor(0.9861, grad_fn=<DivBackward0>)\n",
      "i batch :  4   !!loss : tensor(0.6285, grad_fn=<DivBackward0>)\n",
      "i batch :  5   !!loss : tensor(0.7929, grad_fn=<DivBackward0>)\n",
      "i batch :  6   !!loss : tensor(1.0212, grad_fn=<DivBackward0>)\n",
      "i batch :  7   !!loss : tensor(0.8699, grad_fn=<DivBackward0>)\n",
      "i batch :  8   !!loss : tensor(0.5997, grad_fn=<DivBackward0>)\n",
      "i batch :  9   !!loss : tensor(1.0324, grad_fn=<DivBackward0>)\n",
      "i batch :  10   !!loss : tensor(0.8811, grad_fn=<DivBackward0>)\n",
      "i batch :  11   !!loss : tensor(0.7274, grad_fn=<DivBackward0>)\n",
      "i batch :  12   !!loss : tensor(0.8540, grad_fn=<DivBackward0>)\n",
      "i batch :  13   !!loss : tensor(1.0538, grad_fn=<DivBackward0>)\n",
      "i batch :  14   !!loss : tensor(0.7196, grad_fn=<DivBackward0>)\n",
      "i batch :  15   !!loss : tensor(1.1518, grad_fn=<DivBackward0>)\n",
      "i batch :  16   !!loss : tensor(1.4174, grad_fn=<DivBackward0>)\n",
      "i batch :  17   !!loss : tensor(1.4199, grad_fn=<DivBackward0>)\n",
      "i batch :  18   !!loss : tensor(1.1688, grad_fn=<DivBackward0>)\n",
      "i batch :  19   !!loss : tensor(0.8534, grad_fn=<DivBackward0>)\n",
      "i batch :  20   !!loss : tensor(1.4974, grad_fn=<DivBackward0>)\n",
      "i batch :  21   !!loss : tensor(1.6414, grad_fn=<DivBackward0>)\n",
      "i batch :  22   !!loss : tensor(1.6804, grad_fn=<DivBackward0>)\n",
      "i batch :  23   !!loss : tensor(1.6077, grad_fn=<DivBackward0>)\n",
      "i batch :  24   !!loss : tensor(1.5531, grad_fn=<DivBackward0>)\n",
      "i batch :  25   !!loss : tensor(1.6801, grad_fn=<DivBackward0>)\n",
      "i batch :  26   !!loss : tensor(1.2600, grad_fn=<DivBackward0>)\n",
      "i batch :  27   !!loss : tensor(1.1244, grad_fn=<DivBackward0>)\n",
      "i batch :  28   !!loss : tensor(1.4928, grad_fn=<DivBackward0>)\n",
      "i batch :  29   !!loss : tensor(1.6666, grad_fn=<DivBackward0>)\n",
      "i batch :  30   !!loss : tensor(1.2861, grad_fn=<DivBackward0>)\n",
      "i batch :  31   !!loss : tensor(1.3575, grad_fn=<DivBackward0>)\n",
      "i batch :  32   !!loss : tensor(1.2884, grad_fn=<DivBackward0>)\n",
      "i batch :  33   !!loss : tensor(1.6479, grad_fn=<DivBackward0>)\n",
      "i batch :  34   !!loss : tensor(1.3358, grad_fn=<DivBackward0>)\n",
      "i batch :  35   !!loss : tensor(1.3285, grad_fn=<DivBackward0>)\n",
      "i batch :  36   !!loss : tensor(1.3425, grad_fn=<DivBackward0>)\n",
      "i batch :  37   !!loss : tensor(2.0094, grad_fn=<DivBackward0>)\n",
      "i batch :  38   !!loss : tensor(1.0930, grad_fn=<DivBackward0>)\n",
      "i batch :  39   !!loss : tensor(1.3692, grad_fn=<DivBackward0>)\n",
      "i batch :  40   !!loss : tensor(0.7701, grad_fn=<DivBackward0>)\n",
      "i batch :  41   !!loss : tensor(1.5146, grad_fn=<DivBackward0>)\n",
      "i batch :  42   !!loss : tensor(1.4810, grad_fn=<DivBackward0>)\n",
      "i batch :  43   !!loss : tensor(1.2258, grad_fn=<DivBackward0>)\n",
      "i batch :  44   !!loss : tensor(1.8658, grad_fn=<DivBackward0>)\n",
      "i batch :  45   !!loss : tensor(1.3204, grad_fn=<DivBackward0>)\n",
      "i batch :  46   !!loss : tensor(2.7676, grad_fn=<DivBackward0>)\n",
      "i batch :  47   !!loss : tensor(1.3710, grad_fn=<DivBackward0>)\n",
      "i batch :  48   !!loss : tensor(1.9849, grad_fn=<DivBackward0>)\n",
      "i batch :  49   !!loss : tensor(1.7406, grad_fn=<DivBackward0>)\n",
      "i batch :  50   !!loss : tensor(1.4332, grad_fn=<DivBackward0>)\n",
      "i batch :  51   !!loss : tensor(1.5050, grad_fn=<DivBackward0>)\n",
      "i batch :  52   !!loss : tensor(1.2742, grad_fn=<DivBackward0>)\n",
      "i batch :  53   !!loss : tensor(1.7843, grad_fn=<DivBackward0>)\n",
      "i batch :  54   !!loss : tensor(1.5899, grad_fn=<DivBackward0>)\n",
      "i batch :  55   !!loss : tensor(1.8906, grad_fn=<DivBackward0>)\n",
      "i batch :  56   !!loss : tensor(1.4421, grad_fn=<DivBackward0>)\n",
      "i batch :  57   !!loss : tensor(1.6017, grad_fn=<DivBackward0>)\n",
      "i batch :  58   !!loss : tensor(1.7337, grad_fn=<DivBackward0>)\n",
      "i batch :  59   !!loss : tensor(1.1598, grad_fn=<DivBackward0>)\n",
      "i batch :  60   !!loss : tensor(1.4270, grad_fn=<DivBackward0>)\n",
      "i batch :  61   !!loss : tensor(1.2365, grad_fn=<DivBackward0>)\n",
      "i batch :  62   !!loss : tensor(1.8561, grad_fn=<DivBackward0>)\n",
      "i batch :  63   !!loss : tensor(1.4021, grad_fn=<DivBackward0>)\n",
      "i batch :  64   !!loss : tensor(2.4002, grad_fn=<DivBackward0>)\n",
      "i batch :  65   !!loss : tensor(1.1102, grad_fn=<DivBackward0>)\n",
      "i batch :  66   !!loss : tensor(2.4657, grad_fn=<DivBackward0>)\n",
      "i batch :  67   !!loss : tensor(1.7470, grad_fn=<DivBackward0>)\n",
      "i batch :  68   !!loss : tensor(1.4495, grad_fn=<DivBackward0>)\n",
      "i batch :  69   !!loss : tensor(2.3306, grad_fn=<DivBackward0>)\n",
      "i batch :  70   !!loss : tensor(1.6874, grad_fn=<DivBackward0>)\n",
      "i batch :  71   !!loss : tensor(1.8587, grad_fn=<DivBackward0>)\n",
      "i batch :  72   !!loss : tensor(1.8915, grad_fn=<DivBackward0>)\n",
      "i batch :  73   !!loss : tensor(1.6355, grad_fn=<DivBackward0>)\n",
      "i batch :  74   !!loss : tensor(1.8460, grad_fn=<DivBackward0>)\n",
      "i batch :  75   !!loss : tensor(1.7558, grad_fn=<DivBackward0>)\n",
      "i batch :  76   !!loss : tensor(2.0021, grad_fn=<DivBackward0>)\n",
      "i batch :  77   !!loss : tensor(2.0786, grad_fn=<DivBackward0>)\n",
      "i batch :  78   !!loss : tensor(1.7760, grad_fn=<DivBackward0>)\n",
      "i batch :  79   !!loss : tensor(1.7287, grad_fn=<DivBackward0>)\n",
      "i batch :  80   !!loss : tensor(1.4497, grad_fn=<DivBackward0>)\n",
      "i batch :  81   !!loss : tensor(1.5607, grad_fn=<DivBackward0>)\n",
      "i batch :  82   !!loss : tensor(2.1970, grad_fn=<DivBackward0>)\n",
      "i batch :  83   !!loss : tensor(2.4190, grad_fn=<DivBackward0>)\n",
      "i batch :  84   !!loss : tensor(2.4147, grad_fn=<DivBackward0>)\n",
      "i batch :  85   !!loss : tensor(2.0731, grad_fn=<DivBackward0>)\n",
      "i batch :  86   !!loss : tensor(1.3995, grad_fn=<DivBackward0>)\n",
      "i batch :  87   !!loss : tensor(1.6021, grad_fn=<DivBackward0>)\n",
      "i batch :  88   !!loss : tensor(1.6086, grad_fn=<DivBackward0>)\n",
      "i batch :  89   !!loss : tensor(1.6094, grad_fn=<DivBackward0>)\n",
      "i batch :  90   !!loss : tensor(1.6920, grad_fn=<DivBackward0>)\n",
      "i batch :  91   !!loss : tensor(1.5691, grad_fn=<DivBackward0>)\n",
      "i batch :  92   !!loss : tensor(1.6399, grad_fn=<DivBackward0>)\n",
      "i batch :  93   !!loss : tensor(1.9369, grad_fn=<DivBackward0>)\n",
      "i batch :  94   !!loss : tensor(1.8439, grad_fn=<DivBackward0>)\n",
      "i batch :  95   !!loss : tensor(1.7534, grad_fn=<DivBackward0>)\n",
      "i batch :  96   !!loss : tensor(1.5092, grad_fn=<DivBackward0>)\n",
      "i batch :  97   !!loss : tensor(2.2376, grad_fn=<DivBackward0>)\n",
      "i batch :  98   !!loss : tensor(1.3766, grad_fn=<DivBackward0>)\n",
      "i batch :  99   !!loss : tensor(2.1493, grad_fn=<DivBackward0>)\n",
      "i batch :  100   !!loss : tensor(2.1256, grad_fn=<DivBackward0>)\n",
      "i batch :  101   !!loss : tensor(2.3773, grad_fn=<DivBackward0>)\n",
      "i batch :  102   !!loss : tensor(1.7451, grad_fn=<DivBackward0>)\n",
      "i batch :  103   !!loss : tensor(2.1736, grad_fn=<DivBackward0>)\n",
      "i batch :  104   !!loss : tensor(1.8071, grad_fn=<DivBackward0>)\n",
      "i batch :  105   !!loss : tensor(2.3136, grad_fn=<DivBackward0>)\n",
      "i batch :  106   !!loss : tensor(2.3736, grad_fn=<DivBackward0>)\n",
      "i batch :  107   !!loss : tensor(1.7917, grad_fn=<DivBackward0>)\n",
      "i batch :  108   !!loss : tensor(1.8050, grad_fn=<DivBackward0>)\n",
      "i batch :  109   !!loss : tensor(2.2280, grad_fn=<DivBackward0>)\n",
      "i batch :  110   !!loss : tensor(2.2599, grad_fn=<DivBackward0>)\n",
      "i batch :  111   !!loss : tensor(2.0677, grad_fn=<DivBackward0>)\n",
      "i batch :  112   !!loss : tensor(2.1802, grad_fn=<DivBackward0>)\n",
      "i batch :  113   !!loss : tensor(2.5852, grad_fn=<DivBackward0>)\n",
      "i batch :  114   !!loss : tensor(1.6746, grad_fn=<DivBackward0>)\n",
      "i batch :  115   !!loss : tensor(0.1882, grad_fn=<DivBackward0>)\n",
      "Epoch :  3\n",
      "i batch :  0   !!loss : tensor(1.1414, grad_fn=<DivBackward0>)\n",
      "i batch :  1   !!loss : tensor(1.4426, grad_fn=<DivBackward0>)\n",
      "i batch :  2   !!loss : tensor(1.4822, grad_fn=<DivBackward0>)\n",
      "i batch :  3   !!loss : tensor(1.4675, grad_fn=<DivBackward0>)\n",
      "i batch :  4   !!loss : tensor(1.6374, grad_fn=<DivBackward0>)\n",
      "i batch :  5   !!loss : tensor(1.4004, grad_fn=<DivBackward0>)\n",
      "i batch :  6   !!loss : tensor(1.5401, grad_fn=<DivBackward0>)\n",
      "i batch :  7   !!loss : tensor(1.5016, grad_fn=<DivBackward0>)\n",
      "i batch :  8   !!loss : tensor(1.7891, grad_fn=<DivBackward0>)\n",
      "i batch :  9   !!loss : tensor(1.4883, grad_fn=<DivBackward0>)\n",
      "i batch :  10   !!loss : tensor(1.5544, grad_fn=<DivBackward0>)\n",
      "i batch :  11   !!loss : tensor(1.7053, grad_fn=<DivBackward0>)\n",
      "i batch :  12   !!loss : tensor(1.6855, grad_fn=<DivBackward0>)\n",
      "i batch :  13   !!loss : tensor(1.9603, grad_fn=<DivBackward0>)\n",
      "i batch :  14   !!loss : tensor(1.5988, grad_fn=<DivBackward0>)\n",
      "i batch :  15   !!loss : tensor(1.4867, grad_fn=<DivBackward0>)\n",
      "i batch :  16   !!loss : tensor(2.1567, grad_fn=<DivBackward0>)\n",
      "i batch :  17   !!loss : tensor(1.5783, grad_fn=<DivBackward0>)\n",
      "i batch :  18   !!loss : tensor(1.4219, grad_fn=<DivBackward0>)\n",
      "i batch :  19   !!loss : tensor(1.9522, grad_fn=<DivBackward0>)\n",
      "i batch :  20   !!loss : tensor(1.9348, grad_fn=<DivBackward0>)\n",
      "i batch :  21   !!loss : tensor(1.6283, grad_fn=<DivBackward0>)\n",
      "i batch :  22   !!loss : tensor(1.5230, grad_fn=<DivBackward0>)\n",
      "i batch :  23   !!loss : tensor(1.4013, grad_fn=<DivBackward0>)\n",
      "i batch :  24   !!loss : tensor(1.7899, grad_fn=<DivBackward0>)\n",
      "i batch :  25   !!loss : tensor(1.7772, grad_fn=<DivBackward0>)\n",
      "i batch :  26   !!loss : tensor(2.1457, grad_fn=<DivBackward0>)\n",
      "i batch :  27   !!loss : tensor(1.8550, grad_fn=<DivBackward0>)\n",
      "i batch :  28   !!loss : tensor(1.7865, grad_fn=<DivBackward0>)\n",
      "i batch :  29   !!loss : tensor(1.6673, grad_fn=<DivBackward0>)\n",
      "i batch :  30   !!loss : tensor(1.6529, grad_fn=<DivBackward0>)\n",
      "i batch :  31   !!loss : tensor(1.3851, grad_fn=<DivBackward0>)\n",
      "i batch :  32   !!loss : tensor(1.7194, grad_fn=<DivBackward0>)\n",
      "i batch :  33   !!loss : tensor(1.5773, grad_fn=<DivBackward0>)\n",
      "i batch :  34   !!loss : tensor(2.0583, grad_fn=<DivBackward0>)\n",
      "i batch :  35   !!loss : tensor(1.2021, grad_fn=<DivBackward0>)\n",
      "i batch :  36   !!loss : tensor(1.4865, grad_fn=<DivBackward0>)\n",
      "i batch :  37   !!loss : tensor(1.6194, grad_fn=<DivBackward0>)\n",
      "i batch :  38   !!loss : tensor(1.8464, grad_fn=<DivBackward0>)\n",
      "i batch :  39   !!loss : tensor(1.6065, grad_fn=<DivBackward0>)\n",
      "i batch :  40   !!loss : tensor(1.9135, grad_fn=<DivBackward0>)\n",
      "i batch :  41   !!loss : tensor(1.7136, grad_fn=<DivBackward0>)\n",
      "i batch :  42   !!loss : tensor(2.0560, grad_fn=<DivBackward0>)\n",
      "i batch :  43   !!loss : tensor(1.8824, grad_fn=<DivBackward0>)\n",
      "i batch :  44   !!loss : tensor(2.4415, grad_fn=<DivBackward0>)\n",
      "i batch :  45   !!loss : tensor(1.9467, grad_fn=<DivBackward0>)\n",
      "i batch :  46   !!loss : tensor(1.7657, grad_fn=<DivBackward0>)\n",
      "i batch :  47   !!loss : tensor(1.6980, grad_fn=<DivBackward0>)\n",
      "i batch :  48   !!loss : tensor(1.9767, grad_fn=<DivBackward0>)\n",
      "i batch :  49   !!loss : tensor(1.9573, grad_fn=<DivBackward0>)\n",
      "i batch :  50   !!loss : tensor(2.2294, grad_fn=<DivBackward0>)\n",
      "i batch :  51   !!loss : tensor(1.7709, grad_fn=<DivBackward0>)\n",
      "i batch :  52   !!loss : tensor(1.8539, grad_fn=<DivBackward0>)\n",
      "i batch :  53   !!loss : tensor(1.7596, grad_fn=<DivBackward0>)\n",
      "i batch :  54   !!loss : tensor(2.5969, grad_fn=<DivBackward0>)\n",
      "i batch :  55   !!loss : tensor(2.3735, grad_fn=<DivBackward0>)\n",
      "i batch :  56   !!loss : tensor(1.9103, grad_fn=<DivBackward0>)\n",
      "i batch :  57   !!loss : tensor(1.9442, grad_fn=<DivBackward0>)\n",
      "i batch :  58   !!loss : tensor(2.1195, grad_fn=<DivBackward0>)\n",
      "i batch :  59   !!loss : tensor(2.2527, grad_fn=<DivBackward0>)\n",
      "i batch :  60   !!loss : tensor(2.1290, grad_fn=<DivBackward0>)\n",
      "i batch :  61   !!loss : tensor(2.1425, grad_fn=<DivBackward0>)\n",
      "i batch :  62   !!loss : tensor(2.0719, grad_fn=<DivBackward0>)\n",
      "i batch :  63   !!loss : tensor(2.2320, grad_fn=<DivBackward0>)\n",
      "i batch :  64   !!loss : tensor(1.9767, grad_fn=<DivBackward0>)\n",
      "i batch :  65   !!loss : tensor(1.7434, grad_fn=<DivBackward0>)\n",
      "i batch :  66   !!loss : tensor(1.8400, grad_fn=<DivBackward0>)\n",
      "i batch :  67   !!loss : tensor(2.0759, grad_fn=<DivBackward0>)\n",
      "i batch :  68   !!loss : tensor(1.8562, grad_fn=<DivBackward0>)\n",
      "i batch :  69   !!loss : tensor(1.9692, grad_fn=<DivBackward0>)\n",
      "i batch :  70   !!loss : tensor(1.9885, grad_fn=<DivBackward0>)\n",
      "i batch :  71   !!loss : tensor(2.0418, grad_fn=<DivBackward0>)\n",
      "i batch :  72   !!loss : tensor(2.1264, grad_fn=<DivBackward0>)\n",
      "i batch :  73   !!loss : tensor(1.7776, grad_fn=<DivBackward0>)\n",
      "i batch :  74   !!loss : tensor(1.8389, grad_fn=<DivBackward0>)\n",
      "i batch :  75   !!loss : tensor(1.5654, grad_fn=<DivBackward0>)\n",
      "i batch :  76   !!loss : tensor(1.6029, grad_fn=<DivBackward0>)\n",
      "i batch :  77   !!loss : tensor(2.0825, grad_fn=<DivBackward0>)\n",
      "i batch :  78   !!loss : tensor(1.8516, grad_fn=<DivBackward0>)\n",
      "i batch :  79   !!loss : tensor(1.8999, grad_fn=<DivBackward0>)\n",
      "i batch :  80   !!loss : tensor(2.1829, grad_fn=<DivBackward0>)\n",
      "i batch :  81   !!loss : tensor(1.6295, grad_fn=<DivBackward0>)\n",
      "i batch :  82   !!loss : tensor(1.2782, grad_fn=<DivBackward0>)\n",
      "i batch :  83   !!loss : tensor(2.0541, grad_fn=<DivBackward0>)\n",
      "i batch :  84   !!loss : tensor(2.0280, grad_fn=<DivBackward0>)\n",
      "i batch :  85   !!loss : tensor(1.9048, grad_fn=<DivBackward0>)\n",
      "i batch :  86   !!loss : tensor(2.6688, grad_fn=<DivBackward0>)\n",
      "i batch :  87   !!loss : tensor(1.7698, grad_fn=<DivBackward0>)\n",
      "i batch :  88   !!loss : tensor(1.7734, grad_fn=<DivBackward0>)\n",
      "i batch :  89   !!loss : tensor(1.8976, grad_fn=<DivBackward0>)\n",
      "i batch :  90   !!loss : tensor(2.4697, grad_fn=<DivBackward0>)\n",
      "i batch :  91   !!loss : tensor(2.4689, grad_fn=<DivBackward0>)\n",
      "i batch :  92   !!loss : tensor(2.1648, grad_fn=<DivBackward0>)\n",
      "i batch :  93   !!loss : tensor(2.3269, grad_fn=<DivBackward0>)\n",
      "i batch :  94   !!loss : tensor(2.5543, grad_fn=<DivBackward0>)\n",
      "i batch :  95   !!loss : tensor(2.2434, grad_fn=<DivBackward0>)\n",
      "i batch :  96   !!loss : tensor(2.4923, grad_fn=<DivBackward0>)\n",
      "i batch :  97   !!loss : tensor(2.3654, grad_fn=<DivBackward0>)\n",
      "i batch :  98   !!loss : tensor(2.5425, grad_fn=<DivBackward0>)\n",
      "i batch :  99   !!loss : tensor(2.4067, grad_fn=<DivBackward0>)\n",
      "i batch :  100   !!loss : tensor(2.5905, grad_fn=<DivBackward0>)\n",
      "i batch :  101   !!loss : tensor(2.2185, grad_fn=<DivBackward0>)\n",
      "i batch :  102   !!loss : tensor(2.1829, grad_fn=<DivBackward0>)\n",
      "i batch :  103   !!loss : tensor(2.3611, grad_fn=<DivBackward0>)\n",
      "i batch :  104   !!loss : tensor(2.0447, grad_fn=<DivBackward0>)\n",
      "i batch :  105   !!loss : tensor(2.4028, grad_fn=<DivBackward0>)\n",
      "i batch :  106   !!loss : tensor(1.9630, grad_fn=<DivBackward0>)\n",
      "i batch :  107   !!loss : tensor(2.0072, grad_fn=<DivBackward0>)\n",
      "i batch :  108   !!loss : tensor(1.9814, grad_fn=<DivBackward0>)\n",
      "i batch :  109   !!loss : tensor(1.8203, grad_fn=<DivBackward0>)\n",
      "i batch :  110   !!loss : tensor(2.1768, grad_fn=<DivBackward0>)\n",
      "i batch :  111   !!loss : tensor(2.6977, grad_fn=<DivBackward0>)\n",
      "i batch :  112   !!loss : tensor(2.3704, grad_fn=<DivBackward0>)\n",
      "i batch :  113   !!loss : tensor(2.3599, grad_fn=<DivBackward0>)\n",
      "i batch :  114   !!loss : tensor(1.8477, grad_fn=<DivBackward0>)\n",
      "i batch :  115   !!loss : tensor(0.3316, grad_fn=<DivBackward0>)\n",
      "Epoch :  4\n",
      "i batch :  0   !!loss : tensor(2.2048, grad_fn=<DivBackward0>)\n",
      "i batch :  1   !!loss : tensor(2.3895, grad_fn=<DivBackward0>)\n",
      "i batch :  2   !!loss : tensor(1.6989, grad_fn=<DivBackward0>)\n",
      "i batch :  3   !!loss : tensor(2.2645, grad_fn=<DivBackward0>)\n",
      "i batch :  4   !!loss : tensor(1.9650, grad_fn=<DivBackward0>)\n",
      "i batch :  5   !!loss : tensor(1.5310, grad_fn=<DivBackward0>)\n",
      "i batch :  6   !!loss : tensor(1.4333, grad_fn=<DivBackward0>)\n",
      "i batch :  7   !!loss : tensor(1.9027, grad_fn=<DivBackward0>)\n",
      "i batch :  8   !!loss : tensor(1.8645, grad_fn=<DivBackward0>)\n",
      "i batch :  9   !!loss : tensor(2.2977, grad_fn=<DivBackward0>)\n",
      "i batch :  10   !!loss : tensor(1.8812, grad_fn=<DivBackward0>)\n",
      "i batch :  11   !!loss : tensor(2.0080, grad_fn=<DivBackward0>)\n",
      "i batch :  12   !!loss : tensor(1.3592, grad_fn=<DivBackward0>)\n",
      "i batch :  13   !!loss : tensor(2.1560, grad_fn=<DivBackward0>)\n",
      "i batch :  14   !!loss : tensor(1.8566, grad_fn=<DivBackward0>)\n",
      "i batch :  15   !!loss : tensor(2.0035, grad_fn=<DivBackward0>)\n",
      "i batch :  16   !!loss : tensor(2.0450, grad_fn=<DivBackward0>)\n",
      "i batch :  17   !!loss : tensor(1.6052, grad_fn=<DivBackward0>)\n",
      "i batch :  18   !!loss : tensor(2.0499, grad_fn=<DivBackward0>)\n",
      "i batch :  19   !!loss : tensor(1.4142, grad_fn=<DivBackward0>)\n",
      "i batch :  20   !!loss : tensor(1.8308, grad_fn=<DivBackward0>)\n",
      "i batch :  21   !!loss : tensor(1.8482, grad_fn=<DivBackward0>)\n",
      "i batch :  22   !!loss : tensor(1.9282, grad_fn=<DivBackward0>)\n",
      "i batch :  23   !!loss : tensor(1.6730, grad_fn=<DivBackward0>)\n",
      "i batch :  24   !!loss : tensor(1.8150, grad_fn=<DivBackward0>)\n",
      "i batch :  25   !!loss : tensor(1.3608, grad_fn=<DivBackward0>)\n",
      "i batch :  26   !!loss : tensor(1.6437, grad_fn=<DivBackward0>)\n",
      "i batch :  27   !!loss : tensor(1.5631, grad_fn=<DivBackward0>)\n",
      "i batch :  28   !!loss : tensor(2.0462, grad_fn=<DivBackward0>)\n",
      "i batch :  29   !!loss : tensor(2.1982, grad_fn=<DivBackward0>)\n",
      "i batch :  30   !!loss : tensor(2.0248, grad_fn=<DivBackward0>)\n",
      "i batch :  31   !!loss : tensor(1.6209, grad_fn=<DivBackward0>)\n",
      "i batch :  32   !!loss : tensor(2.0126, grad_fn=<DivBackward0>)\n",
      "i batch :  33   !!loss : tensor(1.9678, grad_fn=<DivBackward0>)\n",
      "i batch :  34   !!loss : tensor(2.2688, grad_fn=<DivBackward0>)\n",
      "i batch :  35   !!loss : tensor(2.0883, grad_fn=<DivBackward0>)\n",
      "i batch :  36   !!loss : tensor(1.5675, grad_fn=<DivBackward0>)\n",
      "i batch :  37   !!loss : tensor(1.8753, grad_fn=<DivBackward0>)\n",
      "i batch :  38   !!loss : tensor(2.0461, grad_fn=<DivBackward0>)\n",
      "i batch :  39   !!loss : tensor(1.7233, grad_fn=<DivBackward0>)\n",
      "i batch :  40   !!loss : tensor(2.1126, grad_fn=<DivBackward0>)\n",
      "i batch :  41   !!loss : tensor(2.1283, grad_fn=<DivBackward0>)\n",
      "i batch :  42   !!loss : tensor(2.0599, grad_fn=<DivBackward0>)\n",
      "i batch :  43   !!loss : tensor(1.9064, grad_fn=<DivBackward0>)\n",
      "i batch :  44   !!loss : tensor(2.1288, grad_fn=<DivBackward0>)\n",
      "i batch :  45   !!loss : tensor(1.8603, grad_fn=<DivBackward0>)\n",
      "i batch :  46   !!loss : tensor(1.8522, grad_fn=<DivBackward0>)\n",
      "i batch :  47   !!loss : tensor(2.2655, grad_fn=<DivBackward0>)\n",
      "i batch :  48   !!loss : tensor(2.1745, grad_fn=<DivBackward0>)\n",
      "i batch :  49   !!loss : tensor(2.0503, grad_fn=<DivBackward0>)\n",
      "i batch :  50   !!loss : tensor(2.4195, grad_fn=<DivBackward0>)\n",
      "i batch :  51   !!loss : tensor(1.9475, grad_fn=<DivBackward0>)\n",
      "i batch :  52   !!loss : tensor(1.8695, grad_fn=<DivBackward0>)\n",
      "i batch :  53   !!loss : tensor(1.9233, grad_fn=<DivBackward0>)\n",
      "i batch :  54   !!loss : tensor(2.2527, grad_fn=<DivBackward0>)\n",
      "i batch :  55   !!loss : tensor(1.9656, grad_fn=<DivBackward0>)\n",
      "i batch :  56   !!loss : tensor(1.6951, grad_fn=<DivBackward0>)\n",
      "i batch :  57   !!loss : tensor(2.1934, grad_fn=<DivBackward0>)\n",
      "i batch :  58   !!loss : tensor(1.8333, grad_fn=<DivBackward0>)\n",
      "i batch :  59   !!loss : tensor(2.0824, grad_fn=<DivBackward0>)\n",
      "i batch :  60   !!loss : tensor(1.5631, grad_fn=<DivBackward0>)\n",
      "i batch :  61   !!loss : tensor(1.7973, grad_fn=<DivBackward0>)\n",
      "i batch :  62   !!loss : tensor(2.2516, grad_fn=<DivBackward0>)\n",
      "i batch :  63   !!loss : tensor(2.3221, grad_fn=<DivBackward0>)\n",
      "i batch :  64   !!loss : tensor(2.4425, grad_fn=<DivBackward0>)\n",
      "i batch :  65   !!loss : tensor(1.7930, grad_fn=<DivBackward0>)\n",
      "i batch :  66   !!loss : tensor(2.0380, grad_fn=<DivBackward0>)\n",
      "i batch :  67   !!loss : tensor(2.5007, grad_fn=<DivBackward0>)\n",
      "i batch :  68   !!loss : tensor(2.3977, grad_fn=<DivBackward0>)\n",
      "i batch :  69   !!loss : tensor(2.0669, grad_fn=<DivBackward0>)\n",
      "i batch :  70   !!loss : tensor(2.0112, grad_fn=<DivBackward0>)\n",
      "i batch :  71   !!loss : tensor(1.9630, grad_fn=<DivBackward0>)\n",
      "i batch :  72   !!loss : tensor(2.0330, grad_fn=<DivBackward0>)\n",
      "i batch :  73   !!loss : tensor(1.8916, grad_fn=<DivBackward0>)\n",
      "i batch :  74   !!loss : tensor(1.8449, grad_fn=<DivBackward0>)\n",
      "i batch :  75   !!loss : tensor(2.2394, grad_fn=<DivBackward0>)\n",
      "i batch :  76   !!loss : tensor(2.2673, grad_fn=<DivBackward0>)\n",
      "i batch :  77   !!loss : tensor(2.6591, grad_fn=<DivBackward0>)\n",
      "i batch :  78   !!loss : tensor(1.8414, grad_fn=<DivBackward0>)\n",
      "i batch :  79   !!loss : tensor(1.9040, grad_fn=<DivBackward0>)\n",
      "i batch :  80   !!loss : tensor(2.2814, grad_fn=<DivBackward0>)\n",
      "i batch :  81   !!loss : tensor(2.0312, grad_fn=<DivBackward0>)\n",
      "i batch :  82   !!loss : tensor(1.7695, grad_fn=<DivBackward0>)\n",
      "i batch :  83   !!loss : tensor(2.1459, grad_fn=<DivBackward0>)\n",
      "i batch :  84   !!loss : tensor(2.3685, grad_fn=<DivBackward0>)\n",
      "i batch :  85   !!loss : tensor(1.9381, grad_fn=<DivBackward0>)\n",
      "i batch :  86   !!loss : tensor(1.4950, grad_fn=<DivBackward0>)\n",
      "i batch :  87   !!loss : tensor(2.1025, grad_fn=<DivBackward0>)\n",
      "i batch :  88   !!loss : tensor(1.9502, grad_fn=<DivBackward0>)\n",
      "i batch :  89   !!loss : tensor(2.3726, grad_fn=<DivBackward0>)\n",
      "i batch :  90   !!loss : tensor(2.1163, grad_fn=<DivBackward0>)\n",
      "i batch :  91   !!loss : tensor(1.9005, grad_fn=<DivBackward0>)\n",
      "i batch :  92   !!loss : tensor(1.8746, grad_fn=<DivBackward0>)\n",
      "i batch :  93   !!loss : tensor(2.0527, grad_fn=<DivBackward0>)\n",
      "i batch :  94   !!loss : tensor(1.7782, grad_fn=<DivBackward0>)\n",
      "i batch :  95   !!loss : tensor(1.9582, grad_fn=<DivBackward0>)\n",
      "i batch :  96   !!loss : tensor(1.9802, grad_fn=<DivBackward0>)\n",
      "i batch :  97   !!loss : tensor(2.4163, grad_fn=<DivBackward0>)\n",
      "i batch :  98   !!loss : tensor(1.8439, grad_fn=<DivBackward0>)\n",
      "i batch :  99   !!loss : tensor(2.2564, grad_fn=<DivBackward0>)\n",
      "i batch :  100   !!loss : tensor(1.8986, grad_fn=<DivBackward0>)\n",
      "i batch :  101   !!loss : tensor(2.1834, grad_fn=<DivBackward0>)\n",
      "i batch :  102   !!loss : tensor(1.8806, grad_fn=<DivBackward0>)\n",
      "i batch :  103   !!loss : tensor(1.9955, grad_fn=<DivBackward0>)\n",
      "i batch :  104   !!loss : tensor(2.1238, grad_fn=<DivBackward0>)\n",
      "i batch :  105   !!loss : tensor(2.4967, grad_fn=<DivBackward0>)\n",
      "i batch :  106   !!loss : tensor(1.8882, grad_fn=<DivBackward0>)\n",
      "i batch :  107   !!loss : tensor(2.5278, grad_fn=<DivBackward0>)\n",
      "i batch :  108   !!loss : tensor(2.0345, grad_fn=<DivBackward0>)\n",
      "i batch :  109   !!loss : tensor(2.1992, grad_fn=<DivBackward0>)\n",
      "i batch :  110   !!loss : tensor(2.3768, grad_fn=<DivBackward0>)\n",
      "i batch :  111   !!loss : tensor(1.7377, grad_fn=<DivBackward0>)\n",
      "i batch :  112   !!loss : tensor(2.3426, grad_fn=<DivBackward0>)\n",
      "i batch :  113   !!loss : tensor(2.1716, grad_fn=<DivBackward0>)\n",
      "i batch :  114   !!loss : tensor(1.7663, grad_fn=<DivBackward0>)\n",
      "i batch :  115   !!loss : tensor(0.2770, grad_fn=<DivBackward0>)\n",
      "Epoch :  5\n",
      "i batch :  0   !!loss : tensor(1.3883, grad_fn=<DivBackward0>)\n",
      "i batch :  1   !!loss : tensor(1.9428, grad_fn=<DivBackward0>)\n",
      "i batch :  2   !!loss : tensor(1.9098, grad_fn=<DivBackward0>)\n",
      "i batch :  3   !!loss : tensor(1.6406, grad_fn=<DivBackward0>)\n",
      "i batch :  4   !!loss : tensor(2.1389, grad_fn=<DivBackward0>)\n",
      "i batch :  5   !!loss : tensor(1.5775, grad_fn=<DivBackward0>)\n",
      "i batch :  6   !!loss : tensor(2.0951, grad_fn=<DivBackward0>)\n",
      "i batch :  7   !!loss : tensor(2.3981, grad_fn=<DivBackward0>)\n",
      "i batch :  8   !!loss : tensor(1.7971, grad_fn=<DivBackward0>)\n",
      "i batch :  9   !!loss : tensor(2.0846, grad_fn=<DivBackward0>)\n",
      "i batch :  10   !!loss : tensor(1.7792, grad_fn=<DivBackward0>)\n",
      "i batch :  11   !!loss : tensor(2.3659, grad_fn=<DivBackward0>)\n",
      "i batch :  12   !!loss : tensor(2.0677, grad_fn=<DivBackward0>)\n",
      "i batch :  13   !!loss : tensor(1.7201, grad_fn=<DivBackward0>)\n",
      "i batch :  14   !!loss : tensor(1.9891, grad_fn=<DivBackward0>)\n",
      "i batch :  15   !!loss : tensor(2.1643, grad_fn=<DivBackward0>)\n",
      "i batch :  16   !!loss : tensor(1.8755, grad_fn=<DivBackward0>)\n",
      "i batch :  17   !!loss : tensor(1.8178, grad_fn=<DivBackward0>)\n",
      "i batch :  18   !!loss : tensor(2.2228, grad_fn=<DivBackward0>)\n",
      "i batch :  19   !!loss : tensor(2.0989, grad_fn=<DivBackward0>)\n",
      "i batch :  20   !!loss : tensor(2.0980, grad_fn=<DivBackward0>)\n",
      "i batch :  21   !!loss : tensor(1.8702, grad_fn=<DivBackward0>)\n",
      "i batch :  22   !!loss : tensor(1.6502, grad_fn=<DivBackward0>)\n",
      "i batch :  23   !!loss : tensor(1.6401, grad_fn=<DivBackward0>)\n",
      "i batch :  24   !!loss : tensor(1.4363, grad_fn=<DivBackward0>)\n",
      "i batch :  25   !!loss : tensor(2.0218, grad_fn=<DivBackward0>)\n",
      "i batch :  26   !!loss : tensor(1.9541, grad_fn=<DivBackward0>)\n",
      "i batch :  27   !!loss : tensor(1.9977, grad_fn=<DivBackward0>)\n",
      "i batch :  28   !!loss : tensor(1.6180, grad_fn=<DivBackward0>)\n",
      "i batch :  29   !!loss : tensor(1.5795, grad_fn=<DivBackward0>)\n",
      "i batch :  30   !!loss : tensor(1.2348, grad_fn=<DivBackward0>)\n",
      "i batch :  31   !!loss : tensor(1.6695, grad_fn=<DivBackward0>)\n",
      "i batch :  32   !!loss : tensor(1.8848, grad_fn=<DivBackward0>)\n",
      "i batch :  33   !!loss : tensor(1.8099, grad_fn=<DivBackward0>)\n",
      "i batch :  34   !!loss : tensor(1.7562, grad_fn=<DivBackward0>)\n",
      "i batch :  35   !!loss : tensor(1.7854, grad_fn=<DivBackward0>)\n",
      "i batch :  36   !!loss : tensor(2.2248, grad_fn=<DivBackward0>)\n",
      "i batch :  37   !!loss : tensor(1.9433, grad_fn=<DivBackward0>)\n",
      "i batch :  38   !!loss : tensor(1.8632, grad_fn=<DivBackward0>)\n",
      "i batch :  39   !!loss : tensor(2.0255, grad_fn=<DivBackward0>)\n",
      "i batch :  40   !!loss : tensor(2.1155, grad_fn=<DivBackward0>)\n",
      "i batch :  41   !!loss : tensor(2.0667, grad_fn=<DivBackward0>)\n",
      "i batch :  42   !!loss : tensor(1.7603, grad_fn=<DivBackward0>)\n",
      "i batch :  43   !!loss : tensor(1.7760, grad_fn=<DivBackward0>)\n",
      "i batch :  44   !!loss : tensor(1.6566, grad_fn=<DivBackward0>)\n",
      "i batch :  45   !!loss : tensor(1.8890, grad_fn=<DivBackward0>)\n",
      "i batch :  46   !!loss : tensor(1.8370, grad_fn=<DivBackward0>)\n",
      "i batch :  47   !!loss : tensor(1.5328, grad_fn=<DivBackward0>)\n",
      "i batch :  48   !!loss : tensor(1.9402, grad_fn=<DivBackward0>)\n",
      "i batch :  49   !!loss : tensor(1.4718, grad_fn=<DivBackward0>)\n",
      "i batch :  50   !!loss : tensor(1.8748, grad_fn=<DivBackward0>)\n",
      "i batch :  51   !!loss : tensor(2.0289, grad_fn=<DivBackward0>)\n",
      "i batch :  52   !!loss : tensor(2.3598, grad_fn=<DivBackward0>)\n",
      "i batch :  53   !!loss : tensor(1.7156, grad_fn=<DivBackward0>)\n",
      "i batch :  54   !!loss : tensor(2.0146, grad_fn=<DivBackward0>)\n",
      "i batch :  55   !!loss : tensor(2.4510, grad_fn=<DivBackward0>)\n",
      "i batch :  56   !!loss : tensor(1.8408, grad_fn=<DivBackward0>)\n",
      "i batch :  57   !!loss : tensor(1.7229, grad_fn=<DivBackward0>)\n",
      "i batch :  58   !!loss : tensor(2.3569, grad_fn=<DivBackward0>)\n",
      "i batch :  59   !!loss : tensor(1.7318, grad_fn=<DivBackward0>)\n",
      "i batch :  60   !!loss : tensor(2.2376, grad_fn=<DivBackward0>)\n",
      "i batch :  61   !!loss : tensor(2.1078, grad_fn=<DivBackward0>)\n",
      "i batch :  62   !!loss : tensor(2.1441, grad_fn=<DivBackward0>)\n",
      "i batch :  63   !!loss : tensor(1.8681, grad_fn=<DivBackward0>)\n",
      "i batch :  64   !!loss : tensor(2.0579, grad_fn=<DivBackward0>)\n",
      "i batch :  65   !!loss : tensor(1.7955, grad_fn=<DivBackward0>)\n",
      "i batch :  66   !!loss : tensor(1.9542, grad_fn=<DivBackward0>)\n",
      "i batch :  67   !!loss : tensor(1.9581, grad_fn=<DivBackward0>)\n",
      "i batch :  68   !!loss : tensor(2.0528, grad_fn=<DivBackward0>)\n",
      "i batch :  69   !!loss : tensor(2.1755, grad_fn=<DivBackward0>)\n",
      "i batch :  70   !!loss : tensor(2.1183, grad_fn=<DivBackward0>)\n",
      "i batch :  71   !!loss : tensor(1.8205, grad_fn=<DivBackward0>)\n",
      "i batch :  72   !!loss : tensor(2.3062, grad_fn=<DivBackward0>)\n",
      "i batch :  73   !!loss : tensor(1.5436, grad_fn=<DivBackward0>)\n",
      "i batch :  74   !!loss : tensor(1.8458, grad_fn=<DivBackward0>)\n",
      "i batch :  75   !!loss : tensor(1.5834, grad_fn=<DivBackward0>)\n",
      "i batch :  76   !!loss : tensor(2.2593, grad_fn=<DivBackward0>)\n",
      "i batch :  77   !!loss : tensor(1.8591, grad_fn=<DivBackward0>)\n",
      "i batch :  78   !!loss : tensor(1.6849, grad_fn=<DivBackward0>)\n",
      "i batch :  79   !!loss : tensor(1.8991, grad_fn=<DivBackward0>)\n",
      "i batch :  80   !!loss : tensor(2.1317, grad_fn=<DivBackward0>)\n",
      "i batch :  81   !!loss : tensor(2.1132, grad_fn=<DivBackward0>)\n",
      "i batch :  82   !!loss : tensor(1.8528, grad_fn=<DivBackward0>)\n",
      "i batch :  83   !!loss : tensor(1.6826, grad_fn=<DivBackward0>)\n",
      "i batch :  84   !!loss : tensor(1.9855, grad_fn=<DivBackward0>)\n",
      "i batch :  85   !!loss : tensor(1.9575, grad_fn=<DivBackward0>)\n",
      "i batch :  86   !!loss : tensor(2.2581, grad_fn=<DivBackward0>)\n",
      "i batch :  87   !!loss : tensor(2.0675, grad_fn=<DivBackward0>)\n",
      "i batch :  88   !!loss : tensor(2.0143, grad_fn=<DivBackward0>)\n",
      "i batch :  89   !!loss : tensor(2.6041, grad_fn=<DivBackward0>)\n",
      "i batch :  90   !!loss : tensor(2.1863, grad_fn=<DivBackward0>)\n",
      "i batch :  91   !!loss : tensor(1.7324, grad_fn=<DivBackward0>)\n",
      "i batch :  92   !!loss : tensor(2.4804, grad_fn=<DivBackward0>)\n",
      "i batch :  93   !!loss : tensor(2.1776, grad_fn=<DivBackward0>)\n",
      "i batch :  94   !!loss : tensor(2.0633, grad_fn=<DivBackward0>)\n",
      "i batch :  95   !!loss : tensor(2.0345, grad_fn=<DivBackward0>)\n",
      "i batch :  96   !!loss : tensor(2.3277, grad_fn=<DivBackward0>)\n",
      "i batch :  97   !!loss : tensor(2.4210, grad_fn=<DivBackward0>)\n",
      "i batch :  98   !!loss : tensor(2.0686, grad_fn=<DivBackward0>)\n",
      "i batch :  99   !!loss : tensor(2.1749, grad_fn=<DivBackward0>)\n",
      "i batch :  100   !!loss : tensor(1.8272, grad_fn=<DivBackward0>)\n",
      "i batch :  101   !!loss : tensor(1.7962, grad_fn=<DivBackward0>)\n",
      "i batch :  102   !!loss : tensor(2.1034, grad_fn=<DivBackward0>)\n",
      "i batch :  103   !!loss : tensor(2.1529, grad_fn=<DivBackward0>)\n",
      "i batch :  104   !!loss : tensor(2.1456, grad_fn=<DivBackward0>)\n",
      "i batch :  105   !!loss : tensor(1.3721, grad_fn=<DivBackward0>)\n",
      "i batch :  106   !!loss : tensor(1.6220, grad_fn=<DivBackward0>)\n",
      "i batch :  107   !!loss : tensor(1.8060, grad_fn=<DivBackward0>)\n",
      "i batch :  108   !!loss : tensor(1.9965, grad_fn=<DivBackward0>)\n",
      "i batch :  109   !!loss : tensor(2.0229, grad_fn=<DivBackward0>)\n",
      "i batch :  110   !!loss : tensor(1.4393, grad_fn=<DivBackward0>)\n",
      "i batch :  111   !!loss : tensor(2.3453, grad_fn=<DivBackward0>)\n",
      "i batch :  112   !!loss : tensor(1.6750, grad_fn=<DivBackward0>)\n",
      "i batch :  113   !!loss : tensor(1.9864, grad_fn=<DivBackward0>)\n",
      "i batch :  114   !!loss : tensor(1.9724, grad_fn=<DivBackward0>)\n",
      "i batch :  115   !!loss : tensor(0.0785, grad_fn=<DivBackward0>)\n",
      "Epoch :  6\n",
      "i batch :  0   !!loss : tensor(1.4349, grad_fn=<DivBackward0>)\n",
      "i batch :  1   !!loss : tensor(1.6797, grad_fn=<DivBackward0>)\n",
      "i batch :  2   !!loss : tensor(1.9513, grad_fn=<DivBackward0>)\n",
      "i batch :  3   !!loss : tensor(1.3606, grad_fn=<DivBackward0>)\n",
      "i batch :  4   !!loss : tensor(1.3132, grad_fn=<DivBackward0>)\n",
      "i batch :  5   !!loss : tensor(1.8179, grad_fn=<DivBackward0>)\n",
      "i batch :  6   !!loss : tensor(1.2422, grad_fn=<DivBackward0>)\n",
      "i batch :  7   !!loss : tensor(1.4412, grad_fn=<DivBackward0>)\n",
      "i batch :  8   !!loss : tensor(1.3247, grad_fn=<DivBackward0>)\n",
      "i batch :  9   !!loss : tensor(1.2825, grad_fn=<DivBackward0>)\n",
      "i batch :  10   !!loss : tensor(1.7856, grad_fn=<DivBackward0>)\n",
      "i batch :  11   !!loss : tensor(1.2721, grad_fn=<DivBackward0>)\n",
      "i batch :  12   !!loss : tensor(1.3852, grad_fn=<DivBackward0>)\n",
      "i batch :  13   !!loss : tensor(1.8301, grad_fn=<DivBackward0>)\n",
      "i batch :  14   !!loss : tensor(1.4831, grad_fn=<DivBackward0>)\n",
      "i batch :  15   !!loss : tensor(1.4426, grad_fn=<DivBackward0>)\n",
      "i batch :  16   !!loss : tensor(1.7560, grad_fn=<DivBackward0>)\n",
      "i batch :  17   !!loss : tensor(1.4977, grad_fn=<DivBackward0>)\n",
      "i batch :  18   !!loss : tensor(1.6086, grad_fn=<DivBackward0>)\n",
      "i batch :  19   !!loss : tensor(1.4207, grad_fn=<DivBackward0>)\n",
      "i batch :  20   !!loss : tensor(1.4383, grad_fn=<DivBackward0>)\n",
      "i batch :  21   !!loss : tensor(1.9016, grad_fn=<DivBackward0>)\n",
      "i batch :  22   !!loss : tensor(1.7672, grad_fn=<DivBackward0>)\n",
      "i batch :  23   !!loss : tensor(1.5549, grad_fn=<DivBackward0>)\n",
      "i batch :  24   !!loss : tensor(1.6003, grad_fn=<DivBackward0>)\n",
      "i batch :  25   !!loss : tensor(1.7903, grad_fn=<DivBackward0>)\n",
      "i batch :  26   !!loss : tensor(1.9527, grad_fn=<DivBackward0>)\n",
      "i batch :  27   !!loss : tensor(1.6747, grad_fn=<DivBackward0>)\n",
      "i batch :  28   !!loss : tensor(1.5653, grad_fn=<DivBackward0>)\n",
      "i batch :  29   !!loss : tensor(1.6928, grad_fn=<DivBackward0>)\n",
      "i batch :  30   !!loss : tensor(1.6919, grad_fn=<DivBackward0>)\n",
      "i batch :  31   !!loss : tensor(2.1410, grad_fn=<DivBackward0>)\n",
      "i batch :  32   !!loss : tensor(1.9423, grad_fn=<DivBackward0>)\n",
      "i batch :  33   !!loss : tensor(1.6804, grad_fn=<DivBackward0>)\n",
      "i batch :  34   !!loss : tensor(1.6093, grad_fn=<DivBackward0>)\n",
      "i batch :  35   !!loss : tensor(1.0738, grad_fn=<DivBackward0>)\n",
      "i batch :  36   !!loss : tensor(1.4395, grad_fn=<DivBackward0>)\n",
      "i batch :  37   !!loss : tensor(1.5131, grad_fn=<DivBackward0>)\n",
      "i batch :  38   !!loss : tensor(1.5343, grad_fn=<DivBackward0>)\n",
      "i batch :  39   !!loss : tensor(1.8002, grad_fn=<DivBackward0>)\n",
      "i batch :  40   !!loss : tensor(1.8692, grad_fn=<DivBackward0>)\n",
      "i batch :  41   !!loss : tensor(1.5767, grad_fn=<DivBackward0>)\n",
      "i batch :  42   !!loss : tensor(1.4644, grad_fn=<DivBackward0>)\n",
      "i batch :  43   !!loss : tensor(1.9088, grad_fn=<DivBackward0>)\n",
      "i batch :  44   !!loss : tensor(1.7803, grad_fn=<DivBackward0>)\n",
      "i batch :  45   !!loss : tensor(1.5050, grad_fn=<DivBackward0>)\n",
      "i batch :  46   !!loss : tensor(1.6640, grad_fn=<DivBackward0>)\n",
      "i batch :  47   !!loss : tensor(2.0466, grad_fn=<DivBackward0>)\n",
      "i batch :  48   !!loss : tensor(1.9565, grad_fn=<DivBackward0>)\n",
      "i batch :  49   !!loss : tensor(1.6890, grad_fn=<DivBackward0>)\n",
      "i batch :  50   !!loss : tensor(1.4240, grad_fn=<DivBackward0>)\n",
      "i batch :  51   !!loss : tensor(1.6438, grad_fn=<DivBackward0>)\n",
      "i batch :  52   !!loss : tensor(1.2323, grad_fn=<DivBackward0>)\n",
      "i batch :  53   !!loss : tensor(1.3673, grad_fn=<DivBackward0>)\n",
      "i batch :  54   !!loss : tensor(1.2869, grad_fn=<DivBackward0>)\n",
      "i batch :  55   !!loss : tensor(1.6082, grad_fn=<DivBackward0>)\n",
      "i batch :  56   !!loss : tensor(2.1869, grad_fn=<DivBackward0>)\n",
      "i batch :  57   !!loss : tensor(1.3445, grad_fn=<DivBackward0>)\n",
      "i batch :  58   !!loss : tensor(1.3270, grad_fn=<DivBackward0>)\n",
      "i batch :  59   !!loss : tensor(1.5004, grad_fn=<DivBackward0>)\n",
      "i batch :  60   !!loss : tensor(1.7863, grad_fn=<DivBackward0>)\n",
      "i batch :  61   !!loss : tensor(2.1068, grad_fn=<DivBackward0>)\n",
      "i batch :  62   !!loss : tensor(1.6889, grad_fn=<DivBackward0>)\n",
      "i batch :  63   !!loss : tensor(2.0188, grad_fn=<DivBackward0>)\n",
      "i batch :  64   !!loss : tensor(1.7829, grad_fn=<DivBackward0>)\n",
      "i batch :  65   !!loss : tensor(1.5625, grad_fn=<DivBackward0>)\n",
      "i batch :  66   !!loss : tensor(1.7500, grad_fn=<DivBackward0>)\n",
      "i batch :  67   !!loss : tensor(1.4393, grad_fn=<DivBackward0>)\n",
      "i batch :  68   !!loss : tensor(1.4137, grad_fn=<DivBackward0>)\n",
      "i batch :  69   !!loss : tensor(1.4599, grad_fn=<DivBackward0>)\n",
      "i batch :  70   !!loss : tensor(1.5446, grad_fn=<DivBackward0>)\n",
      "i batch :  71   !!loss : tensor(2.0098, grad_fn=<DivBackward0>)\n",
      "i batch :  72   !!loss : tensor(1.1405, grad_fn=<DivBackward0>)\n",
      "i batch :  73   !!loss : tensor(1.8102, grad_fn=<DivBackward0>)\n",
      "i batch :  74   !!loss : tensor(1.4233, grad_fn=<DivBackward0>)\n",
      "i batch :  75   !!loss : tensor(1.7578, grad_fn=<DivBackward0>)\n",
      "i batch :  76   !!loss : tensor(1.6951, grad_fn=<DivBackward0>)\n",
      "i batch :  77   !!loss : tensor(1.7869, grad_fn=<DivBackward0>)\n",
      "i batch :  78   !!loss : tensor(1.3456, grad_fn=<DivBackward0>)\n",
      "i batch :  79   !!loss : tensor(1.7524, grad_fn=<DivBackward0>)\n",
      "i batch :  80   !!loss : tensor(1.2831, grad_fn=<DivBackward0>)\n",
      "i batch :  81   !!loss : tensor(1.6417, grad_fn=<DivBackward0>)\n",
      "i batch :  82   !!loss : tensor(2.1248, grad_fn=<DivBackward0>)\n",
      "i batch :  83   !!loss : tensor(1.3281, grad_fn=<DivBackward0>)\n",
      "i batch :  84   !!loss : tensor(1.3751, grad_fn=<DivBackward0>)\n",
      "i batch :  85   !!loss : tensor(1.5899, grad_fn=<DivBackward0>)\n",
      "i batch :  86   !!loss : tensor(1.9288, grad_fn=<DivBackward0>)\n",
      "i batch :  87   !!loss : tensor(1.3167, grad_fn=<DivBackward0>)\n",
      "i batch :  88   !!loss : tensor(2.0554, grad_fn=<DivBackward0>)\n",
      "i batch :  89   !!loss : tensor(1.3563, grad_fn=<DivBackward0>)\n",
      "i batch :  90   !!loss : tensor(1.6257, grad_fn=<DivBackward0>)\n",
      "i batch :  91   !!loss : tensor(1.6510, grad_fn=<DivBackward0>)\n",
      "i batch :  92   !!loss : tensor(1.6764, grad_fn=<DivBackward0>)\n",
      "i batch :  93   !!loss : tensor(1.3023, grad_fn=<DivBackward0>)\n",
      "i batch :  94   !!loss : tensor(2.5152, grad_fn=<DivBackward0>)\n",
      "i batch :  95   !!loss : tensor(2.8355, grad_fn=<DivBackward0>)\n",
      "i batch :  96   !!loss : tensor(1.9120, grad_fn=<DivBackward0>)\n",
      "i batch :  97   !!loss : tensor(2.4234, grad_fn=<DivBackward0>)\n",
      "i batch :  98   !!loss : tensor(1.9566, grad_fn=<DivBackward0>)\n",
      "i batch :  99   !!loss : tensor(2.0038, grad_fn=<DivBackward0>)\n",
      "i batch :  100   !!loss : tensor(1.9965, grad_fn=<DivBackward0>)\n",
      "i batch :  101   !!loss : tensor(2.0478, grad_fn=<DivBackward0>)\n",
      "i batch :  102   !!loss : tensor(2.1595, grad_fn=<DivBackward0>)\n",
      "i batch :  103   !!loss : tensor(2.0670, grad_fn=<DivBackward0>)\n",
      "i batch :  104   !!loss : tensor(2.1961, grad_fn=<DivBackward0>)\n",
      "i batch :  105   !!loss : tensor(1.8714, grad_fn=<DivBackward0>)\n",
      "i batch :  106   !!loss : tensor(1.7500, grad_fn=<DivBackward0>)\n",
      "i batch :  107   !!loss : tensor(2.1388, grad_fn=<DivBackward0>)\n",
      "i batch :  108   !!loss : tensor(1.4631, grad_fn=<DivBackward0>)\n",
      "i batch :  109   !!loss : tensor(1.8851, grad_fn=<DivBackward0>)\n",
      "i batch :  110   !!loss : tensor(1.6892, grad_fn=<DivBackward0>)\n",
      "i batch :  111   !!loss : tensor(1.7955, grad_fn=<DivBackward0>)\n",
      "i batch :  112   !!loss : tensor(1.3108, grad_fn=<DivBackward0>)\n",
      "i batch :  113   !!loss : tensor(1.4557, grad_fn=<DivBackward0>)\n",
      "i batch :  114   !!loss : tensor(2.0137, grad_fn=<DivBackward0>)\n",
      "i batch :  115   !!loss : tensor(0.1266, grad_fn=<DivBackward0>)\n",
      "Epoch :  7\n",
      "i batch :  0   !!loss : tensor(2.2359, grad_fn=<DivBackward0>)\n",
      "i batch :  1   !!loss : tensor(1.1985, grad_fn=<DivBackward0>)\n",
      "i batch :  2   !!loss : tensor(1.5981, grad_fn=<DivBackward0>)\n",
      "i batch :  3   !!loss : tensor(1.7308, grad_fn=<DivBackward0>)\n",
      "i batch :  4   !!loss : tensor(1.0748, grad_fn=<DivBackward0>)\n",
      "i batch :  5   !!loss : tensor(0.9725, grad_fn=<DivBackward0>)\n",
      "i batch :  6   !!loss : tensor(1.4966, grad_fn=<DivBackward0>)\n",
      "i batch :  7   !!loss : tensor(1.2671, grad_fn=<DivBackward0>)\n",
      "i batch :  8   !!loss : tensor(1.6597, grad_fn=<DivBackward0>)\n",
      "i batch :  9   !!loss : tensor(1.4526, grad_fn=<DivBackward0>)\n",
      "i batch :  10   !!loss : tensor(1.3709, grad_fn=<DivBackward0>)\n",
      "i batch :  11   !!loss : tensor(1.6128, grad_fn=<DivBackward0>)\n",
      "i batch :  12   !!loss : tensor(0.9605, grad_fn=<DivBackward0>)\n",
      "i batch :  13   !!loss : tensor(0.9293, grad_fn=<DivBackward0>)\n",
      "i batch :  14   !!loss : tensor(1.0758, grad_fn=<DivBackward0>)\n",
      "i batch :  15   !!loss : tensor(1.1854, grad_fn=<DivBackward0>)\n",
      "i batch :  16   !!loss : tensor(1.0129, grad_fn=<DivBackward0>)\n",
      "i batch :  17   !!loss : tensor(1.3944, grad_fn=<DivBackward0>)\n",
      "i batch :  18   !!loss : tensor(1.2339, grad_fn=<DivBackward0>)\n",
      "i batch :  19   !!loss : tensor(1.4543, grad_fn=<DivBackward0>)\n",
      "i batch :  20   !!loss : tensor(1.3069, grad_fn=<DivBackward0>)\n",
      "i batch :  21   !!loss : tensor(1.6335, grad_fn=<DivBackward0>)\n",
      "i batch :  22   !!loss : tensor(1.9250, grad_fn=<DivBackward0>)\n",
      "i batch :  23   !!loss : tensor(1.6951, grad_fn=<DivBackward0>)\n",
      "i batch :  24   !!loss : tensor(1.5345, grad_fn=<DivBackward0>)\n",
      "i batch :  25   !!loss : tensor(1.6918, grad_fn=<DivBackward0>)\n",
      "i batch :  26   !!loss : tensor(1.2057, grad_fn=<DivBackward0>)\n",
      "i batch :  27   !!loss : tensor(1.4102, grad_fn=<DivBackward0>)\n",
      "i batch :  28   !!loss : tensor(1.3868, grad_fn=<DivBackward0>)\n",
      "i batch :  29   !!loss : tensor(1.3853, grad_fn=<DivBackward0>)\n",
      "i batch :  30   !!loss : tensor(1.3919, grad_fn=<DivBackward0>)\n",
      "i batch :  31   !!loss : tensor(1.9687, grad_fn=<DivBackward0>)\n",
      "i batch :  32   !!loss : tensor(0.6283, grad_fn=<DivBackward0>)\n",
      "i batch :  33   !!loss : tensor(1.6412, grad_fn=<DivBackward0>)\n",
      "i batch :  34   !!loss : tensor(0.9251, grad_fn=<DivBackward0>)\n",
      "i batch :  35   !!loss : tensor(1.2468, grad_fn=<DivBackward0>)\n",
      "i batch :  36   !!loss : tensor(1.5785, grad_fn=<DivBackward0>)\n",
      "i batch :  37   !!loss : tensor(1.1539, grad_fn=<DivBackward0>)\n",
      "i batch :  38   !!loss : tensor(1.1616, grad_fn=<DivBackward0>)\n",
      "i batch :  39   !!loss : tensor(1.2763, grad_fn=<DivBackward0>)\n",
      "i batch :  40   !!loss : tensor(1.4602, grad_fn=<DivBackward0>)\n",
      "i batch :  41   !!loss : tensor(1.9097, grad_fn=<DivBackward0>)\n",
      "i batch :  42   !!loss : tensor(0.9561, grad_fn=<DivBackward0>)\n",
      "i batch :  43   !!loss : tensor(1.2147, grad_fn=<DivBackward0>)\n",
      "i batch :  44   !!loss : tensor(1.7603, grad_fn=<DivBackward0>)\n",
      "i batch :  45   !!loss : tensor(1.7451, grad_fn=<DivBackward0>)\n",
      "i batch :  46   !!loss : tensor(1.5339, grad_fn=<DivBackward0>)\n",
      "i batch :  47   !!loss : tensor(1.5253, grad_fn=<DivBackward0>)\n",
      "i batch :  48   !!loss : tensor(1.2027, grad_fn=<DivBackward0>)\n",
      "i batch :  49   !!loss : tensor(1.3811, grad_fn=<DivBackward0>)\n",
      "i batch :  50   !!loss : tensor(1.4562, grad_fn=<DivBackward0>)\n",
      "i batch :  51   !!loss : tensor(1.2357, grad_fn=<DivBackward0>)\n",
      "i batch :  52   !!loss : tensor(1.0736, grad_fn=<DivBackward0>)\n",
      "i batch :  53   !!loss : tensor(1.3974, grad_fn=<DivBackward0>)\n",
      "i batch :  54   !!loss : tensor(1.2466, grad_fn=<DivBackward0>)\n",
      "i batch :  55   !!loss : tensor(1.3751, grad_fn=<DivBackward0>)\n",
      "i batch :  56   !!loss : tensor(1.4543, grad_fn=<DivBackward0>)\n",
      "i batch :  57   !!loss : tensor(1.1017, grad_fn=<DivBackward0>)\n",
      "i batch :  58   !!loss : tensor(1.3288, grad_fn=<DivBackward0>)\n",
      "i batch :  59   !!loss : tensor(1.5159, grad_fn=<DivBackward0>)\n",
      "i batch :  60   !!loss : tensor(0.9923, grad_fn=<DivBackward0>)\n",
      "i batch :  61   !!loss : tensor(1.7952, grad_fn=<DivBackward0>)\n",
      "i batch :  62   !!loss : tensor(1.4290, grad_fn=<DivBackward0>)\n",
      "i batch :  63   !!loss : tensor(1.5524, grad_fn=<DivBackward0>)\n",
      "i batch :  64   !!loss : tensor(1.3862, grad_fn=<DivBackward0>)\n",
      "i batch :  65   !!loss : tensor(1.5485, grad_fn=<DivBackward0>)\n",
      "i batch :  66   !!loss : tensor(1.5874, grad_fn=<DivBackward0>)\n",
      "i batch :  67   !!loss : tensor(1.5753, grad_fn=<DivBackward0>)\n",
      "i batch :  68   !!loss : tensor(1.5287, grad_fn=<DivBackward0>)\n",
      "i batch :  69   !!loss : tensor(1.6128, grad_fn=<DivBackward0>)\n",
      "i batch :  70   !!loss : tensor(1.7053, grad_fn=<DivBackward0>)\n",
      "i batch :  71   !!loss : tensor(1.1669, grad_fn=<DivBackward0>)\n",
      "i batch :  72   !!loss : tensor(1.2528, grad_fn=<DivBackward0>)\n",
      "i batch :  73   !!loss : tensor(1.5555, grad_fn=<DivBackward0>)\n",
      "i batch :  74   !!loss : tensor(1.3268, grad_fn=<DivBackward0>)\n",
      "i batch :  75   !!loss : tensor(1.5129, grad_fn=<DivBackward0>)\n",
      "i batch :  76   !!loss : tensor(1.3603, grad_fn=<DivBackward0>)\n",
      "i batch :  77   !!loss : tensor(1.7756, grad_fn=<DivBackward0>)\n",
      "i batch :  78   !!loss : tensor(1.1716, grad_fn=<DivBackward0>)\n",
      "i batch :  79   !!loss : tensor(1.2170, grad_fn=<DivBackward0>)\n",
      "i batch :  80   !!loss : tensor(1.3641, grad_fn=<DivBackward0>)\n",
      "i batch :  81   !!loss : tensor(1.7160, grad_fn=<DivBackward0>)\n",
      "i batch :  82   !!loss : tensor(1.6716, grad_fn=<DivBackward0>)\n",
      "i batch :  83   !!loss : tensor(1.2094, grad_fn=<DivBackward0>)\n",
      "i batch :  84   !!loss : tensor(0.8395, grad_fn=<DivBackward0>)\n",
      "i batch :  85   !!loss : tensor(1.0757, grad_fn=<DivBackward0>)\n",
      "i batch :  86   !!loss : tensor(1.5216, grad_fn=<DivBackward0>)\n",
      "i batch :  87   !!loss : tensor(1.1401, grad_fn=<DivBackward0>)\n",
      "i batch :  88   !!loss : tensor(1.1135, grad_fn=<DivBackward0>)\n",
      "i batch :  89   !!loss : tensor(1.5715, grad_fn=<DivBackward0>)\n",
      "i batch :  90   !!loss : tensor(1.5933, grad_fn=<DivBackward0>)\n",
      "i batch :  91   !!loss : tensor(1.6729, grad_fn=<DivBackward0>)\n",
      "i batch :  92   !!loss : tensor(1.6202, grad_fn=<DivBackward0>)\n",
      "i batch :  93   !!loss : tensor(1.2029, grad_fn=<DivBackward0>)\n",
      "i batch :  94   !!loss : tensor(1.1251, grad_fn=<DivBackward0>)\n",
      "i batch :  95   !!loss : tensor(1.8807, grad_fn=<DivBackward0>)\n",
      "i batch :  96   !!loss : tensor(1.3519, grad_fn=<DivBackward0>)\n",
      "i batch :  97   !!loss : tensor(1.0852, grad_fn=<DivBackward0>)\n",
      "i batch :  98   !!loss : tensor(1.4283, grad_fn=<DivBackward0>)\n",
      "i batch :  99   !!loss : tensor(1.2566, grad_fn=<DivBackward0>)\n",
      "i batch :  100   !!loss : tensor(1.4543, grad_fn=<DivBackward0>)\n",
      "i batch :  101   !!loss : tensor(1.3279, grad_fn=<DivBackward0>)\n",
      "i batch :  102   !!loss : tensor(1.6876, grad_fn=<DivBackward0>)\n",
      "i batch :  103   !!loss : tensor(1.5783, grad_fn=<DivBackward0>)\n",
      "i batch :  104   !!loss : tensor(1.4201, grad_fn=<DivBackward0>)\n",
      "i batch :  105   !!loss : tensor(1.5555, grad_fn=<DivBackward0>)\n",
      "i batch :  106   !!loss : tensor(1.4207, grad_fn=<DivBackward0>)\n",
      "i batch :  107   !!loss : tensor(1.4679, grad_fn=<DivBackward0>)\n",
      "i batch :  108   !!loss : tensor(1.9627, grad_fn=<DivBackward0>)\n",
      "i batch :  109   !!loss : tensor(0.8886, grad_fn=<DivBackward0>)\n",
      "i batch :  110   !!loss : tensor(1.1829, grad_fn=<DivBackward0>)\n",
      "i batch :  111   !!loss : tensor(1.7633, grad_fn=<DivBackward0>)\n",
      "i batch :  112   !!loss : tensor(1.5763, grad_fn=<DivBackward0>)\n",
      "i batch :  113   !!loss : tensor(1.4492, grad_fn=<DivBackward0>)\n",
      "i batch :  114   !!loss : tensor(1.2878, grad_fn=<DivBackward0>)\n",
      "i batch :  115   !!loss : tensor(0.0690, grad_fn=<DivBackward0>)\n",
      "Epoch :  8\n",
      "i batch :  0   !!loss : tensor(1.1248, grad_fn=<DivBackward0>)\n",
      "i batch :  1   !!loss : tensor(1.5828, grad_fn=<DivBackward0>)\n",
      "i batch :  2   !!loss : tensor(1.3678, grad_fn=<DivBackward0>)\n",
      "i batch :  3   !!loss : tensor(0.7547, grad_fn=<DivBackward0>)\n",
      "i batch :  4   !!loss : tensor(1.3973, grad_fn=<DivBackward0>)\n",
      "i batch :  5   !!loss : tensor(1.2164, grad_fn=<DivBackward0>)\n",
      "i batch :  6   !!loss : tensor(1.5431, grad_fn=<DivBackward0>)\n",
      "i batch :  7   !!loss : tensor(1.5343, grad_fn=<DivBackward0>)\n",
      "i batch :  8   !!loss : tensor(1.3436, grad_fn=<DivBackward0>)\n",
      "i batch :  9   !!loss : tensor(0.9191, grad_fn=<DivBackward0>)\n",
      "i batch :  10   !!loss : tensor(1.3438, grad_fn=<DivBackward0>)\n",
      "i batch :  11   !!loss : tensor(1.1329, grad_fn=<DivBackward0>)\n",
      "i batch :  12   !!loss : tensor(1.5351, grad_fn=<DivBackward0>)\n",
      "i batch :  13   !!loss : tensor(1.1404, grad_fn=<DivBackward0>)\n",
      "i batch :  14   !!loss : tensor(1.0946, grad_fn=<DivBackward0>)\n",
      "i batch :  15   !!loss : tensor(0.9774, grad_fn=<DivBackward0>)\n",
      "i batch :  16   !!loss : tensor(1.0674, grad_fn=<DivBackward0>)\n",
      "i batch :  17   !!loss : tensor(1.4622, grad_fn=<DivBackward0>)\n",
      "i batch :  18   !!loss : tensor(1.2425, grad_fn=<DivBackward0>)\n",
      "i batch :  19   !!loss : tensor(1.3468, grad_fn=<DivBackward0>)\n",
      "i batch :  20   !!loss : tensor(0.7817, grad_fn=<DivBackward0>)\n",
      "i batch :  21   !!loss : tensor(0.8690, grad_fn=<DivBackward0>)\n",
      "i batch :  22   !!loss : tensor(0.8932, grad_fn=<DivBackward0>)\n",
      "i batch :  23   !!loss : tensor(0.9008, grad_fn=<DivBackward0>)\n",
      "i batch :  24   !!loss : tensor(0.7784, grad_fn=<DivBackward0>)\n",
      "i batch :  25   !!loss : tensor(1.4297, grad_fn=<DivBackward0>)\n",
      "i batch :  26   !!loss : tensor(1.1644, grad_fn=<DivBackward0>)\n",
      "i batch :  27   !!loss : tensor(1.1688, grad_fn=<DivBackward0>)\n",
      "i batch :  28   !!loss : tensor(1.3121, grad_fn=<DivBackward0>)\n",
      "i batch :  29   !!loss : tensor(1.5837, grad_fn=<DivBackward0>)\n",
      "i batch :  30   !!loss : tensor(1.2513, grad_fn=<DivBackward0>)\n",
      "i batch :  31   !!loss : tensor(1.0731, grad_fn=<DivBackward0>)\n",
      "i batch :  32   !!loss : tensor(1.2844, grad_fn=<DivBackward0>)\n",
      "i batch :  33   !!loss : tensor(0.9994, grad_fn=<DivBackward0>)\n",
      "i batch :  34   !!loss : tensor(0.8903, grad_fn=<DivBackward0>)\n",
      "i batch :  35   !!loss : tensor(0.7341, grad_fn=<DivBackward0>)\n",
      "i batch :  36   !!loss : tensor(1.1313, grad_fn=<DivBackward0>)\n",
      "i batch :  37   !!loss : tensor(0.8493, grad_fn=<DivBackward0>)\n",
      "i batch :  38   !!loss : tensor(1.0942, grad_fn=<DivBackward0>)\n",
      "i batch :  39   !!loss : tensor(1.7780, grad_fn=<DivBackward0>)\n",
      "i batch :  40   !!loss : tensor(1.0887, grad_fn=<DivBackward0>)\n",
      "i batch :  41   !!loss : tensor(1.2386, grad_fn=<DivBackward0>)\n",
      "i batch :  42   !!loss : tensor(1.8768, grad_fn=<DivBackward0>)\n",
      "i batch :  43   !!loss : tensor(0.7796, grad_fn=<DivBackward0>)\n",
      "i batch :  44   !!loss : tensor(1.1980, grad_fn=<DivBackward0>)\n",
      "i batch :  45   !!loss : tensor(1.1938, grad_fn=<DivBackward0>)\n",
      "i batch :  46   !!loss : tensor(1.2744, grad_fn=<DivBackward0>)\n",
      "i batch :  47   !!loss : tensor(0.9241, grad_fn=<DivBackward0>)\n",
      "i batch :  48   !!loss : tensor(1.5617, grad_fn=<DivBackward0>)\n",
      "i batch :  49   !!loss : tensor(1.0552, grad_fn=<DivBackward0>)\n",
      "i batch :  50   !!loss : tensor(0.9673, grad_fn=<DivBackward0>)\n",
      "i batch :  51   !!loss : tensor(0.8380, grad_fn=<DivBackward0>)\n",
      "i batch :  52   !!loss : tensor(0.7387, grad_fn=<DivBackward0>)\n",
      "i batch :  53   !!loss : tensor(1.2981, grad_fn=<DivBackward0>)\n",
      "i batch :  54   !!loss : tensor(0.9675, grad_fn=<DivBackward0>)\n",
      "i batch :  55   !!loss : tensor(1.1470, grad_fn=<DivBackward0>)\n",
      "i batch :  56   !!loss : tensor(1.1078, grad_fn=<DivBackward0>)\n",
      "i batch :  57   !!loss : tensor(0.9242, grad_fn=<DivBackward0>)\n",
      "i batch :  58   !!loss : tensor(0.3066, grad_fn=<DivBackward0>)\n",
      "i batch :  59   !!loss : tensor(0.7338, grad_fn=<DivBackward0>)\n",
      "i batch :  60   !!loss : tensor(1.2731, grad_fn=<DivBackward0>)\n",
      "i batch :  61   !!loss : tensor(1.2327, grad_fn=<DivBackward0>)\n",
      "i batch :  62   !!loss : tensor(1.0120, grad_fn=<DivBackward0>)\n",
      "i batch :  63   !!loss : tensor(0.4658, grad_fn=<DivBackward0>)\n",
      "i batch :  64   !!loss : tensor(1.1456, grad_fn=<DivBackward0>)\n",
      "i batch :  65   !!loss : tensor(1.3444, grad_fn=<DivBackward0>)\n",
      "i batch :  66   !!loss : tensor(1.4006, grad_fn=<DivBackward0>)\n",
      "i batch :  67   !!loss : tensor(1.0341, grad_fn=<DivBackward0>)\n",
      "i batch :  68   !!loss : tensor(1.0681, grad_fn=<DivBackward0>)\n",
      "i batch :  69   !!loss : tensor(1.4393, grad_fn=<DivBackward0>)\n",
      "i batch :  70   !!loss : tensor(1.4804, grad_fn=<DivBackward0>)\n",
      "i batch :  71   !!loss : tensor(1.0677, grad_fn=<DivBackward0>)\n",
      "i batch :  72   !!loss : tensor(1.2790, grad_fn=<DivBackward0>)\n",
      "i batch :  73   !!loss : tensor(0.7809, grad_fn=<DivBackward0>)\n",
      "i batch :  74   !!loss : tensor(1.1579, grad_fn=<DivBackward0>)\n",
      "i batch :  75   !!loss : tensor(0.8646, grad_fn=<DivBackward0>)\n",
      "i batch :  76   !!loss : tensor(1.5755, grad_fn=<DivBackward0>)\n",
      "i batch :  77   !!loss : tensor(0.7743, grad_fn=<DivBackward0>)\n",
      "i batch :  78   !!loss : tensor(1.3819, grad_fn=<DivBackward0>)\n",
      "i batch :  79   !!loss : tensor(1.2564, grad_fn=<DivBackward0>)\n",
      "i batch :  80   !!loss : tensor(1.1617, grad_fn=<DivBackward0>)\n",
      "i batch :  81   !!loss : tensor(1.1119, grad_fn=<DivBackward0>)\n",
      "i batch :  82   !!loss : tensor(1.0156, grad_fn=<DivBackward0>)\n",
      "i batch :  83   !!loss : tensor(1.3194, grad_fn=<DivBackward0>)\n",
      "i batch :  84   !!loss : tensor(1.2246, grad_fn=<DivBackward0>)\n",
      "i batch :  85   !!loss : tensor(1.9568, grad_fn=<DivBackward0>)\n",
      "i batch :  86   !!loss : tensor(1.5357, grad_fn=<DivBackward0>)\n",
      "i batch :  87   !!loss : tensor(1.3098, grad_fn=<DivBackward0>)\n",
      "i batch :  88   !!loss : tensor(0.8655, grad_fn=<DivBackward0>)\n",
      "i batch :  89   !!loss : tensor(1.1569, grad_fn=<DivBackward0>)\n",
      "i batch :  90   !!loss : tensor(1.0620, grad_fn=<DivBackward0>)\n",
      "i batch :  91   !!loss : tensor(1.5843, grad_fn=<DivBackward0>)\n",
      "i batch :  92   !!loss : tensor(0.7905, grad_fn=<DivBackward0>)\n",
      "i batch :  93   !!loss : tensor(1.3485, grad_fn=<DivBackward0>)\n",
      "i batch :  94   !!loss : tensor(1.3699, grad_fn=<DivBackward0>)\n",
      "i batch :  95   !!loss : tensor(0.9483, grad_fn=<DivBackward0>)\n",
      "i batch :  96   !!loss : tensor(0.9835, grad_fn=<DivBackward0>)\n",
      "i batch :  97   !!loss : tensor(1.3247, grad_fn=<DivBackward0>)\n",
      "i batch :  98   !!loss : tensor(0.9330, grad_fn=<DivBackward0>)\n",
      "i batch :  99   !!loss : tensor(0.7560, grad_fn=<DivBackward0>)\n",
      "i batch :  100   !!loss : tensor(1.1486, grad_fn=<DivBackward0>)\n",
      "i batch :  101   !!loss : tensor(1.3998, grad_fn=<DivBackward0>)\n",
      "i batch :  102   !!loss : tensor(1.1831, grad_fn=<DivBackward0>)\n",
      "i batch :  103   !!loss : tensor(1.5269, grad_fn=<DivBackward0>)\n",
      "i batch :  104   !!loss : tensor(1.7478, grad_fn=<DivBackward0>)\n",
      "i batch :  105   !!loss : tensor(1.2844, grad_fn=<DivBackward0>)\n",
      "i batch :  106   !!loss : tensor(1.0390, grad_fn=<DivBackward0>)\n",
      "i batch :  107   !!loss : tensor(1.5492, grad_fn=<DivBackward0>)\n",
      "i batch :  108   !!loss : tensor(1.4408, grad_fn=<DivBackward0>)\n",
      "i batch :  109   !!loss : tensor(1.2077, grad_fn=<DivBackward0>)\n",
      "i batch :  110   !!loss : tensor(1.3647, grad_fn=<DivBackward0>)\n",
      "i batch :  111   !!loss : tensor(1.5120, grad_fn=<DivBackward0>)\n",
      "i batch :  112   !!loss : tensor(1.3206, grad_fn=<DivBackward0>)\n",
      "i batch :  113   !!loss : tensor(1.5828, grad_fn=<DivBackward0>)\n",
      "i batch :  114   !!loss : tensor(1.5445, grad_fn=<DivBackward0>)\n",
      "i batch :  115   !!loss : tensor(0.0021, grad_fn=<DivBackward0>)\n",
      "Epoch :  9\n",
      "i batch :  0   !!loss : tensor(1.6846, grad_fn=<DivBackward0>)\n",
      "i batch :  1   !!loss : tensor(1.1879, grad_fn=<DivBackward0>)\n",
      "i batch :  2   !!loss : tensor(1.3340, grad_fn=<DivBackward0>)\n",
      "i batch :  3   !!loss : tensor(1.0406, grad_fn=<DivBackward0>)\n",
      "i batch :  4   !!loss : tensor(1.1703, grad_fn=<DivBackward0>)\n",
      "i batch :  5   !!loss : tensor(1.3132, grad_fn=<DivBackward0>)\n",
      "i batch :  6   !!loss : tensor(0.9951, grad_fn=<DivBackward0>)\n",
      "i batch :  7   !!loss : tensor(0.7721, grad_fn=<DivBackward0>)\n",
      "i batch :  8   !!loss : tensor(0.8672, grad_fn=<DivBackward0>)\n",
      "i batch :  9   !!loss : tensor(0.8927, grad_fn=<DivBackward0>)\n",
      "i batch :  10   !!loss : tensor(0.8864, grad_fn=<DivBackward0>)\n",
      "i batch :  11   !!loss : tensor(0.7756, grad_fn=<DivBackward0>)\n",
      "i batch :  12   !!loss : tensor(0.8013, grad_fn=<DivBackward0>)\n",
      "i batch :  13   !!loss : tensor(0.5198, grad_fn=<DivBackward0>)\n",
      "i batch :  14   !!loss : tensor(0.5291, grad_fn=<DivBackward0>)\n",
      "i batch :  15   !!loss : tensor(0.6994, grad_fn=<DivBackward0>)\n",
      "i batch :  16   !!loss : tensor(0.7231, grad_fn=<DivBackward0>)\n",
      "i batch :  17   !!loss : tensor(0.9345, grad_fn=<DivBackward0>)\n",
      "i batch :  18   !!loss : tensor(0.4285, grad_fn=<DivBackward0>)\n",
      "i batch :  19   !!loss : tensor(1.1858, grad_fn=<DivBackward0>)\n",
      "i batch :  20   !!loss : tensor(0.8946, grad_fn=<DivBackward0>)\n",
      "i batch :  21   !!loss : tensor(1.3277, grad_fn=<DivBackward0>)\n",
      "i batch :  22   !!loss : tensor(1.4113, grad_fn=<DivBackward0>)\n",
      "i batch :  23   !!loss : tensor(0.6238, grad_fn=<DivBackward0>)\n",
      "i batch :  24   !!loss : tensor(1.4320, grad_fn=<DivBackward0>)\n",
      "i batch :  25   !!loss : tensor(0.9406, grad_fn=<DivBackward0>)\n",
      "i batch :  26   !!loss : tensor(0.8623, grad_fn=<DivBackward0>)\n",
      "i batch :  27   !!loss : tensor(0.9867, grad_fn=<DivBackward0>)\n",
      "i batch :  28   !!loss : tensor(1.1669, grad_fn=<DivBackward0>)\n",
      "i batch :  29   !!loss : tensor(0.7527, grad_fn=<DivBackward0>)\n",
      "i batch :  30   !!loss : tensor(0.8211, grad_fn=<DivBackward0>)\n",
      "i batch :  31   !!loss : tensor(1.1844, grad_fn=<DivBackward0>)\n",
      "i batch :  32   !!loss : tensor(0.9954, grad_fn=<DivBackward0>)\n",
      "i batch :  33   !!loss : tensor(1.1090, grad_fn=<DivBackward0>)\n",
      "i batch :  34   !!loss : tensor(0.7014, grad_fn=<DivBackward0>)\n",
      "i batch :  35   !!loss : tensor(1.0642, grad_fn=<DivBackward0>)\n",
      "i batch :  36   !!loss : tensor(0.6319, grad_fn=<DivBackward0>)\n",
      "i batch :  37   !!loss : tensor(1.2036, grad_fn=<DivBackward0>)\n",
      "i batch :  38   !!loss : tensor(0.6540, grad_fn=<DivBackward0>)\n",
      "i batch :  39   !!loss : tensor(0.8709, grad_fn=<DivBackward0>)\n",
      "i batch :  40   !!loss : tensor(0.8471, grad_fn=<DivBackward0>)\n",
      "i batch :  41   !!loss : tensor(1.3326, grad_fn=<DivBackward0>)\n",
      "i batch :  42   !!loss : tensor(0.6531, grad_fn=<DivBackward0>)\n",
      "i batch :  43   !!loss : tensor(1.0047, grad_fn=<DivBackward0>)\n",
      "i batch :  44   !!loss : tensor(0.6734, grad_fn=<DivBackward0>)\n",
      "i batch :  45   !!loss : tensor(0.5550, grad_fn=<DivBackward0>)\n",
      "i batch :  46   !!loss : tensor(1.1420, grad_fn=<DivBackward0>)\n",
      "i batch :  47   !!loss : tensor(0.9885, grad_fn=<DivBackward0>)\n",
      "i batch :  48   !!loss : tensor(1.3402, grad_fn=<DivBackward0>)\n",
      "i batch :  49   !!loss : tensor(1.4104, grad_fn=<DivBackward0>)\n",
      "i batch :  50   !!loss : tensor(0.9956, grad_fn=<DivBackward0>)\n",
      "i batch :  51   !!loss : tensor(0.8259, grad_fn=<DivBackward0>)\n",
      "i batch :  52   !!loss : tensor(1.1562, grad_fn=<DivBackward0>)\n",
      "i batch :  53   !!loss : tensor(1.0592, grad_fn=<DivBackward0>)\n",
      "i batch :  54   !!loss : tensor(0.7858, grad_fn=<DivBackward0>)\n",
      "i batch :  55   !!loss : tensor(0.8041, grad_fn=<DivBackward0>)\n",
      "i batch :  56   !!loss : tensor(0.9563, grad_fn=<DivBackward0>)\n",
      "i batch :  57   !!loss : tensor(1.1431, grad_fn=<DivBackward0>)\n",
      "i batch :  58   !!loss : tensor(0.7650, grad_fn=<DivBackward0>)\n",
      "i batch :  59   !!loss : tensor(0.9042, grad_fn=<DivBackward0>)\n",
      "i batch :  60   !!loss : tensor(1.1681, grad_fn=<DivBackward0>)\n",
      "i batch :  61   !!loss : tensor(0.8875, grad_fn=<DivBackward0>)\n",
      "i batch :  62   !!loss : tensor(0.8275, grad_fn=<DivBackward0>)\n",
      "i batch :  63   !!loss : tensor(1.0249, grad_fn=<DivBackward0>)\n",
      "i batch :  64   !!loss : tensor(0.9337, grad_fn=<DivBackward0>)\n",
      "i batch :  65   !!loss : tensor(0.6309, grad_fn=<DivBackward0>)\n",
      "i batch :  66   !!loss : tensor(0.7561, grad_fn=<DivBackward0>)\n",
      "i batch :  67   !!loss : tensor(0.8990, grad_fn=<DivBackward0>)\n",
      "i batch :  68   !!loss : tensor(0.5817, grad_fn=<DivBackward0>)\n",
      "i batch :  69   !!loss : tensor(0.9433, grad_fn=<DivBackward0>)\n",
      "i batch :  70   !!loss : tensor(1.0067, grad_fn=<DivBackward0>)\n",
      "i batch :  71   !!loss : tensor(0.9465, grad_fn=<DivBackward0>)\n",
      "i batch :  72   !!loss : tensor(0.9510, grad_fn=<DivBackward0>)\n",
      "i batch :  73   !!loss : tensor(1.3850, grad_fn=<DivBackward0>)\n",
      "i batch :  74   !!loss : tensor(0.6857, grad_fn=<DivBackward0>)\n",
      "i batch :  75   !!loss : tensor(0.9601, grad_fn=<DivBackward0>)\n",
      "i batch :  76   !!loss : tensor(1.3631, grad_fn=<DivBackward0>)\n",
      "i batch :  77   !!loss : tensor(1.9572, grad_fn=<DivBackward0>)\n",
      "i batch :  78   !!loss : tensor(1.1508, grad_fn=<DivBackward0>)\n",
      "i batch :  79   !!loss : tensor(0.9044, grad_fn=<DivBackward0>)\n",
      "i batch :  80   !!loss : tensor(0.8970, grad_fn=<DivBackward0>)\n",
      "i batch :  81   !!loss : tensor(1.6195, grad_fn=<DivBackward0>)\n",
      "i batch :  82   !!loss : tensor(0.9548, grad_fn=<DivBackward0>)\n",
      "i batch :  83   !!loss : tensor(1.3180, grad_fn=<DivBackward0>)\n",
      "i batch :  84   !!loss : tensor(0.9828, grad_fn=<DivBackward0>)\n",
      "i batch :  85   !!loss : tensor(1.3125, grad_fn=<DivBackward0>)\n",
      "i batch :  86   !!loss : tensor(1.1301, grad_fn=<DivBackward0>)\n",
      "i batch :  87   !!loss : tensor(0.8218, grad_fn=<DivBackward0>)\n",
      "i batch :  88   !!loss : tensor(1.7273, grad_fn=<DivBackward0>)\n",
      "i batch :  89   !!loss : tensor(0.9731, grad_fn=<DivBackward0>)\n",
      "i batch :  90   !!loss : tensor(1.2382, grad_fn=<DivBackward0>)\n",
      "i batch :  91   !!loss : tensor(2.0470, grad_fn=<DivBackward0>)\n",
      "i batch :  92   !!loss : tensor(1.2229, grad_fn=<DivBackward0>)\n",
      "i batch :  93   !!loss : tensor(1.2354, grad_fn=<DivBackward0>)\n",
      "i batch :  94   !!loss : tensor(1.8530, grad_fn=<DivBackward0>)\n",
      "i batch :  95   !!loss : tensor(1.4851, grad_fn=<DivBackward0>)\n",
      "i batch :  96   !!loss : tensor(1.7315, grad_fn=<DivBackward0>)\n",
      "i batch :  97   !!loss : tensor(1.0652, grad_fn=<DivBackward0>)\n",
      "i batch :  98   !!loss : tensor(0.7381, grad_fn=<DivBackward0>)\n",
      "i batch :  99   !!loss : tensor(1.1426, grad_fn=<DivBackward0>)\n",
      "i batch :  100   !!loss : tensor(1.3086, grad_fn=<DivBackward0>)\n",
      "i batch :  101   !!loss : tensor(1.3097, grad_fn=<DivBackward0>)\n",
      "i batch :  102   !!loss : tensor(1.3523, grad_fn=<DivBackward0>)\n",
      "i batch :  103   !!loss : tensor(0.8405, grad_fn=<DivBackward0>)\n",
      "i batch :  104   !!loss : tensor(1.0302, grad_fn=<DivBackward0>)\n",
      "i batch :  105   !!loss : tensor(0.9360, grad_fn=<DivBackward0>)\n",
      "i batch :  106   !!loss : tensor(1.6710, grad_fn=<DivBackward0>)\n",
      "i batch :  107   !!loss : tensor(0.9869, grad_fn=<DivBackward0>)\n",
      "i batch :  108   !!loss : tensor(0.9815, grad_fn=<DivBackward0>)\n",
      "i batch :  109   !!loss : tensor(1.7285, grad_fn=<DivBackward0>)\n",
      "i batch :  110   !!loss : tensor(1.0061, grad_fn=<DivBackward0>)\n",
      "i batch :  111   !!loss : tensor(1.3540, grad_fn=<DivBackward0>)\n",
      "i batch :  112   !!loss : tensor(1.7151, grad_fn=<DivBackward0>)\n",
      "i batch :  113   !!loss : tensor(1.2297, grad_fn=<DivBackward0>)\n",
      "i batch :  114   !!loss : tensor(0.7765, grad_fn=<DivBackward0>)\n",
      "i batch :  115   !!loss : tensor(0.0016, grad_fn=<DivBackward0>)\n",
      "Epoch :  10\n",
      "i batch :  0   !!loss : tensor(0.9684, grad_fn=<DivBackward0>)\n",
      "i batch :  1   !!loss : tensor(1.0087, grad_fn=<DivBackward0>)\n",
      "i batch :  2   !!loss : tensor(0.9433, grad_fn=<DivBackward0>)\n",
      "i batch :  3   !!loss : tensor(1.0555, grad_fn=<DivBackward0>)\n",
      "i batch :  4   !!loss : tensor(0.8366, grad_fn=<DivBackward0>)\n",
      "i batch :  5   !!loss : tensor(0.5124, grad_fn=<DivBackward0>)\n",
      "i batch :  6   !!loss : tensor(1.3379, grad_fn=<DivBackward0>)\n",
      "i batch :  7   !!loss : tensor(1.0638, grad_fn=<DivBackward0>)\n",
      "i batch :  8   !!loss : tensor(0.7917, grad_fn=<DivBackward0>)\n",
      "i batch :  9   !!loss : tensor(0.7300, grad_fn=<DivBackward0>)\n",
      "i batch :  10   !!loss : tensor(0.9510, grad_fn=<DivBackward0>)\n",
      "i batch :  11   !!loss : tensor(0.6350, grad_fn=<DivBackward0>)\n",
      "i batch :  12   !!loss : tensor(1.0754, grad_fn=<DivBackward0>)\n",
      "i batch :  13   !!loss : tensor(1.2097, grad_fn=<DivBackward0>)\n",
      "i batch :  14   !!loss : tensor(0.9730, grad_fn=<DivBackward0>)\n",
      "i batch :  15   !!loss : tensor(0.9937, grad_fn=<DivBackward0>)\n",
      "i batch :  16   !!loss : tensor(0.9509, grad_fn=<DivBackward0>)\n",
      "i batch :  17   !!loss : tensor(0.9285, grad_fn=<DivBackward0>)\n",
      "i batch :  18   !!loss : tensor(1.4890, grad_fn=<DivBackward0>)\n",
      "i batch :  19   !!loss : tensor(0.9053, grad_fn=<DivBackward0>)\n",
      "i batch :  20   !!loss : tensor(0.6103, grad_fn=<DivBackward0>)\n",
      "i batch :  21   !!loss : tensor(1.3915, grad_fn=<DivBackward0>)\n",
      "i batch :  22   !!loss : tensor(1.2871, grad_fn=<DivBackward0>)\n",
      "i batch :  23   !!loss : tensor(1.1314, grad_fn=<DivBackward0>)\n",
      "i batch :  24   !!loss : tensor(0.9670, grad_fn=<DivBackward0>)\n",
      "i batch :  25   !!loss : tensor(1.0441, grad_fn=<DivBackward0>)\n",
      "i batch :  26   !!loss : tensor(1.9405, grad_fn=<DivBackward0>)\n",
      "i batch :  27   !!loss : tensor(1.1434, grad_fn=<DivBackward0>)\n",
      "i batch :  28   !!loss : tensor(1.5130, grad_fn=<DivBackward0>)\n",
      "i batch :  29   !!loss : tensor(1.0207, grad_fn=<DivBackward0>)\n",
      "i batch :  30   !!loss : tensor(1.1412, grad_fn=<DivBackward0>)\n",
      "i batch :  31   !!loss : tensor(1.2113, grad_fn=<DivBackward0>)\n",
      "i batch :  32   !!loss : tensor(1.0183, grad_fn=<DivBackward0>)\n",
      "i batch :  33   !!loss : tensor(1.1999, grad_fn=<DivBackward0>)\n",
      "i batch :  34   !!loss : tensor(1.1658, grad_fn=<DivBackward0>)\n",
      "i batch :  35   !!loss : tensor(0.4951, grad_fn=<DivBackward0>)\n",
      "i batch :  36   !!loss : tensor(0.9597, grad_fn=<DivBackward0>)\n",
      "i batch :  37   !!loss : tensor(0.6169, grad_fn=<DivBackward0>)\n",
      "i batch :  38   !!loss : tensor(0.7675, grad_fn=<DivBackward0>)\n",
      "i batch :  39   !!loss : tensor(0.5320, grad_fn=<DivBackward0>)\n",
      "i batch :  40   !!loss : tensor(0.9279, grad_fn=<DivBackward0>)\n",
      "i batch :  41   !!loss : tensor(0.5392, grad_fn=<DivBackward0>)\n",
      "i batch :  42   !!loss : tensor(0.6303, grad_fn=<DivBackward0>)\n",
      "i batch :  43   !!loss : tensor(0.7432, grad_fn=<DivBackward0>)\n",
      "i batch :  44   !!loss : tensor(1.1687, grad_fn=<DivBackward0>)\n",
      "i batch :  45   !!loss : tensor(0.8566, grad_fn=<DivBackward0>)\n",
      "i batch :  46   !!loss : tensor(0.7831, grad_fn=<DivBackward0>)\n",
      "i batch :  47   !!loss : tensor(1.0729, grad_fn=<DivBackward0>)\n",
      "i batch :  48   !!loss : tensor(0.8777, grad_fn=<DivBackward0>)\n",
      "i batch :  49   !!loss : tensor(0.7982, grad_fn=<DivBackward0>)\n",
      "i batch :  50   !!loss : tensor(0.9407, grad_fn=<DivBackward0>)\n",
      "i batch :  51   !!loss : tensor(0.9505, grad_fn=<DivBackward0>)\n",
      "i batch :  52   !!loss : tensor(0.7487, grad_fn=<DivBackward0>)\n",
      "i batch :  53   !!loss : tensor(0.7065, grad_fn=<DivBackward0>)\n",
      "i batch :  54   !!loss : tensor(1.2660, grad_fn=<DivBackward0>)\n",
      "i batch :  55   !!loss : tensor(0.7520, grad_fn=<DivBackward0>)\n",
      "i batch :  56   !!loss : tensor(0.8848, grad_fn=<DivBackward0>)\n",
      "i batch :  57   !!loss : tensor(1.3881, grad_fn=<DivBackward0>)\n",
      "i batch :  58   !!loss : tensor(0.9324, grad_fn=<DivBackward0>)\n",
      "i batch :  59   !!loss : tensor(0.6517, grad_fn=<DivBackward0>)\n",
      "i batch :  60   !!loss : tensor(1.2441, grad_fn=<DivBackward0>)\n",
      "i batch :  61   !!loss : tensor(1.1973, grad_fn=<DivBackward0>)\n",
      "i batch :  62   !!loss : tensor(0.8999, grad_fn=<DivBackward0>)\n",
      "i batch :  63   !!loss : tensor(0.8633, grad_fn=<DivBackward0>)\n",
      "i batch :  64   !!loss : tensor(0.7333, grad_fn=<DivBackward0>)\n",
      "i batch :  65   !!loss : tensor(1.3552, grad_fn=<DivBackward0>)\n",
      "i batch :  66   !!loss : tensor(0.9633, grad_fn=<DivBackward0>)\n",
      "i batch :  67   !!loss : tensor(1.1341, grad_fn=<DivBackward0>)\n",
      "i batch :  68   !!loss : tensor(0.8883, grad_fn=<DivBackward0>)\n",
      "i batch :  69   !!loss : tensor(1.0163, grad_fn=<DivBackward0>)\n",
      "i batch :  70   !!loss : tensor(0.9222, grad_fn=<DivBackward0>)\n",
      "i batch :  71   !!loss : tensor(0.9468, grad_fn=<DivBackward0>)\n",
      "i batch :  72   !!loss : tensor(1.2299, grad_fn=<DivBackward0>)\n",
      "i batch :  73   !!loss : tensor(0.4649, grad_fn=<DivBackward0>)\n",
      "i batch :  74   !!loss : tensor(0.7925, grad_fn=<DivBackward0>)\n",
      "i batch :  75   !!loss : tensor(1.2806, grad_fn=<DivBackward0>)\n",
      "i batch :  76   !!loss : tensor(1.1035, grad_fn=<DivBackward0>)\n",
      "i batch :  77   !!loss : tensor(0.9797, grad_fn=<DivBackward0>)\n",
      "i batch :  78   !!loss : tensor(0.7272, grad_fn=<DivBackward0>)\n",
      "i batch :  79   !!loss : tensor(0.8212, grad_fn=<DivBackward0>)\n",
      "i batch :  80   !!loss : tensor(0.9780, grad_fn=<DivBackward0>)\n",
      "i batch :  81   !!loss : tensor(1.0863, grad_fn=<DivBackward0>)\n",
      "i batch :  82   !!loss : tensor(1.0300, grad_fn=<DivBackward0>)\n",
      "i batch :  83   !!loss : tensor(0.9010, grad_fn=<DivBackward0>)\n",
      "i batch :  84   !!loss : tensor(1.3019, grad_fn=<DivBackward0>)\n",
      "i batch :  85   !!loss : tensor(0.8392, grad_fn=<DivBackward0>)\n",
      "i batch :  86   !!loss : tensor(1.0997, grad_fn=<DivBackward0>)\n",
      "i batch :  87   !!loss : tensor(1.2222, grad_fn=<DivBackward0>)\n",
      "i batch :  88   !!loss : tensor(1.5370, grad_fn=<DivBackward0>)\n",
      "i batch :  89   !!loss : tensor(1.0094, grad_fn=<DivBackward0>)\n",
      "i batch :  90   !!loss : tensor(1.3280, grad_fn=<DivBackward0>)\n",
      "i batch :  91   !!loss : tensor(0.9289, grad_fn=<DivBackward0>)\n",
      "i batch :  92   !!loss : tensor(0.9286, grad_fn=<DivBackward0>)\n",
      "i batch :  93   !!loss : tensor(1.2858, grad_fn=<DivBackward0>)\n",
      "i batch :  94   !!loss : tensor(0.7910, grad_fn=<DivBackward0>)\n",
      "i batch :  95   !!loss : tensor(1.1380, grad_fn=<DivBackward0>)\n",
      "i batch :  96   !!loss : tensor(1.4424, grad_fn=<DivBackward0>)\n",
      "i batch :  97   !!loss : tensor(0.9290, grad_fn=<DivBackward0>)\n",
      "i batch :  98   !!loss : tensor(0.7281, grad_fn=<DivBackward0>)\n",
      "i batch :  99   !!loss : tensor(0.7228, grad_fn=<DivBackward0>)\n",
      "i batch :  100   !!loss : tensor(0.6163, grad_fn=<DivBackward0>)\n",
      "i batch :  101   !!loss : tensor(0.9140, grad_fn=<DivBackward0>)\n",
      "i batch :  102   !!loss : tensor(1.0296, grad_fn=<DivBackward0>)\n",
      "i batch :  103   !!loss : tensor(0.5175, grad_fn=<DivBackward0>)\n",
      "i batch :  104   !!loss : tensor(1.1308, grad_fn=<DivBackward0>)\n",
      "i batch :  105   !!loss : tensor(1.2185, grad_fn=<DivBackward0>)\n",
      "i batch :  106   !!loss : tensor(1.8170, grad_fn=<DivBackward0>)\n",
      "i batch :  107   !!loss : tensor(0.7962, grad_fn=<DivBackward0>)\n",
      "i batch :  108   !!loss : tensor(1.3706, grad_fn=<DivBackward0>)\n",
      "i batch :  109   !!loss : tensor(1.1223, grad_fn=<DivBackward0>)\n",
      "i batch :  110   !!loss : tensor(1.4245, grad_fn=<DivBackward0>)\n",
      "i batch :  111   !!loss : tensor(0.7462, grad_fn=<DivBackward0>)\n",
      "i batch :  112   !!loss : tensor(1.0058, grad_fn=<DivBackward0>)\n",
      "i batch :  113   !!loss : tensor(0.8972, grad_fn=<DivBackward0>)\n",
      "i batch :  114   !!loss : tensor(1.6710, grad_fn=<DivBackward0>)\n",
      "i batch :  115   !!loss : tensor(0.0842, grad_fn=<DivBackward0>)\n",
      "Epoch :  11\n",
      "i batch :  0   !!loss : tensor(1.0786, grad_fn=<DivBackward0>)\n",
      "i batch :  1   !!loss : tensor(1.1542, grad_fn=<DivBackward0>)\n",
      "i batch :  2   !!loss : tensor(0.6551, grad_fn=<DivBackward0>)\n",
      "i batch :  3   !!loss : tensor(0.5480, grad_fn=<DivBackward0>)\n",
      "i batch :  4   !!loss : tensor(1.0169, grad_fn=<DivBackward0>)\n",
      "i batch :  5   !!loss : tensor(0.8310, grad_fn=<DivBackward0>)\n",
      "i batch :  6   !!loss : tensor(1.0011, grad_fn=<DivBackward0>)\n",
      "i batch :  7   !!loss : tensor(0.5458, grad_fn=<DivBackward0>)\n",
      "i batch :  8   !!loss : tensor(0.6337, grad_fn=<DivBackward0>)\n",
      "i batch :  9   !!loss : tensor(0.3974, grad_fn=<DivBackward0>)\n",
      "i batch :  10   !!loss : tensor(1.0030, grad_fn=<DivBackward0>)\n",
      "i batch :  11   !!loss : tensor(0.6989, grad_fn=<DivBackward0>)\n",
      "i batch :  12   !!loss : tensor(1.0968, grad_fn=<DivBackward0>)\n",
      "i batch :  13   !!loss : tensor(0.5804, grad_fn=<DivBackward0>)\n",
      "i batch :  14   !!loss : tensor(1.0128, grad_fn=<DivBackward0>)\n",
      "i batch :  15   !!loss : tensor(1.1091, grad_fn=<DivBackward0>)\n",
      "i batch :  16   !!loss : tensor(0.9507, grad_fn=<DivBackward0>)\n",
      "i batch :  17   !!loss : tensor(0.5447, grad_fn=<DivBackward0>)\n",
      "i batch :  18   !!loss : tensor(1.3694, grad_fn=<DivBackward0>)\n",
      "i batch :  19   !!loss : tensor(0.8106, grad_fn=<DivBackward0>)\n",
      "i batch :  20   !!loss : tensor(0.9939, grad_fn=<DivBackward0>)\n",
      "i batch :  21   !!loss : tensor(0.8871, grad_fn=<DivBackward0>)\n",
      "i batch :  22   !!loss : tensor(0.9139, grad_fn=<DivBackward0>)\n",
      "i batch :  23   !!loss : tensor(0.6809, grad_fn=<DivBackward0>)\n",
      "i batch :  24   !!loss : tensor(0.9252, grad_fn=<DivBackward0>)\n",
      "i batch :  25   !!loss : tensor(0.7964, grad_fn=<DivBackward0>)\n",
      "i batch :  26   !!loss : tensor(0.7606, grad_fn=<DivBackward0>)\n",
      "i batch :  27   !!loss : tensor(0.7798, grad_fn=<DivBackward0>)\n",
      "i batch :  28   !!loss : tensor(0.8644, grad_fn=<DivBackward0>)\n",
      "i batch :  29   !!loss : tensor(1.1598, grad_fn=<DivBackward0>)\n",
      "i batch :  30   !!loss : tensor(0.7688, grad_fn=<DivBackward0>)\n",
      "i batch :  31   !!loss : tensor(0.9860, grad_fn=<DivBackward0>)\n",
      "i batch :  32   !!loss : tensor(0.7927, grad_fn=<DivBackward0>)\n",
      "i batch :  33   !!loss : tensor(1.3000, grad_fn=<DivBackward0>)\n",
      "i batch :  34   !!loss : tensor(0.4280, grad_fn=<DivBackward0>)\n",
      "i batch :  35   !!loss : tensor(1.2129, grad_fn=<DivBackward0>)\n",
      "i batch :  36   !!loss : tensor(0.5911, grad_fn=<DivBackward0>)\n",
      "i batch :  37   !!loss : tensor(1.4970, grad_fn=<DivBackward0>)\n",
      "i batch :  38   !!loss : tensor(0.9303, grad_fn=<DivBackward0>)\n",
      "i batch :  39   !!loss : tensor(0.9137, grad_fn=<DivBackward0>)\n",
      "i batch :  40   !!loss : tensor(0.9363, grad_fn=<DivBackward0>)\n",
      "i batch :  41   !!loss : tensor(0.7058, grad_fn=<DivBackward0>)\n",
      "i batch :  42   !!loss : tensor(0.5890, grad_fn=<DivBackward0>)\n",
      "i batch :  43   !!loss : tensor(0.6875, grad_fn=<DivBackward0>)\n",
      "i batch :  44   !!loss : tensor(0.7029, grad_fn=<DivBackward0>)\n",
      "i batch :  45   !!loss : tensor(0.8939, grad_fn=<DivBackward0>)\n",
      "i batch :  46   !!loss : tensor(0.5075, grad_fn=<DivBackward0>)\n",
      "i batch :  47   !!loss : tensor(0.9950, grad_fn=<DivBackward0>)\n",
      "i batch :  48   !!loss : tensor(0.5399, grad_fn=<DivBackward0>)\n",
      "i batch :  49   !!loss : tensor(0.6726, grad_fn=<DivBackward0>)\n",
      "i batch :  50   !!loss : tensor(1.0955, grad_fn=<DivBackward0>)\n",
      "i batch :  51   !!loss : tensor(1.4394, grad_fn=<DivBackward0>)\n",
      "i batch :  52   !!loss : tensor(0.6127, grad_fn=<DivBackward0>)\n",
      "i batch :  53   !!loss : tensor(0.6779, grad_fn=<DivBackward0>)\n",
      "i batch :  54   !!loss : tensor(1.2588, grad_fn=<DivBackward0>)\n",
      "i batch :  55   !!loss : tensor(0.8255, grad_fn=<DivBackward0>)\n",
      "i batch :  56   !!loss : tensor(0.9838, grad_fn=<DivBackward0>)\n",
      "i batch :  57   !!loss : tensor(1.2404, grad_fn=<DivBackward0>)\n",
      "i batch :  58   !!loss : tensor(0.7026, grad_fn=<DivBackward0>)\n",
      "i batch :  59   !!loss : tensor(1.0794, grad_fn=<DivBackward0>)\n",
      "i batch :  60   !!loss : tensor(1.0869, grad_fn=<DivBackward0>)\n",
      "i batch :  61   !!loss : tensor(0.8324, grad_fn=<DivBackward0>)\n",
      "i batch :  62   !!loss : tensor(0.7443, grad_fn=<DivBackward0>)\n",
      "i batch :  63   !!loss : tensor(0.7552, grad_fn=<DivBackward0>)\n",
      "i batch :  64   !!loss : tensor(0.7809, grad_fn=<DivBackward0>)\n",
      "i batch :  65   !!loss : tensor(0.8739, grad_fn=<DivBackward0>)\n",
      "i batch :  66   !!loss : tensor(0.6084, grad_fn=<DivBackward0>)\n",
      "i batch :  67   !!loss : tensor(0.7072, grad_fn=<DivBackward0>)\n",
      "i batch :  68   !!loss : tensor(0.9359, grad_fn=<DivBackward0>)\n",
      "i batch :  69   !!loss : tensor(1.0782, grad_fn=<DivBackward0>)\n",
      "i batch :  70   !!loss : tensor(0.6027, grad_fn=<DivBackward0>)\n",
      "i batch :  71   !!loss : tensor(0.7677, grad_fn=<DivBackward0>)\n",
      "i batch :  72   !!loss : tensor(0.9138, grad_fn=<DivBackward0>)\n",
      "i batch :  73   !!loss : tensor(1.1165, grad_fn=<DivBackward0>)\n",
      "i batch :  74   !!loss : tensor(1.3424, grad_fn=<DivBackward0>)\n",
      "i batch :  75   !!loss : tensor(1.1971, grad_fn=<DivBackward0>)\n",
      "i batch :  76   !!loss : tensor(1.0018, grad_fn=<DivBackward0>)\n",
      "i batch :  77   !!loss : tensor(0.7977, grad_fn=<DivBackward0>)\n",
      "i batch :  78   !!loss : tensor(1.0852, grad_fn=<DivBackward0>)\n",
      "i batch :  79   !!loss : tensor(1.1618, grad_fn=<DivBackward0>)\n",
      "i batch :  80   !!loss : tensor(0.9708, grad_fn=<DivBackward0>)\n",
      "i batch :  81   !!loss : tensor(1.1523, grad_fn=<DivBackward0>)\n",
      "i batch :  82   !!loss : tensor(0.7465, grad_fn=<DivBackward0>)\n",
      "i batch :  83   !!loss : tensor(1.1058, grad_fn=<DivBackward0>)\n",
      "i batch :  84   !!loss : tensor(0.6699, grad_fn=<DivBackward0>)\n",
      "i batch :  85   !!loss : tensor(0.8208, grad_fn=<DivBackward0>)\n",
      "i batch :  86   !!loss : tensor(0.6613, grad_fn=<DivBackward0>)\n",
      "i batch :  87   !!loss : tensor(0.8867, grad_fn=<DivBackward0>)\n",
      "i batch :  88   !!loss : tensor(0.7044, grad_fn=<DivBackward0>)\n",
      "i batch :  89   !!loss : tensor(0.9819, grad_fn=<DivBackward0>)\n",
      "i batch :  90   !!loss : tensor(1.1334, grad_fn=<DivBackward0>)\n",
      "i batch :  91   !!loss : tensor(1.0949, grad_fn=<DivBackward0>)\n",
      "i batch :  92   !!loss : tensor(1.0738, grad_fn=<DivBackward0>)\n",
      "i batch :  93   !!loss : tensor(1.1826, grad_fn=<DivBackward0>)\n",
      "i batch :  94   !!loss : tensor(0.7341, grad_fn=<DivBackward0>)\n",
      "i batch :  95   !!loss : tensor(0.4201, grad_fn=<DivBackward0>)\n",
      "i batch :  96   !!loss : tensor(0.9283, grad_fn=<DivBackward0>)\n",
      "i batch :  97   !!loss : tensor(0.9334, grad_fn=<DivBackward0>)\n",
      "i batch :  98   !!loss : tensor(0.5139, grad_fn=<DivBackward0>)\n",
      "i batch :  99   !!loss : tensor(1.0478, grad_fn=<DivBackward0>)\n",
      "i batch :  100   !!loss : tensor(1.2223, grad_fn=<DivBackward0>)\n",
      "i batch :  101   !!loss : tensor(0.9174, grad_fn=<DivBackward0>)\n",
      "i batch :  102   !!loss : tensor(1.0656, grad_fn=<DivBackward0>)\n",
      "i batch :  103   !!loss : tensor(1.2997, grad_fn=<DivBackward0>)\n",
      "i batch :  104   !!loss : tensor(0.8801, grad_fn=<DivBackward0>)\n",
      "i batch :  105   !!loss : tensor(1.3531, grad_fn=<DivBackward0>)\n",
      "i batch :  106   !!loss : tensor(0.6132, grad_fn=<DivBackward0>)\n",
      "i batch :  107   !!loss : tensor(0.7411, grad_fn=<DivBackward0>)\n",
      "i batch :  108   !!loss : tensor(1.0213, grad_fn=<DivBackward0>)\n",
      "i batch :  109   !!loss : tensor(1.1799, grad_fn=<DivBackward0>)\n",
      "i batch :  110   !!loss : tensor(0.7220, grad_fn=<DivBackward0>)\n",
      "i batch :  111   !!loss : tensor(0.7339, grad_fn=<DivBackward0>)\n",
      "i batch :  112   !!loss : tensor(0.9900, grad_fn=<DivBackward0>)\n",
      "i batch :  113   !!loss : tensor(0.8217, grad_fn=<DivBackward0>)\n",
      "i batch :  114   !!loss : tensor(0.6393, grad_fn=<DivBackward0>)\n",
      "i batch :  115   !!loss : tensor(0.0002, grad_fn=<DivBackward0>)\n",
      "Epoch :  12\n",
      "i batch :  0   !!loss : tensor(1.4981, grad_fn=<DivBackward0>)\n",
      "i batch :  1   !!loss : tensor(0.4796, grad_fn=<DivBackward0>)\n",
      "i batch :  2   !!loss : tensor(0.4063, grad_fn=<DivBackward0>)\n",
      "i batch :  3   !!loss : tensor(0.3889, grad_fn=<DivBackward0>)\n",
      "i batch :  4   !!loss : tensor(0.8263, grad_fn=<DivBackward0>)\n",
      "i batch :  5   !!loss : tensor(0.7178, grad_fn=<DivBackward0>)\n",
      "i batch :  6   !!loss : tensor(0.7896, grad_fn=<DivBackward0>)\n",
      "i batch :  7   !!loss : tensor(0.5157, grad_fn=<DivBackward0>)\n",
      "i batch :  8   !!loss : tensor(0.6643, grad_fn=<DivBackward0>)\n",
      "i batch :  9   !!loss : tensor(1.0611, grad_fn=<DivBackward0>)\n",
      "i batch :  10   !!loss : tensor(1.2225, grad_fn=<DivBackward0>)\n",
      "i batch :  11   !!loss : tensor(0.6414, grad_fn=<DivBackward0>)\n",
      "i batch :  12   !!loss : tensor(0.9274, grad_fn=<DivBackward0>)\n",
      "i batch :  13   !!loss : tensor(0.6198, grad_fn=<DivBackward0>)\n",
      "i batch :  14   !!loss : tensor(0.6563, grad_fn=<DivBackward0>)\n",
      "i batch :  15   !!loss : tensor(0.6643, grad_fn=<DivBackward0>)\n",
      "i batch :  16   !!loss : tensor(0.7020, grad_fn=<DivBackward0>)\n",
      "i batch :  17   !!loss : tensor(0.7153, grad_fn=<DivBackward0>)\n",
      "i batch :  18   !!loss : tensor(1.1697, grad_fn=<DivBackward0>)\n",
      "i batch :  19   !!loss : tensor(0.5401, grad_fn=<DivBackward0>)\n",
      "i batch :  20   !!loss : tensor(0.9875, grad_fn=<DivBackward0>)\n",
      "i batch :  21   !!loss : tensor(0.6291, grad_fn=<DivBackward0>)\n",
      "i batch :  22   !!loss : tensor(0.8294, grad_fn=<DivBackward0>)\n",
      "i batch :  23   !!loss : tensor(1.7106, grad_fn=<DivBackward0>)\n",
      "i batch :  24   !!loss : tensor(0.7537, grad_fn=<DivBackward0>)\n",
      "i batch :  25   !!loss : tensor(0.7295, grad_fn=<DivBackward0>)\n",
      "i batch :  26   !!loss : tensor(0.8281, grad_fn=<DivBackward0>)\n",
      "i batch :  27   !!loss : tensor(1.1834, grad_fn=<DivBackward0>)\n",
      "i batch :  28   !!loss : tensor(0.4132, grad_fn=<DivBackward0>)\n",
      "i batch :  29   !!loss : tensor(0.6351, grad_fn=<DivBackward0>)\n",
      "i batch :  30   !!loss : tensor(1.3966, grad_fn=<DivBackward0>)\n",
      "i batch :  31   !!loss : tensor(0.8643, grad_fn=<DivBackward0>)\n",
      "i batch :  32   !!loss : tensor(0.8815, grad_fn=<DivBackward0>)\n",
      "i batch :  33   !!loss : tensor(0.7859, grad_fn=<DivBackward0>)\n",
      "i batch :  34   !!loss : tensor(0.9409, grad_fn=<DivBackward0>)\n",
      "i batch :  35   !!loss : tensor(0.6309, grad_fn=<DivBackward0>)\n",
      "i batch :  36   !!loss : tensor(1.0194, grad_fn=<DivBackward0>)\n",
      "i batch :  37   !!loss : tensor(1.5826, grad_fn=<DivBackward0>)\n",
      "i batch :  38   !!loss : tensor(0.6097, grad_fn=<DivBackward0>)\n",
      "i batch :  39   !!loss : tensor(1.2794, grad_fn=<DivBackward0>)\n",
      "i batch :  40   !!loss : tensor(0.8066, grad_fn=<DivBackward0>)\n",
      "i batch :  41   !!loss : tensor(0.7935, grad_fn=<DivBackward0>)\n",
      "i batch :  42   !!loss : tensor(0.7547, grad_fn=<DivBackward0>)\n",
      "i batch :  43   !!loss : tensor(0.8931, grad_fn=<DivBackward0>)\n",
      "i batch :  44   !!loss : tensor(0.7631, grad_fn=<DivBackward0>)\n",
      "i batch :  45   !!loss : tensor(0.7341, grad_fn=<DivBackward0>)\n",
      "i batch :  46   !!loss : tensor(0.5022, grad_fn=<DivBackward0>)\n",
      "i batch :  47   !!loss : tensor(0.6422, grad_fn=<DivBackward0>)\n",
      "i batch :  48   !!loss : tensor(0.9948, grad_fn=<DivBackward0>)\n",
      "i batch :  49   !!loss : tensor(0.6483, grad_fn=<DivBackward0>)\n",
      "i batch :  50   !!loss : tensor(1.0217, grad_fn=<DivBackward0>)\n",
      "i batch :  51   !!loss : tensor(0.4498, grad_fn=<DivBackward0>)\n",
      "i batch :  52   !!loss : tensor(0.5865, grad_fn=<DivBackward0>)\n",
      "i batch :  53   !!loss : tensor(1.2790, grad_fn=<DivBackward0>)\n",
      "i batch :  54   !!loss : tensor(0.5658, grad_fn=<DivBackward0>)\n",
      "i batch :  55   !!loss : tensor(0.7438, grad_fn=<DivBackward0>)\n",
      "i batch :  56   !!loss : tensor(0.8907, grad_fn=<DivBackward0>)\n",
      "i batch :  57   !!loss : tensor(0.9379, grad_fn=<DivBackward0>)\n",
      "i batch :  58   !!loss : tensor(1.0901, grad_fn=<DivBackward0>)\n",
      "i batch :  59   !!loss : tensor(0.7177, grad_fn=<DivBackward0>)\n",
      "i batch :  60   !!loss : tensor(0.8766, grad_fn=<DivBackward0>)\n",
      "i batch :  61   !!loss : tensor(0.5745, grad_fn=<DivBackward0>)\n",
      "i batch :  62   !!loss : tensor(1.0417, grad_fn=<DivBackward0>)\n",
      "i batch :  63   !!loss : tensor(1.0046, grad_fn=<DivBackward0>)\n",
      "i batch :  64   !!loss : tensor(0.8354, grad_fn=<DivBackward0>)\n",
      "i batch :  65   !!loss : tensor(0.6293, grad_fn=<DivBackward0>)\n",
      "i batch :  66   !!loss : tensor(0.8980, grad_fn=<DivBackward0>)\n",
      "i batch :  67   !!loss : tensor(0.5087, grad_fn=<DivBackward0>)\n",
      "i batch :  68   !!loss : tensor(0.7425, grad_fn=<DivBackward0>)\n",
      "i batch :  69   !!loss : tensor(1.4726, grad_fn=<DivBackward0>)\n",
      "i batch :  70   !!loss : tensor(0.6213, grad_fn=<DivBackward0>)\n",
      "i batch :  71   !!loss : tensor(1.0143, grad_fn=<DivBackward0>)\n",
      "i batch :  72   !!loss : tensor(1.1200, grad_fn=<DivBackward0>)\n",
      "i batch :  73   !!loss : tensor(0.8316, grad_fn=<DivBackward0>)\n",
      "i batch :  74   !!loss : tensor(0.7502, grad_fn=<DivBackward0>)\n",
      "i batch :  75   !!loss : tensor(1.0365, grad_fn=<DivBackward0>)\n",
      "i batch :  76   !!loss : tensor(0.5884, grad_fn=<DivBackward0>)\n",
      "i batch :  77   !!loss : tensor(0.8756, grad_fn=<DivBackward0>)\n",
      "i batch :  78   !!loss : tensor(0.6051, grad_fn=<DivBackward0>)\n",
      "i batch :  79   !!loss : tensor(0.9185, grad_fn=<DivBackward0>)\n",
      "i batch :  80   !!loss : tensor(0.6974, grad_fn=<DivBackward0>)\n",
      "i batch :  81   !!loss : tensor(0.5960, grad_fn=<DivBackward0>)\n",
      "i batch :  82   !!loss : tensor(1.1049, grad_fn=<DivBackward0>)\n",
      "i batch :  83   !!loss : tensor(0.5419, grad_fn=<DivBackward0>)\n",
      "i batch :  84   !!loss : tensor(0.6290, grad_fn=<DivBackward0>)\n",
      "i batch :  85   !!loss : tensor(0.4044, grad_fn=<DivBackward0>)\n",
      "i batch :  86   !!loss : tensor(0.5673, grad_fn=<DivBackward0>)\n",
      "i batch :  87   !!loss : tensor(0.5627, grad_fn=<DivBackward0>)\n",
      "i batch :  88   !!loss : tensor(0.3241, grad_fn=<DivBackward0>)\n",
      "i batch :  89   !!loss : tensor(0.6701, grad_fn=<DivBackward0>)\n",
      "i batch :  90   !!loss : tensor(1.2651, grad_fn=<DivBackward0>)\n",
      "i batch :  91   !!loss : tensor(1.8941, grad_fn=<DivBackward0>)\n",
      "i batch :  92   !!loss : tensor(0.6662, grad_fn=<DivBackward0>)\n",
      "i batch :  93   !!loss : tensor(1.1322, grad_fn=<DivBackward0>)\n",
      "i batch :  94   !!loss : tensor(1.1497, grad_fn=<DivBackward0>)\n",
      "i batch :  95   !!loss : tensor(1.0260, grad_fn=<DivBackward0>)\n",
      "i batch :  96   !!loss : tensor(0.5673, grad_fn=<DivBackward0>)\n",
      "i batch :  97   !!loss : tensor(0.9816, grad_fn=<DivBackward0>)\n",
      "i batch :  98   !!loss : tensor(0.9042, grad_fn=<DivBackward0>)\n",
      "i batch :  99   !!loss : tensor(1.2091, grad_fn=<DivBackward0>)\n",
      "i batch :  100   !!loss : tensor(0.8297, grad_fn=<DivBackward0>)\n",
      "i batch :  101   !!loss : tensor(0.6152, grad_fn=<DivBackward0>)\n",
      "i batch :  102   !!loss : tensor(0.7056, grad_fn=<DivBackward0>)\n",
      "i batch :  103   !!loss : tensor(0.9020, grad_fn=<DivBackward0>)\n",
      "i batch :  104   !!loss : tensor(0.8343, grad_fn=<DivBackward0>)\n",
      "i batch :  105   !!loss : tensor(0.8126, grad_fn=<DivBackward0>)\n",
      "i batch :  106   !!loss : tensor(0.5880, grad_fn=<DivBackward0>)\n",
      "i batch :  107   !!loss : tensor(0.6646, grad_fn=<DivBackward0>)\n",
      "i batch :  108   !!loss : tensor(0.8055, grad_fn=<DivBackward0>)\n",
      "i batch :  109   !!loss : tensor(0.9932, grad_fn=<DivBackward0>)\n",
      "i batch :  110   !!loss : tensor(0.8372, grad_fn=<DivBackward0>)\n",
      "i batch :  111   !!loss : tensor(1.1594, grad_fn=<DivBackward0>)\n",
      "i batch :  112   !!loss : tensor(0.7166, grad_fn=<DivBackward0>)\n",
      "i batch :  113   !!loss : tensor(0.7103, grad_fn=<DivBackward0>)\n",
      "i batch :  114   !!loss : tensor(0.8425, grad_fn=<DivBackward0>)\n",
      "i batch :  115   !!loss : tensor(3.2363e-05, grad_fn=<DivBackward0>)\n",
      "Epoch :  13\n",
      "i batch :  0   !!loss : tensor(0.4694, grad_fn=<DivBackward0>)\n",
      "i batch :  1   !!loss : tensor(0.9636, grad_fn=<DivBackward0>)\n",
      "i batch :  2   !!loss : tensor(0.5168, grad_fn=<DivBackward0>)\n",
      "i batch :  3   !!loss : tensor(0.7083, grad_fn=<DivBackward0>)\n",
      "i batch :  4   !!loss : tensor(0.7499, grad_fn=<DivBackward0>)\n",
      "i batch :  5   !!loss : tensor(0.3995, grad_fn=<DivBackward0>)\n",
      "i batch :  6   !!loss : tensor(0.6786, grad_fn=<DivBackward0>)\n",
      "i batch :  7   !!loss : tensor(1.1397, grad_fn=<DivBackward0>)\n",
      "i batch :  8   !!loss : tensor(0.5914, grad_fn=<DivBackward0>)\n",
      "i batch :  9   !!loss : tensor(0.2962, grad_fn=<DivBackward0>)\n",
      "i batch :  10   !!loss : tensor(1.4854, grad_fn=<DivBackward0>)\n",
      "i batch :  11   !!loss : tensor(0.4677, grad_fn=<DivBackward0>)\n",
      "i batch :  12   !!loss : tensor(0.7357, grad_fn=<DivBackward0>)\n",
      "i batch :  13   !!loss : tensor(0.4594, grad_fn=<DivBackward0>)\n",
      "i batch :  14   !!loss : tensor(0.8162, grad_fn=<DivBackward0>)\n",
      "i batch :  15   !!loss : tensor(0.5741, grad_fn=<DivBackward0>)\n",
      "i batch :  16   !!loss : tensor(0.9667, grad_fn=<DivBackward0>)\n",
      "i batch :  17   !!loss : tensor(0.3742, grad_fn=<DivBackward0>)\n",
      "i batch :  18   !!loss : tensor(0.4485, grad_fn=<DivBackward0>)\n",
      "i batch :  19   !!loss : tensor(0.5836, grad_fn=<DivBackward0>)\n",
      "i batch :  20   !!loss : tensor(0.3336, grad_fn=<DivBackward0>)\n",
      "i batch :  21   !!loss : tensor(0.2025, grad_fn=<DivBackward0>)\n",
      "i batch :  22   !!loss : tensor(0.3828, grad_fn=<DivBackward0>)\n",
      "i batch :  23   !!loss : tensor(0.7535, grad_fn=<DivBackward0>)\n",
      "i batch :  24   !!loss : tensor(0.2151, grad_fn=<DivBackward0>)\n",
      "i batch :  25   !!loss : tensor(0.6463, grad_fn=<DivBackward0>)\n",
      "i batch :  26   !!loss : tensor(0.6286, grad_fn=<DivBackward0>)\n",
      "i batch :  27   !!loss : tensor(0.7101, grad_fn=<DivBackward0>)\n",
      "i batch :  28   !!loss : tensor(0.5337, grad_fn=<DivBackward0>)\n",
      "i batch :  29   !!loss : tensor(0.6170, grad_fn=<DivBackward0>)\n",
      "i batch :  30   !!loss : tensor(0.4356, grad_fn=<DivBackward0>)\n",
      "i batch :  31   !!loss : tensor(1.3417, grad_fn=<DivBackward0>)\n",
      "i batch :  32   !!loss : tensor(0.9166, grad_fn=<DivBackward0>)\n",
      "i batch :  33   !!loss : tensor(0.7648, grad_fn=<DivBackward0>)\n",
      "i batch :  34   !!loss : tensor(1.5946, grad_fn=<DivBackward0>)\n",
      "i batch :  35   !!loss : tensor(0.8976, grad_fn=<DivBackward0>)\n",
      "i batch :  36   !!loss : tensor(0.6467, grad_fn=<DivBackward0>)\n",
      "i batch :  37   !!loss : tensor(1.0103, grad_fn=<DivBackward0>)\n",
      "i batch :  38   !!loss : tensor(1.1798, grad_fn=<DivBackward0>)\n",
      "i batch :  39   !!loss : tensor(1.3733, grad_fn=<DivBackward0>)\n",
      "i batch :  40   !!loss : tensor(0.9524, grad_fn=<DivBackward0>)\n",
      "i batch :  41   !!loss : tensor(0.9312, grad_fn=<DivBackward0>)\n",
      "i batch :  42   !!loss : tensor(0.7944, grad_fn=<DivBackward0>)\n",
      "i batch :  43   !!loss : tensor(0.9444, grad_fn=<DivBackward0>)\n",
      "i batch :  44   !!loss : tensor(0.4513, grad_fn=<DivBackward0>)\n",
      "i batch :  45   !!loss : tensor(1.1151, grad_fn=<DivBackward0>)\n",
      "i batch :  46   !!loss : tensor(0.4511, grad_fn=<DivBackward0>)\n",
      "i batch :  47   !!loss : tensor(0.5524, grad_fn=<DivBackward0>)\n",
      "i batch :  48   !!loss : tensor(0.6835, grad_fn=<DivBackward0>)\n",
      "i batch :  49   !!loss : tensor(0.8586, grad_fn=<DivBackward0>)\n",
      "i batch :  50   !!loss : tensor(0.9078, grad_fn=<DivBackward0>)\n",
      "i batch :  51   !!loss : tensor(0.5303, grad_fn=<DivBackward0>)\n",
      "i batch :  52   !!loss : tensor(0.5487, grad_fn=<DivBackward0>)\n",
      "i batch :  53   !!loss : tensor(0.3959, grad_fn=<DivBackward0>)\n",
      "i batch :  54   !!loss : tensor(0.3456, grad_fn=<DivBackward0>)\n",
      "i batch :  55   !!loss : tensor(0.4683, grad_fn=<DivBackward0>)\n",
      "i batch :  56   !!loss : tensor(0.6818, grad_fn=<DivBackward0>)\n",
      "i batch :  57   !!loss : tensor(0.3774, grad_fn=<DivBackward0>)\n",
      "i batch :  58   !!loss : tensor(1.1314, grad_fn=<DivBackward0>)\n",
      "i batch :  59   !!loss : tensor(0.7105, grad_fn=<DivBackward0>)\n",
      "i batch :  60   !!loss : tensor(1.0919, grad_fn=<DivBackward0>)\n",
      "i batch :  61   !!loss : tensor(0.9129, grad_fn=<DivBackward0>)\n",
      "i batch :  62   !!loss : tensor(0.7830, grad_fn=<DivBackward0>)\n",
      "i batch :  63   !!loss : tensor(1.0577, grad_fn=<DivBackward0>)\n",
      "i batch :  64   !!loss : tensor(0.6286, grad_fn=<DivBackward0>)\n",
      "i batch :  65   !!loss : tensor(0.8527, grad_fn=<DivBackward0>)\n",
      "i batch :  66   !!loss : tensor(0.8036, grad_fn=<DivBackward0>)\n",
      "i batch :  67   !!loss : tensor(1.0275, grad_fn=<DivBackward0>)\n",
      "i batch :  68   !!loss : tensor(1.1378, grad_fn=<DivBackward0>)\n",
      "i batch :  69   !!loss : tensor(1.1596, grad_fn=<DivBackward0>)\n",
      "i batch :  70   !!loss : tensor(0.7737, grad_fn=<DivBackward0>)\n",
      "i batch :  71   !!loss : tensor(0.6669, grad_fn=<DivBackward0>)\n",
      "i batch :  72   !!loss : tensor(1.0612, grad_fn=<DivBackward0>)\n",
      "i batch :  73   !!loss : tensor(0.9529, grad_fn=<DivBackward0>)\n",
      "i batch :  74   !!loss : tensor(0.8864, grad_fn=<DivBackward0>)\n",
      "i batch :  75   !!loss : tensor(0.5180, grad_fn=<DivBackward0>)\n",
      "i batch :  76   !!loss : tensor(0.8537, grad_fn=<DivBackward0>)\n",
      "i batch :  77   !!loss : tensor(0.9349, grad_fn=<DivBackward0>)\n",
      "i batch :  78   !!loss : tensor(0.4454, grad_fn=<DivBackward0>)\n",
      "i batch :  79   !!loss : tensor(0.9950, grad_fn=<DivBackward0>)\n",
      "i batch :  80   !!loss : tensor(0.6308, grad_fn=<DivBackward0>)\n",
      "i batch :  81   !!loss : tensor(0.7493, grad_fn=<DivBackward0>)\n",
      "i batch :  82   !!loss : tensor(0.9742, grad_fn=<DivBackward0>)\n",
      "i batch :  83   !!loss : tensor(0.2551, grad_fn=<DivBackward0>)\n",
      "i batch :  84   !!loss : tensor(1.5304, grad_fn=<DivBackward0>)\n",
      "i batch :  85   !!loss : tensor(1.2433, grad_fn=<DivBackward0>)\n",
      "i batch :  86   !!loss : tensor(1.3261, grad_fn=<DivBackward0>)\n",
      "i batch :  87   !!loss : tensor(0.7144, grad_fn=<DivBackward0>)\n",
      "i batch :  88   !!loss : tensor(0.5129, grad_fn=<DivBackward0>)\n",
      "i batch :  89   !!loss : tensor(0.8681, grad_fn=<DivBackward0>)\n",
      "i batch :  90   !!loss : tensor(1.3792, grad_fn=<DivBackward0>)\n",
      "i batch :  91   !!loss : tensor(0.7602, grad_fn=<DivBackward0>)\n",
      "i batch :  92   !!loss : tensor(1.2927, grad_fn=<DivBackward0>)\n",
      "i batch :  93   !!loss : tensor(0.8840, grad_fn=<DivBackward0>)\n",
      "i batch :  94   !!loss : tensor(0.7418, grad_fn=<DivBackward0>)\n",
      "i batch :  95   !!loss : tensor(1.2028, grad_fn=<DivBackward0>)\n",
      "i batch :  96   !!loss : tensor(0.8812, grad_fn=<DivBackward0>)\n",
      "i batch :  97   !!loss : tensor(1.3477, grad_fn=<DivBackward0>)\n",
      "i batch :  98   !!loss : tensor(0.4665, grad_fn=<DivBackward0>)\n",
      "i batch :  99   !!loss : tensor(0.9075, grad_fn=<DivBackward0>)\n",
      "i batch :  100   !!loss : tensor(0.7936, grad_fn=<DivBackward0>)\n",
      "i batch :  101   !!loss : tensor(0.8990, grad_fn=<DivBackward0>)\n",
      "i batch :  102   !!loss : tensor(0.6796, grad_fn=<DivBackward0>)\n",
      "i batch :  103   !!loss : tensor(0.8333, grad_fn=<DivBackward0>)\n",
      "i batch :  104   !!loss : tensor(1.3338, grad_fn=<DivBackward0>)\n",
      "i batch :  105   !!loss : tensor(1.1532, grad_fn=<DivBackward0>)\n",
      "i batch :  106   !!loss : tensor(0.6096, grad_fn=<DivBackward0>)\n",
      "i batch :  107   !!loss : tensor(1.0126, grad_fn=<DivBackward0>)\n",
      "i batch :  108   !!loss : tensor(0.4890, grad_fn=<DivBackward0>)\n",
      "i batch :  109   !!loss : tensor(1.0476, grad_fn=<DivBackward0>)\n",
      "i batch :  110   !!loss : tensor(0.6689, grad_fn=<DivBackward0>)\n",
      "i batch :  111   !!loss : tensor(0.6660, grad_fn=<DivBackward0>)\n",
      "i batch :  112   !!loss : tensor(0.7872, grad_fn=<DivBackward0>)\n",
      "i batch :  113   !!loss : tensor(0.9134, grad_fn=<DivBackward0>)\n",
      "i batch :  114   !!loss : tensor(0.6848, grad_fn=<DivBackward0>)\n",
      "i batch :  115   !!loss : tensor(0.0009, grad_fn=<DivBackward0>)\n",
      "Epoch :  14\n",
      "i batch :  0   !!loss : tensor(0.6085, grad_fn=<DivBackward0>)\n",
      "i batch :  1   !!loss : tensor(1.2569, grad_fn=<DivBackward0>)\n",
      "i batch :  2   !!loss : tensor(0.3159, grad_fn=<DivBackward0>)\n",
      "i batch :  3   !!loss : tensor(0.6768, grad_fn=<DivBackward0>)\n",
      "i batch :  4   !!loss : tensor(0.8887, grad_fn=<DivBackward0>)\n",
      "i batch :  5   !!loss : tensor(0.7856, grad_fn=<DivBackward0>)\n",
      "i batch :  6   !!loss : tensor(0.8703, grad_fn=<DivBackward0>)\n",
      "i batch :  7   !!loss : tensor(0.6926, grad_fn=<DivBackward0>)\n",
      "i batch :  8   !!loss : tensor(0.7825, grad_fn=<DivBackward0>)\n",
      "i batch :  9   !!loss : tensor(1.0618, grad_fn=<DivBackward0>)\n",
      "i batch :  10   !!loss : tensor(0.7276, grad_fn=<DivBackward0>)\n",
      "i batch :  11   !!loss : tensor(0.9390, grad_fn=<DivBackward0>)\n",
      "i batch :  12   !!loss : tensor(0.7806, grad_fn=<DivBackward0>)\n",
      "i batch :  13   !!loss : tensor(0.8275, grad_fn=<DivBackward0>)\n",
      "i batch :  14   !!loss : tensor(0.5570, grad_fn=<DivBackward0>)\n",
      "i batch :  15   !!loss : tensor(0.7296, grad_fn=<DivBackward0>)\n",
      "i batch :  16   !!loss : tensor(0.7711, grad_fn=<DivBackward0>)\n",
      "i batch :  17   !!loss : tensor(0.3743, grad_fn=<DivBackward0>)\n",
      "i batch :  18   !!loss : tensor(0.6782, grad_fn=<DivBackward0>)\n",
      "i batch :  19   !!loss : tensor(1.3970, grad_fn=<DivBackward0>)\n",
      "i batch :  20   !!loss : tensor(0.6019, grad_fn=<DivBackward0>)\n",
      "i batch :  21   !!loss : tensor(0.4352, grad_fn=<DivBackward0>)\n",
      "i batch :  22   !!loss : tensor(0.5091, grad_fn=<DivBackward0>)\n",
      "i batch :  23   !!loss : tensor(0.8724, grad_fn=<DivBackward0>)\n",
      "i batch :  24   !!loss : tensor(0.8969, grad_fn=<DivBackward0>)\n",
      "i batch :  25   !!loss : tensor(0.7127, grad_fn=<DivBackward0>)\n",
      "i batch :  26   !!loss : tensor(0.6120, grad_fn=<DivBackward0>)\n",
      "i batch :  27   !!loss : tensor(0.7506, grad_fn=<DivBackward0>)\n",
      "i batch :  28   !!loss : tensor(0.9310, grad_fn=<DivBackward0>)\n",
      "i batch :  29   !!loss : tensor(0.6656, grad_fn=<DivBackward0>)\n",
      "i batch :  30   !!loss : tensor(0.7441, grad_fn=<DivBackward0>)\n",
      "i batch :  31   !!loss : tensor(1.2313, grad_fn=<DivBackward0>)\n",
      "i batch :  32   !!loss : tensor(0.4901, grad_fn=<DivBackward0>)\n",
      "i batch :  33   !!loss : tensor(0.5336, grad_fn=<DivBackward0>)\n",
      "i batch :  34   !!loss : tensor(0.8091, grad_fn=<DivBackward0>)\n",
      "i batch :  35   !!loss : tensor(0.9985, grad_fn=<DivBackward0>)\n",
      "i batch :  36   !!loss : tensor(0.5654, grad_fn=<DivBackward0>)\n",
      "i batch :  37   !!loss : tensor(0.8022, grad_fn=<DivBackward0>)\n",
      "i batch :  38   !!loss : tensor(0.6995, grad_fn=<DivBackward0>)\n",
      "i batch :  39   !!loss : tensor(1.2605, grad_fn=<DivBackward0>)\n",
      "i batch :  40   !!loss : tensor(0.5219, grad_fn=<DivBackward0>)\n",
      "i batch :  41   !!loss : tensor(0.9776, grad_fn=<DivBackward0>)\n",
      "i batch :  42   !!loss : tensor(0.9397, grad_fn=<DivBackward0>)\n",
      "i batch :  43   !!loss : tensor(1.0998, grad_fn=<DivBackward0>)\n",
      "i batch :  44   !!loss : tensor(0.9868, grad_fn=<DivBackward0>)\n",
      "i batch :  45   !!loss : tensor(0.6624, grad_fn=<DivBackward0>)\n",
      "i batch :  46   !!loss : tensor(1.0384, grad_fn=<DivBackward0>)\n",
      "i batch :  47   !!loss : tensor(0.7976, grad_fn=<DivBackward0>)\n",
      "i batch :  48   !!loss : tensor(0.6253, grad_fn=<DivBackward0>)\n",
      "i batch :  49   !!loss : tensor(0.5109, grad_fn=<DivBackward0>)\n",
      "i batch :  50   !!loss : tensor(1.5040, grad_fn=<DivBackward0>)\n",
      "i batch :  51   !!loss : tensor(0.4485, grad_fn=<DivBackward0>)\n",
      "i batch :  52   !!loss : tensor(0.4178, grad_fn=<DivBackward0>)\n",
      "i batch :  53   !!loss : tensor(0.8579, grad_fn=<DivBackward0>)\n",
      "i batch :  54   !!loss : tensor(1.0001, grad_fn=<DivBackward0>)\n",
      "i batch :  55   !!loss : tensor(0.7281, grad_fn=<DivBackward0>)\n",
      "i batch :  56   !!loss : tensor(0.5160, grad_fn=<DivBackward0>)\n",
      "i batch :  57   !!loss : tensor(0.4359, grad_fn=<DivBackward0>)\n",
      "i batch :  58   !!loss : tensor(1.4621, grad_fn=<DivBackward0>)\n",
      "i batch :  59   !!loss : tensor(0.7352, grad_fn=<DivBackward0>)\n",
      "i batch :  60   !!loss : tensor(0.7397, grad_fn=<DivBackward0>)\n",
      "i batch :  61   !!loss : tensor(0.5508, grad_fn=<DivBackward0>)\n",
      "i batch :  62   !!loss : tensor(0.6013, grad_fn=<DivBackward0>)\n",
      "i batch :  63   !!loss : tensor(0.8457, grad_fn=<DivBackward0>)\n",
      "i batch :  64   !!loss : tensor(0.8083, grad_fn=<DivBackward0>)\n",
      "i batch :  65   !!loss : tensor(0.5510, grad_fn=<DivBackward0>)\n",
      "i batch :  66   !!loss : tensor(0.5856, grad_fn=<DivBackward0>)\n",
      "i batch :  67   !!loss : tensor(0.8812, grad_fn=<DivBackward0>)\n",
      "i batch :  68   !!loss : tensor(0.6266, grad_fn=<DivBackward0>)\n",
      "i batch :  69   !!loss : tensor(0.4279, grad_fn=<DivBackward0>)\n",
      "i batch :  70   !!loss : tensor(0.5214, grad_fn=<DivBackward0>)\n",
      "i batch :  71   !!loss : tensor(1.0520, grad_fn=<DivBackward0>)\n",
      "i batch :  72   !!loss : tensor(0.9066, grad_fn=<DivBackward0>)\n",
      "i batch :  73   !!loss : tensor(0.5052, grad_fn=<DivBackward0>)\n",
      "i batch :  74   !!loss : tensor(0.7128, grad_fn=<DivBackward0>)\n",
      "i batch :  75   !!loss : tensor(0.9160, grad_fn=<DivBackward0>)\n",
      "i batch :  76   !!loss : tensor(0.6675, grad_fn=<DivBackward0>)\n",
      "i batch :  77   !!loss : tensor(0.7951, grad_fn=<DivBackward0>)\n",
      "i batch :  78   !!loss : tensor(0.9088, grad_fn=<DivBackward0>)\n",
      "i batch :  79   !!loss : tensor(0.9446, grad_fn=<DivBackward0>)\n",
      "i batch :  80   !!loss : tensor(0.8385, grad_fn=<DivBackward0>)\n",
      "i batch :  81   !!loss : tensor(0.6932, grad_fn=<DivBackward0>)\n",
      "i batch :  82   !!loss : tensor(0.5389, grad_fn=<DivBackward0>)\n",
      "i batch :  83   !!loss : tensor(0.7348, grad_fn=<DivBackward0>)\n",
      "i batch :  84   !!loss : tensor(1.0852, grad_fn=<DivBackward0>)\n",
      "i batch :  85   !!loss : tensor(0.5398, grad_fn=<DivBackward0>)\n",
      "i batch :  86   !!loss : tensor(0.8742, grad_fn=<DivBackward0>)\n",
      "i batch :  87   !!loss : tensor(0.9851, grad_fn=<DivBackward0>)\n",
      "i batch :  88   !!loss : tensor(0.5344, grad_fn=<DivBackward0>)\n",
      "i batch :  89   !!loss : tensor(0.4966, grad_fn=<DivBackward0>)\n",
      "i batch :  90   !!loss : tensor(0.8349, grad_fn=<DivBackward0>)\n",
      "i batch :  91   !!loss : tensor(1.0181, grad_fn=<DivBackward0>)\n",
      "i batch :  92   !!loss : tensor(0.7585, grad_fn=<DivBackward0>)\n",
      "i batch :  93   !!loss : tensor(0.5327, grad_fn=<DivBackward0>)\n",
      "i batch :  94   !!loss : tensor(0.7228, grad_fn=<DivBackward0>)\n",
      "i batch :  95   !!loss : tensor(0.7863, grad_fn=<DivBackward0>)\n",
      "i batch :  96   !!loss : tensor(0.8512, grad_fn=<DivBackward0>)\n",
      "i batch :  97   !!loss : tensor(0.4561, grad_fn=<DivBackward0>)\n",
      "i batch :  98   !!loss : tensor(1.1778, grad_fn=<DivBackward0>)\n",
      "i batch :  99   !!loss : tensor(0.7431, grad_fn=<DivBackward0>)\n",
      "i batch :  100   !!loss : tensor(0.9203, grad_fn=<DivBackward0>)\n",
      "i batch :  101   !!loss : tensor(0.9772, grad_fn=<DivBackward0>)\n",
      "i batch :  102   !!loss : tensor(1.1929, grad_fn=<DivBackward0>)\n",
      "i batch :  103   !!loss : tensor(0.4175, grad_fn=<DivBackward0>)\n",
      "i batch :  104   !!loss : tensor(0.5355, grad_fn=<DivBackward0>)\n",
      "i batch :  105   !!loss : tensor(0.9135, grad_fn=<DivBackward0>)\n",
      "i batch :  106   !!loss : tensor(0.6644, grad_fn=<DivBackward0>)\n",
      "i batch :  107   !!loss : tensor(0.2362, grad_fn=<DivBackward0>)\n",
      "i batch :  108   !!loss : tensor(0.5052, grad_fn=<DivBackward0>)\n",
      "i batch :  109   !!loss : tensor(0.6413, grad_fn=<DivBackward0>)\n",
      "i batch :  110   !!loss : tensor(0.4055, grad_fn=<DivBackward0>)\n",
      "i batch :  111   !!loss : tensor(0.8924, grad_fn=<DivBackward0>)\n",
      "i batch :  112   !!loss : tensor(0.6212, grad_fn=<DivBackward0>)\n",
      "i batch :  113   !!loss : tensor(0.6721, grad_fn=<DivBackward0>)\n",
      "i batch :  114   !!loss : tensor(1.0808, grad_fn=<DivBackward0>)\n",
      "i batch :  115   !!loss : tensor(0., grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import tqdm \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "print('out device :',device)\n",
    "\n",
    "\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "\n",
    "\n",
    "if device == 'cpu':\n",
    "    model.float()\n",
    "else :\n",
    "    clip.model.convert_weights(model) # Actually this line is unnecessary since clip by default already on float16\n",
    "\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) #Params used from paper, the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "EPOCH = 15\n",
    "# add your own code to track the training progress.\n",
    "for epoch in range(EPOCH):\n",
    "    print(\"Epoch : \",epoch)\n",
    "    for i,batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images,texts = batch \n",
    "\n",
    "        images= images.to(device)\n",
    "        texts = texts.to(device)\n",
    "        \n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "        # tensor.size(batch_size, batch_size)\n",
    "        \n",
    "        ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "        # tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  ....\n",
    "        \n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "        total_loss.backward()\n",
    "        print('i batch : ',i,'  !!loss :',total_loss)\n",
    "        if device == 'cpu':\n",
    "            optimizer.step()\n",
    "        else : \n",
    "            convert_models_to_fp32(model)\n",
    "            optimizer.step()\n",
    "            clip.model.convert_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved file\n"
     ]
    }
   ],
   "source": [
    "print('saved file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': total_loss,\n",
    "        }, f\"model_{EPOCH}.pt\") #just change to your preferred folder/filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "checkpoint = torch.load(\"model_10.pt\")\n",
    "\n",
    "# Use these 3 lines if you use default model setting(not training setting) of the clip. For example, if you set context_length to 100 since your string is very long during training, then assign 100 to checkpoint['model_state_dict'][\"context_length\"] \n",
    "# checkpoint['model_state_dict'][\"input_resolution\"] = model.input_resolution #default is 224\n",
    "# checkpoint['model_state_dict'][\"context_length\"] = model.context_length # default is 77\n",
    "# checkpoint['model_state_dict'][\"vocab_size\"] = model.vocab_size \n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7fe5bd629d60>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss function modify\n",
    "\n",
    "## cross entropy -> contrastive learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out device : cpu\n",
      "0\n",
      "torch.Size([16, 3, 224, 224]) torch.Size([16, 77])\n",
      "i batch :  0   !!loss : tensor(0.1016, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import tqdm \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "print('out device :',device)\n",
    "\n",
    "\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "\n",
    "\n",
    "if device == 'cpu':\n",
    "    model.float()\n",
    "else :\n",
    "    clip.model.convert_weights(model) # Actually this line is unnecessary since clip by default already on float16\n",
    "\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) #Params used from paper, the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "EPOCH = 15\n",
    "# add your own code to track the training progress.\n",
    "# for epoch in range(EPOCH):\n",
    "    # print(\"Epoch : \",epoch)\n",
    "for i,batch in enumerate(train_dataloader):\n",
    "    print(i)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    images,texts = batch \n",
    "    print(images.shape,texts.shape)\n",
    "    images= images.to(device)\n",
    "    texts = texts.to(device)\n",
    "\n",
    "    logits_per_image, logits_per_text = model(images, texts)\n",
    "    # tensor.size(batch_size, batch_size) bvgty56 \n",
    "    # ./,...........''.,5\n",
    "    \n",
    "    ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "    # tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  ....\n",
    "    \n",
    "    total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "    total_loss.backward()\n",
    "    print('i batch : ',i,'  !!loss :',total_loss)\n",
    "    if device == 'cpu':\n",
    "        optimizer.step()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 3, 0])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "counts = 0\n",
    "for i in range(16):\n",
    "    for j in range(16):\n",
    "        if logits_per_image[i][j]==logits_per_text.T[i][j]:\n",
    "            counts+=1\n",
    "        \n",
    "            # print(i,j)\n",
    "\n",
    "print(counts)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "veda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
