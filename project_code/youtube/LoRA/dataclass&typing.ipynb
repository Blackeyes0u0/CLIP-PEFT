{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataclass를 써야하는 이유를 알아보자!\n",
    "\n",
    "https://www.daleseo.com/python-dataclasses/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "class User:\n",
    "    def __init__(\n",
    "        self, id: int, name: str, birthdate: date, admin: bool = False\n",
    "    ) -> None:\n",
    "        self.id = id\n",
    "        self.name = name\n",
    "        self.birthdate = birthdate\n",
    "        self.admin = admin\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            self.__class__.__qualname__ + f\"(id={self.id!r}, name={self.name!r}, \"\n",
    "            f\"birthdate={self.birthdate!r}, admin={self.admin!r})\"\n",
    "        )\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if other.__class__ is self.__class__:\n",
    "            return (self.id, self.name, self.birthdate, self.admin) == (\n",
    "                other.id,\n",
    "                other.name,\n",
    "                other.birthdate,\n",
    "                other.admin,\n",
    "            )\n",
    "        return NotImplemented\n",
    "\n",
    "user1 = User(id=1, name=\"Steve Jobs\", birthdate=date(1955, 2, 24))\n",
    "user2 = User(id=1, name=\"Steve Jobs\", birthdate=date(1955, 2, 24))\n",
    "user1==user2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class User:\n",
    "    id: int\n",
    "    name: str\n",
    "    birthdate: date\n",
    "    admin: bool = False\n",
    "    \n",
    "user1 = User(id=1, name=\"Steve Jobs\", birthdate=date(1955, 2, 24))\n",
    "user2 = User(id=1, name=\"Steve Jobs\", birthdate=date(1955, 2, 24))\n",
    "user1==user2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## typing에 대해서 알아보자\n",
    "\n",
    "\n",
    "타입 어노테이션을 사용하다 보면 리스트, 사전, 튜플, 세트와 같은 파이썬 내장 자료구조에 대한 타입을 명시해야 할 때가 있습니다. 이럴 때는 typing 모듈에서 제공하는 List, Dict, Tuple, Set를 사용하여 타입 어노테이션을 추가하면 됩니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 'Dale', True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dict\n",
    "from typing import Dict\n",
    "countries: Dict[str, str] = {\"KR\": \"South Korea\", \"US\": \"United States\", \"CN\": \"China\"}\n",
    "\n",
    "# List\n",
    "from typing import List\n",
    "nums: List[int] = [1, 2, 3]\n",
    "\n",
    "\n",
    "# Tuple\n",
    "from typing import Tuple\n",
    "user: Tuple[int, str, bool] = (3, \"Dale\", True)\n",
    "\n",
    "user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Optional\n",
    "\n",
    "typing 모듈의 Optional은 None이 허용되는 함수의 매개 변수에 대한 타입을 명시할 때 유용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def repeat(message: str, times: Optional[int] = None) -> list:\n",
    "    if times:\n",
    "        return [message] * times\n",
    "    else:\n",
    "        return [message]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 4096\n",
    "    n_layers: int = 32\n",
    "    n_heads: int = 32\n",
    "    n_kv_heads: Optional[int] = None\n",
    "    vocab_size: int = -1  # defined later by tokenizer\n",
    "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    ffn_dim_multiplier: Optional[float] = None\n",
    "    norm_eps: float = 1e-5\n",
    "\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 2048\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Multi-head attention module.\"\"\"\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        \"\"\"\n",
    "        Initialize the Attention module.\n",
    "\n",
    "        Args:\n",
    "            args (ModelArgs): Model configuration parameters.\n",
    "\n",
    "        Attributes:\n",
    "            n_kv_heads (int): Number of key and value heads.\n",
    "            n_local_heads (int): Number of local query heads.\n",
    "            n_local_kv_heads (int): Number of local key and value heads.\n",
    "            n_rep (int): Number of repetitions for local heads.\n",
    "            head_dim (int): Dimension size of each attention head.\n",
    "            wq (ColumnParallelLinear): Linear transformation for queries.\n",
    "            wk (ColumnParallelLinear): Linear transformation for keys.\n",
    "            wv (ColumnParallelLinear): Linear transformation for values.\n",
    "            wo (RowParallelLinear): Linear transformation for output.\n",
    "            cache_k (torch.Tensor): Cached keys for attention.\n",
    "            cache_v (torch.Tensor): Cached values for attention.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        model_parallel_size = fs_init.get_model_parallel_world_size()\n",
    "        self.n_local_heads = args.n_heads // model_parallel_size\n",
    "        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size\n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        self.wq = ColumnParallelLinear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "            gather_output=False,\n",
    "            init_method=lambda x: x,\n",
    "        )\n",
    "        self.wk = ColumnParallelLinear(\n",
    "            args.dim,\n",
    "            self.n_kv_heads * self.head_dim,\n",
    "            bias=False,\n",
    "            gather_output=False,\n",
    "            init_method=lambda x: x,\n",
    "        )\n",
    "        self.wv = ColumnParallelLinear(\n",
    "            args.dim,\n",
    "            self.n_kv_heads * self.head_dim,\n",
    "            bias=False,\n",
    "            gather_output=False,\n",
    "            init_method=lambda x: x,\n",
    "        )\n",
    "        self.wo = RowParallelLinear(\n",
    "            args.n_heads * self.head_dim,\n",
    "            args.dim,\n",
    "            bias=False,\n",
    "            input_is_parallel=True,\n",
    "            init_method=lambda x: x,\n",
    "        )\n",
    "\n",
    "        self.cache_k = torch.zeros(\n",
    "            (\n",
    "                args.max_batch_size,\n",
    "                args.max_seq_len,\n",
    "                self.n_local_kv_heads,\n",
    "                self.head_dim,\n",
    "            )\n",
    "        ).cuda()\n",
    "        self.cache_v = torch.zeros(\n",
    "            (\n",
    "                args.max_batch_size,\n",
    "                args.max_seq_len,\n",
    "                self.n_local_kv_heads,\n",
    "                self.head_dim,\n",
    "            )\n",
    "        ).cuda()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        start_pos: int,\n",
    "        freqs_cis: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Forward pass of the attention module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            start_pos (int): Starting position for caching.\n",
    "            freqs_cis (torch.Tensor): Precomputed frequency tensor.\n",
    "            mask (torch.Tensor, optional): Attention mask tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after attention.\n",
    "\n",
    "        \"\"\"\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "\n",
    "        self.cache_k = self.cache_k.to(xq)\n",
    "        self.cache_v = self.cache_v.to(xq)\n",
    "\n",
    "        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\n",
    "        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv\n",
    "\n",
    "        keys = self.cache_k[:bsz, : start_pos + seqlen]\n",
    "        values = self.cache_v[:bsz, : start_pos + seqlen]\n",
    "\n",
    "        # repeat k/v heads if n_kv_heads < n_heads\n",
    "        keys = repeat_kv(keys, self.n_rep)  # (bs, seqlen, n_local_heads, head_dim)\n",
    "        values = repeat_kv(values, self.n_rep)  # (bs, seqlen, n_local_heads, head_dim)\n",
    "\n",
    "        xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores + mask  # (bs, n_local_heads, seqlen, cache_len + seqlen)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        output = torch.matmul(scores, values)  # (bs, n_local_heads, seqlen, head_dim)\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "        return self.wo(output)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        multiple_of: int,\n",
    "        ffn_dim_multiplier: Optional[float],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the FeedForward module.\n",
    "\n",
    "        Args:\n",
    "            dim (int): Input dimension.\n",
    "            hidden_dim (int): Hidden dimension of the feedforward layer.\n",
    "            multiple_of (int): Value to ensure hidden dimension is a multiple of this value.\n",
    "            ffn_dim_multiplier (float, optional): Custom multiplier for hidden dimension. Defaults to None.\n",
    "\n",
    "        Attributes:\n",
    "            w1 (ColumnParallelLinear): Linear transformation for the first layer.\n",
    "            w2 (RowParallelLinear): Linear transformation for the second layer.\n",
    "            w3 (ColumnParallelLinear): Linear transformation for the third layer.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        # custom dim factor multiplier\n",
    "        if ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        self.w1 = ColumnParallelLinear(\n",
    "            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x\n",
    "        )\n",
    "        self.w2 = RowParallelLinear(\n",
    "            hidden_dim, dim, bias=False, input_is_parallel=True, init_method=lambda x: x\n",
    "        )\n",
    "        self.w3 = ColumnParallelLinear(\n",
    "            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, params: ModelArgs):\n",
    "        \"\"\"\n",
    "        Initialize a Transformer model.\n",
    "\n",
    "        Args:\n",
    "            params (ModelArgs): Model configuration parameters.\n",
    "\n",
    "        Attributes:\n",
    "            params (ModelArgs): Model configuration parameters.\n",
    "            vocab_size (int): Vocabulary size.\n",
    "            n_layers (int): Number of layers in the model.\n",
    "            tok_embeddings (ParallelEmbedding): Token embeddings.\n",
    "            layers (torch.nn.ModuleList): List of Transformer blocks.\n",
    "            norm (RMSNorm): Layer normalization for the model output.\n",
    "            output (ColumnParallelLinear): Linear layer for final output.\n",
    "            freqs_cis (torch.Tensor): Precomputed cosine and sine frequencies.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.vocab_size = params.vocab_size\n",
    "        self.n_layers = params.n_layers\n",
    "\n",
    "        self.tok_embeddings = ParallelEmbedding(\n",
    "            params.vocab_size, params.dim, init_method=lambda x: x\n",
    "        )\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_id in range(params.n_layers):\n",
    "            self.layers.append(TransformerBlock(layer_id, params))\n",
    "\n",
    "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
    "        self.output = ColumnParallelLinear(\n",
    "            params.dim, params.vocab_size, bias=False, init_method=lambda x: x\n",
    "        )\n",
    "\n",
    "        self.freqs_cis = precompute_freqs_cis(\n",
    "            # Note that self.params.max_seq_len is multiplied by 2 because the token limit for the Llama 2 generation of models is 4096. \n",
    "            # Adding this multiplier instead of using 4096 directly allows for dynamism of token lengths while training or fine-tuning.\n",
    "            self.params.dim // self.params.n_heads, self.params.max_seq_len * 2\n",
    "        )\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the Transformer model.\n",
    "\n",
    "        Args:\n",
    "            tokens (torch.Tensor): Input token indices.\n",
    "            start_pos (int): Starting position for attention caching.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits after applying the Transformer model.\n",
    "\n",
    "        \"\"\"\n",
    "        _bsz, seqlen = tokens.shape\n",
    "        h = self.tok_embeddings(tokens)\n",
    "        self.freqs_cis = self.freqs_cis.to(h.device)\n",
    "        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n",
    "\n",
    "        mask = None\n",
    "        if seqlen > 1:\n",
    "            mask = torch.full(\n",
    "                (1, 1, seqlen, seqlen), float(\"-inf\"), device=tokens.device\n",
    "            )\n",
    "            mask = torch.triu(mask, diagonal=start_pos + 1).type_as(h)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_cis, mask)\n",
    "        h = self.norm(h)\n",
    "        output = self.output(h).float()\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "veda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
