{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 positional_embedding\n",
      "torch.Size([77, 512])\n",
      "1 text_projection\n",
      "torch.Size([512, 512])\n",
      "2 logit_scale\n",
      "torch.Size([])\n",
      "3 visual.class_embedding\n",
      "torch.Size([768])\n",
      "4 visual.positional_embedding\n",
      "torch.Size([50, 768])\n",
      "5 visual.proj\n",
      "torch.Size([768, 512])\n",
      "6 visual.conv1.weight\n",
      "torch.Size([768, 3, 32, 32])\n",
      "7 visual.ln_pre.weight\n",
      "torch.Size([768])\n",
      "8 visual.ln_pre.bias\n",
      "torch.Size([768])\n",
      "9 visual.transformer.resblocks.0.attn.in_proj_weight\n",
      "torch.Size([2304, 768])\n",
      "10 visual.transformer.resblocks.0.attn.in_proj_bias\n",
      "torch.Size([2304])\n",
      "11 visual.transformer.resblocks.0.attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "12 visual.transformer.resblocks.0.attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "13 visual.transformer.resblocks.0.ln_1.weight\n",
      "torch.Size([768])\n",
      "14 visual.transformer.resblocks.0.ln_1.bias\n",
      "torch.Size([768])\n",
      "15 visual.transformer.resblocks.0.mlp.c_fc.weight\n",
      "torch.Size([3072, 768])\n",
      "16 visual.transformer.resblocks.0.mlp.c_fc.bias\n",
      "torch.Size([3072])\n",
      "17 visual.transformer.resblocks.0.mlp.c_proj.weight\n",
      "torch.Size([768, 3072])\n",
      "18 visual.transformer.resblocks.0.mlp.c_proj.bias\n",
      "torch.Size([768])\n",
      "19 visual.transformer.resblocks.0.ln_2.weight\n",
      "torch.Size([768])\n",
      "20 visual.transformer.resblocks.0.ln_2.bias\n",
      "torch.Size([768])\n",
      "21 visual.transformer.resblocks.1.attn.in_proj_weight\n",
      "torch.Size([2304, 768])\n",
      "22 visual.transformer.resblocks.1.attn.in_proj_bias\n",
      "torch.Size([2304])\n",
      "23 visual.transformer.resblocks.1.attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "24 visual.transformer.resblocks.1.attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "25 visual.transformer.resblocks.1.ln_1.weight\n",
      "torch.Size([768])\n",
      "26 visual.transformer.resblocks.1.ln_1.bias\n",
      "torch.Size([768])\n",
      "27 visual.transformer.resblocks.1.mlp.c_fc.weight\n",
      "torch.Size([3072, 768])\n",
      "28 visual.transformer.resblocks.1.mlp.c_fc.bias\n",
      "torch.Size([3072])\n",
      "29 visual.transformer.resblocks.1.mlp.c_proj.weight\n",
      "torch.Size([768, 3072])\n",
      "30 visual.transformer.resblocks.1.mlp.c_proj.bias\n",
      "torch.Size([768])\n",
      "31 visual.transformer.resblocks.1.ln_2.weight\n",
      "torch.Size([768])\n",
      "32 visual.transformer.resblocks.1.ln_2.bias\n",
      "torch.Size([768])\n",
      "33 visual.transformer.resblocks.2.attn.in_proj_weight\n",
      "torch.Size([2304, 768])\n",
      "34 visual.transformer.resblocks.2.attn.in_proj_bias\n",
      "torch.Size([2304])\n",
      "35 visual.transformer.resblocks.2.attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "36 visual.transformer.resblocks.2.attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "37 visual.transformer.resblocks.2.ln_1.weight\n",
      "torch.Size([768])\n",
      "38 visual.transformer.resblocks.2.ln_1.bias\n",
      "torch.Size([768])\n",
      "39 visual.transformer.resblocks.2.mlp.c_fc.weight\n",
      "torch.Size([3072, 768])\n",
      "40 visual.transformer.resblocks.2.mlp.c_fc.bias\n",
      "torch.Size([3072])\n",
      "41 visual.transformer.resblocks.2.mlp.c_proj.weight\n",
      "torch.Size([768, 3072])\n",
      "42 visual.transformer.resblocks.2.mlp.c_proj.bias\n",
      "torch.Size([768])\n",
      "43 visual.transformer.resblocks.2.ln_2.weight\n",
      "torch.Size([768])\n",
      "44 visual.transformer.resblocks.2.ln_2.bias\n",
      "torch.Size([768])\n",
      "45 visual.transformer.resblocks.3.attn.in_proj_weight\n",
      "torch.Size([2304, 768])\n",
      "46 visual.transformer.resblocks.3.attn.in_proj_bias\n",
      "torch.Size([2304])\n",
      "47 visual.transformer.resblocks.3.attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "48 visual.transformer.resblocks.3.attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "49 visual.transformer.resblocks.3.ln_1.weight\n",
      "torch.Size([768])\n",
      "50 visual.transformer.resblocks.3.ln_1.bias\n",
      "torch.Size([768])\n",
      "51 visual.transformer.resblocks.3.mlp.c_fc.weight\n",
      "torch.Size([3072, 768])\n",
      "52 visual.transformer.resblocks.3.mlp.c_fc.bias\n",
      "torch.Size([3072])\n",
      "53 visual.transformer.resblocks.3.mlp.c_proj.weight\n",
      "torch.Size([768, 3072])\n",
      "54 visual.transformer.resblocks.3.mlp.c_proj.bias\n",
      "torch.Size([768])\n",
      "55 visual.transformer.resblocks.3.ln_2.weight\n",
      "torch.Size([768])\n",
      "56 visual.transformer.resblocks.3.ln_2.bias\n",
      "torch.Size([768])\n",
      "57 visual.transformer.resblocks.4.attn.in_proj_weight\n",
      "torch.Size([2304, 768])\n",
      "58 visual.transformer.resblocks.4.attn.in_proj_bias\n",
      "torch.Size([2304])\n",
      "59 visual.transformer.resblocks.4.attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "60 visual.transformer.resblocks.4.attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "61 visual.transformer.resblocks.4.ln_1.weight\n",
      "torch.Size([768])\n",
      "62 visual.transformer.resblocks.4.ln_1.bias\n",
      "torch.Size([768])\n",
      "63 visual.transformer.resblocks.4.mlp.c_fc.weight\n",
      "torch.Size([3072, 768])\n",
      "64 visual.transformer.resblocks.4.mlp.c_fc.bias\n",
      "torch.Size([3072])\n",
      "65 visual.transformer.resblocks.4.mlp.c_proj.weight\n",
      "torch.Size([768, 3072])\n",
      "66 visual.transformer.resblocks.4.mlp.c_proj.bias\n",
      "torch.Size([768])\n",
      "67 visual.transformer.resblocks.4.ln_2.weight\n",
      "torch.Size([768])\n",
      "68 visual.transformer.resblocks.4.ln_2.bias\n",
      "torch.Size([768])\n",
      "69 visual.transformer.resblocks.5.attn.in_proj_weight\n",
      "torch.Size([2304, 768])\n",
      "70 visual.transformer.resblocks.5.attn.in_proj_bias\n",
      "torch.Size([2304])\n",
      "71 visual.transformer.resblocks.5.attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "72 visual.transformer.resblocks.5.attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "73 visual.transformer.resblocks.5.ln_1.weight\n",
      "torch.Size([768])\n",
      "74 visual.transformer.resblocks.5.ln_1.bias\n",
      "torch.Size([768])\n",
      "75 visual.transformer.resblocks.5.mlp.c_fc.weight\n",
      "torch.Size([3072, 768])\n",
      "76 visual.transformer.resblocks.5.mlp.c_fc.bias\n",
      "torch.Size([3072])\n",
      "77 visual.transformer.resblocks.5.mlp.c_proj.weight\n",
      "torch.Size([768, 3072])\n",
      "78 visual.transformer.resblocks.5.mlp.c_proj.bias\n",
      "torch.Size([768])\n",
      "79 visual.transformer.resblocks.5.ln_2.weight\n",
      "torch.Size([768])\n",
      "80 visual.transformer.resblocks.5.ln_2.bias\n",
      "torch.Size([768])\n",
      "81 visual.transformer.resblocks.6.attn.in_proj_weight\n",
      "torch.Size([2304, 768])\n",
      "82 visual.transformer.resblocks.6.attn.in_proj_bias\n",
      "torch.Size([2304])\n",
      "83 visual.transformer.resblocks.6.attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "84 visual.transformer.resblocks.6.attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "85 visual.transformer.resblocks.6.ln_1.weight\n",
      "torch.Size([768])\n",
      "86 visual.transformer.resblocks.6.ln_1.bias\n",
      "torch.Size([768])\n",
      "87 visual.transformer.resblocks.6.mlp.c_fc.weight\n",
      "torch.Size([3072, 768])\n",
      "88 visual.transformer.resblocks.6.mlp.c_fc.bias\n",
      "torch.Size([3072])\n",
      "89 visual.transformer.resblocks.6.mlp.c_proj.weight\n",
      "torch.Size([768, 3072])\n",
      "90 visual.transformer.resblocks.6.mlp.c_proj.bias\n",
      "torch.Size([768])\n",
      "91 visual.transformer.resblocks.6.ln_2.weight\n",
      "torch.Size([768])\n",
      "92 visual.transformer.resblocks.6.ln_2.bias\n",
      "torch.Size([768])\n",
      "93 visual.transformer.resblocks.7.attn.in_proj_weight\n",
      "torch.Size([2304, 768])\n",
      "94 visual.transformer.resblocks.7.attn.in_proj_bias\n",
      "torch.Size([2304])\n",
      "95 visual.transformer.resblocks.7.attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "96 visual.transformer.resblocks.7.attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "97 visual.transformer.resblocks.7.ln_1.weight\n",
      "torch.Size([768])\n",
      "98 visual.transformer.resblocks.7.ln_1.bias\n",
      "torch.Size([768])\n",
      "99 visual.transformer.resblocks.7.mlp.c_fc.weight\n",
      "torch.Size([3072, 768])\n",
      "100 visual.transformer.resblocks.7.mlp.c_fc.bias\n",
      "torch.Size([3072])\n",
      "101 visual.transformer.resblocks.7.mlp.c_proj.weight\n",
      "torch.Size([768, 3072])\n",
      "102 visual.transformer.resblocks.7.mlp.c_proj.bias\n",
      "torch.Size([768])\n",
      "103 visual.transformer.resblocks.7.ln_2.weight\n",
      "torch.Size([768])\n",
      "104 visual.transformer.resblocks.7.ln_2.bias\n",
      "torch.Size([768])\n",
      "105 visual.transformer.resblocks.8.attn.in_proj_weight\n",
      "torch.Size([2304, 768])\n",
      "106 visual.transformer.resblocks.8.attn.in_proj_bias\n",
      "torch.Size([2304])\n",
      "107 visual.transformer.resblocks.8.attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "108 visual.transformer.resblocks.8.attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "109 visual.transformer.resblocks.8.ln_1.weight\n",
      "torch.Size([768])\n",
      "110 visual.transformer.resblocks.8.ln_1.bias\n",
      "torch.Size([768])\n",
      "111 visual.transformer.resblocks.8.mlp.c_fc.weight\n",
      "torch.Size([3072, 768])\n",
      "112 visual.transformer.resblocks.8.mlp.c_fc.bias\n",
      "torch.Size([3072])\n",
      "113 visual.transformer.resblocks.8.mlp.c_proj.weight\n",
      "torch.Size([768, 3072])\n",
      "114 visual.transformer.resblocks.8.mlp.c_proj.bias\n",
      "torch.Size([768])\n",
      "115 visual.transformer.resblocks.8.ln_2.weight\n",
      "torch.Size([768])\n",
      "116 visual.transformer.resblocks.8.ln_2.bias\n",
      "torch.Size([768])\n",
      "117 visual.transformer.resblocks.9.attn.in_proj_weight\n",
      "torch.Size([2304, 768])\n",
      "118 visual.transformer.resblocks.9.attn.in_proj_bias\n",
      "torch.Size([2304])\n",
      "119 visual.transformer.resblocks.9.attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "120 visual.transformer.resblocks.9.attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "121 visual.transformer.resblocks.9.ln_1.weight\n",
      "torch.Size([768])\n",
      "122 visual.transformer.resblocks.9.ln_1.bias\n",
      "torch.Size([768])\n",
      "123 visual.transformer.resblocks.9.mlp.c_fc.weight\n",
      "torch.Size([3072, 768])\n",
      "124 visual.transformer.resblocks.9.mlp.c_fc.bias\n",
      "torch.Size([3072])\n",
      "125 visual.transformer.resblocks.9.mlp.c_proj.weight\n",
      "torch.Size([768, 3072])\n",
      "126 visual.transformer.resblocks.9.mlp.c_proj.bias\n",
      "torch.Size([768])\n",
      "127 visual.transformer.resblocks.9.ln_2.weight\n",
      "torch.Size([768])\n",
      "128 visual.transformer.resblocks.9.ln_2.bias\n",
      "torch.Size([768])\n",
      "129 visual.transformer.resblocks.10.attn.in_proj_weight\n",
      "torch.Size([2304, 768])\n",
      "130 visual.transformer.resblocks.10.attn.in_proj_bias\n",
      "torch.Size([2304])\n",
      "131 visual.transformer.resblocks.10.attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "132 visual.transformer.resblocks.10.attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "133 visual.transformer.resblocks.10.ln_1.weight\n",
      "torch.Size([768])\n",
      "134 visual.transformer.resblocks.10.ln_1.bias\n",
      "torch.Size([768])\n",
      "135 visual.transformer.resblocks.10.mlp.c_fc.weight\n",
      "torch.Size([3072, 768])\n",
      "136 visual.transformer.resblocks.10.mlp.c_fc.bias\n",
      "torch.Size([3072])\n",
      "137 visual.transformer.resblocks.10.mlp.c_proj.weight\n",
      "torch.Size([768, 3072])\n",
      "138 visual.transformer.resblocks.10.mlp.c_proj.bias\n",
      "torch.Size([768])\n",
      "139 visual.transformer.resblocks.10.ln_2.weight\n",
      "torch.Size([768])\n",
      "140 visual.transformer.resblocks.10.ln_2.bias\n",
      "torch.Size([768])\n",
      "141 visual.transformer.resblocks.11.attn.in_proj_weight\n",
      "torch.Size([2304, 768])\n",
      "142 visual.transformer.resblocks.11.attn.in_proj_bias\n",
      "torch.Size([2304])\n",
      "143 visual.transformer.resblocks.11.attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "144 visual.transformer.resblocks.11.attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "145 visual.transformer.resblocks.11.ln_1.weight\n",
      "torch.Size([768])\n",
      "146 visual.transformer.resblocks.11.ln_1.bias\n",
      "torch.Size([768])\n",
      "147 visual.transformer.resblocks.11.mlp.c_fc.weight\n",
      "torch.Size([3072, 768])\n",
      "148 visual.transformer.resblocks.11.mlp.c_fc.bias\n",
      "torch.Size([3072])\n",
      "149 visual.transformer.resblocks.11.mlp.c_proj.weight\n",
      "torch.Size([768, 3072])\n",
      "150 visual.transformer.resblocks.11.mlp.c_proj.bias\n",
      "torch.Size([768])\n",
      "151 visual.transformer.resblocks.11.ln_2.weight\n",
      "torch.Size([768])\n",
      "152 visual.transformer.resblocks.11.ln_2.bias\n",
      "torch.Size([768])\n",
      "153 visual.ln_post.weight\n",
      "torch.Size([768])\n",
      "154 visual.ln_post.bias\n",
      "torch.Size([768])\n",
      "155 transformer.resblocks.0.attn.in_proj_weight\n",
      "torch.Size([1536, 512])\n",
      "156 transformer.resblocks.0.attn.in_proj_bias\n",
      "torch.Size([1536])\n",
      "157 transformer.resblocks.0.attn.out_proj.weight\n",
      "torch.Size([512, 512])\n",
      "158 transformer.resblocks.0.attn.out_proj.bias\n",
      "torch.Size([512])\n",
      "159 transformer.resblocks.0.ln_1.weight\n",
      "torch.Size([512])\n",
      "160 transformer.resblocks.0.ln_1.bias\n",
      "torch.Size([512])\n",
      "161 transformer.resblocks.0.mlp.c_fc.weight\n",
      "torch.Size([2048, 512])\n",
      "162 transformer.resblocks.0.mlp.c_fc.bias\n",
      "torch.Size([2048])\n",
      "163 transformer.resblocks.0.mlp.c_proj.weight\n",
      "torch.Size([512, 2048])\n",
      "164 transformer.resblocks.0.mlp.c_proj.bias\n",
      "torch.Size([512])\n",
      "165 transformer.resblocks.0.ln_2.weight\n",
      "torch.Size([512])\n",
      "166 transformer.resblocks.0.ln_2.bias\n",
      "torch.Size([512])\n",
      "167 transformer.resblocks.1.attn.in_proj_weight\n",
      "torch.Size([1536, 512])\n",
      "168 transformer.resblocks.1.attn.in_proj_bias\n",
      "torch.Size([1536])\n",
      "169 transformer.resblocks.1.attn.out_proj.weight\n",
      "torch.Size([512, 512])\n",
      "170 transformer.resblocks.1.attn.out_proj.bias\n",
      "torch.Size([512])\n",
      "171 transformer.resblocks.1.ln_1.weight\n",
      "torch.Size([512])\n",
      "172 transformer.resblocks.1.ln_1.bias\n",
      "torch.Size([512])\n",
      "173 transformer.resblocks.1.mlp.c_fc.weight\n",
      "torch.Size([2048, 512])\n",
      "174 transformer.resblocks.1.mlp.c_fc.bias\n",
      "torch.Size([2048])\n",
      "175 transformer.resblocks.1.mlp.c_proj.weight\n",
      "torch.Size([512, 2048])\n",
      "176 transformer.resblocks.1.mlp.c_proj.bias\n",
      "torch.Size([512])\n",
      "177 transformer.resblocks.1.ln_2.weight\n",
      "torch.Size([512])\n",
      "178 transformer.resblocks.1.ln_2.bias\n",
      "torch.Size([512])\n",
      "179 transformer.resblocks.2.attn.in_proj_weight\n",
      "torch.Size([1536, 512])\n",
      "180 transformer.resblocks.2.attn.in_proj_bias\n",
      "torch.Size([1536])\n",
      "181 transformer.resblocks.2.attn.out_proj.weight\n",
      "torch.Size([512, 512])\n",
      "182 transformer.resblocks.2.attn.out_proj.bias\n",
      "torch.Size([512])\n",
      "183 transformer.resblocks.2.ln_1.weight\n",
      "torch.Size([512])\n",
      "184 transformer.resblocks.2.ln_1.bias\n",
      "torch.Size([512])\n",
      "185 transformer.resblocks.2.mlp.c_fc.weight\n",
      "torch.Size([2048, 512])\n",
      "186 transformer.resblocks.2.mlp.c_fc.bias\n",
      "torch.Size([2048])\n",
      "187 transformer.resblocks.2.mlp.c_proj.weight\n",
      "torch.Size([512, 2048])\n",
      "188 transformer.resblocks.2.mlp.c_proj.bias\n",
      "torch.Size([512])\n",
      "189 transformer.resblocks.2.ln_2.weight\n",
      "torch.Size([512])\n",
      "190 transformer.resblocks.2.ln_2.bias\n",
      "torch.Size([512])\n",
      "191 transformer.resblocks.3.attn.in_proj_weight\n",
      "torch.Size([1536, 512])\n",
      "192 transformer.resblocks.3.attn.in_proj_bias\n",
      "torch.Size([1536])\n",
      "193 transformer.resblocks.3.attn.out_proj.weight\n",
      "torch.Size([512, 512])\n",
      "194 transformer.resblocks.3.attn.out_proj.bias\n",
      "torch.Size([512])\n",
      "195 transformer.resblocks.3.ln_1.weight\n",
      "torch.Size([512])\n",
      "196 transformer.resblocks.3.ln_1.bias\n",
      "torch.Size([512])\n",
      "197 transformer.resblocks.3.mlp.c_fc.weight\n",
      "torch.Size([2048, 512])\n",
      "198 transformer.resblocks.3.mlp.c_fc.bias\n",
      "torch.Size([2048])\n",
      "199 transformer.resblocks.3.mlp.c_proj.weight\n",
      "torch.Size([512, 2048])\n",
      "200 transformer.resblocks.3.mlp.c_proj.bias\n",
      "torch.Size([512])\n",
      "201 transformer.resblocks.3.ln_2.weight\n",
      "torch.Size([512])\n",
      "202 transformer.resblocks.3.ln_2.bias\n",
      "torch.Size([512])\n",
      "203 transformer.resblocks.4.attn.in_proj_weight\n",
      "torch.Size([1536, 512])\n",
      "204 transformer.resblocks.4.attn.in_proj_bias\n",
      "torch.Size([1536])\n",
      "205 transformer.resblocks.4.attn.out_proj.weight\n",
      "torch.Size([512, 512])\n",
      "206 transformer.resblocks.4.attn.out_proj.bias\n",
      "torch.Size([512])\n",
      "207 transformer.resblocks.4.ln_1.weight\n",
      "torch.Size([512])\n",
      "208 transformer.resblocks.4.ln_1.bias\n",
      "torch.Size([512])\n",
      "209 transformer.resblocks.4.mlp.c_fc.weight\n",
      "torch.Size([2048, 512])\n",
      "210 transformer.resblocks.4.mlp.c_fc.bias\n",
      "torch.Size([2048])\n",
      "211 transformer.resblocks.4.mlp.c_proj.weight\n",
      "torch.Size([512, 2048])\n",
      "212 transformer.resblocks.4.mlp.c_proj.bias\n",
      "torch.Size([512])\n",
      "213 transformer.resblocks.4.ln_2.weight\n",
      "torch.Size([512])\n",
      "214 transformer.resblocks.4.ln_2.bias\n",
      "torch.Size([512])\n",
      "215 transformer.resblocks.5.attn.in_proj_weight\n",
      "torch.Size([1536, 512])\n",
      "216 transformer.resblocks.5.attn.in_proj_bias\n",
      "torch.Size([1536])\n",
      "217 transformer.resblocks.5.attn.out_proj.weight\n",
      "torch.Size([512, 512])\n",
      "218 transformer.resblocks.5.attn.out_proj.bias\n",
      "torch.Size([512])\n",
      "219 transformer.resblocks.5.ln_1.weight\n",
      "torch.Size([512])\n",
      "220 transformer.resblocks.5.ln_1.bias\n",
      "torch.Size([512])\n",
      "221 transformer.resblocks.5.mlp.c_fc.weight\n",
      "torch.Size([2048, 512])\n",
      "222 transformer.resblocks.5.mlp.c_fc.bias\n",
      "torch.Size([2048])\n",
      "223 transformer.resblocks.5.mlp.c_proj.weight\n",
      "torch.Size([512, 2048])\n",
      "224 transformer.resblocks.5.mlp.c_proj.bias\n",
      "torch.Size([512])\n",
      "225 transformer.resblocks.5.ln_2.weight\n",
      "torch.Size([512])\n",
      "226 transformer.resblocks.5.ln_2.bias\n",
      "torch.Size([512])\n",
      "227 transformer.resblocks.6.attn.in_proj_weight\n",
      "torch.Size([1536, 512])\n",
      "228 transformer.resblocks.6.attn.in_proj_bias\n",
      "torch.Size([1536])\n",
      "229 transformer.resblocks.6.attn.out_proj.weight\n",
      "torch.Size([512, 512])\n",
      "230 transformer.resblocks.6.attn.out_proj.bias\n",
      "torch.Size([512])\n",
      "231 transformer.resblocks.6.ln_1.weight\n",
      "torch.Size([512])\n",
      "232 transformer.resblocks.6.ln_1.bias\n",
      "torch.Size([512])\n",
      "233 transformer.resblocks.6.mlp.c_fc.weight\n",
      "torch.Size([2048, 512])\n",
      "234 transformer.resblocks.6.mlp.c_fc.bias\n",
      "torch.Size([2048])\n",
      "235 transformer.resblocks.6.mlp.c_proj.weight\n",
      "torch.Size([512, 2048])\n",
      "236 transformer.resblocks.6.mlp.c_proj.bias\n",
      "torch.Size([512])\n",
      "237 transformer.resblocks.6.ln_2.weight\n",
      "torch.Size([512])\n",
      "238 transformer.resblocks.6.ln_2.bias\n",
      "torch.Size([512])\n",
      "239 transformer.resblocks.7.attn.in_proj_weight\n",
      "torch.Size([1536, 512])\n",
      "240 transformer.resblocks.7.attn.in_proj_bias\n",
      "torch.Size([1536])\n",
      "241 transformer.resblocks.7.attn.out_proj.weight\n",
      "torch.Size([512, 512])\n",
      "242 transformer.resblocks.7.attn.out_proj.bias\n",
      "torch.Size([512])\n",
      "243 transformer.resblocks.7.ln_1.weight\n",
      "torch.Size([512])\n",
      "244 transformer.resblocks.7.ln_1.bias\n",
      "torch.Size([512])\n",
      "245 transformer.resblocks.7.mlp.c_fc.weight\n",
      "torch.Size([2048, 512])\n",
      "246 transformer.resblocks.7.mlp.c_fc.bias\n",
      "torch.Size([2048])\n",
      "247 transformer.resblocks.7.mlp.c_proj.weight\n",
      "torch.Size([512, 2048])\n",
      "248 transformer.resblocks.7.mlp.c_proj.bias\n",
      "torch.Size([512])\n",
      "249 transformer.resblocks.7.ln_2.weight\n",
      "torch.Size([512])\n",
      "250 transformer.resblocks.7.ln_2.bias\n",
      "torch.Size([512])\n",
      "251 transformer.resblocks.8.attn.in_proj_weight\n",
      "torch.Size([1536, 512])\n",
      "252 transformer.resblocks.8.attn.in_proj_bias\n",
      "torch.Size([1536])\n",
      "253 transformer.resblocks.8.attn.out_proj.weight\n",
      "torch.Size([512, 512])\n",
      "254 transformer.resblocks.8.attn.out_proj.bias\n",
      "torch.Size([512])\n",
      "255 transformer.resblocks.8.ln_1.weight\n",
      "torch.Size([512])\n",
      "256 transformer.resblocks.8.ln_1.bias\n",
      "torch.Size([512])\n",
      "257 transformer.resblocks.8.mlp.c_fc.weight\n",
      "torch.Size([2048, 512])\n",
      "258 transformer.resblocks.8.mlp.c_fc.bias\n",
      "torch.Size([2048])\n",
      "259 transformer.resblocks.8.mlp.c_proj.weight\n",
      "torch.Size([512, 2048])\n",
      "260 transformer.resblocks.8.mlp.c_proj.bias\n",
      "torch.Size([512])\n",
      "261 transformer.resblocks.8.ln_2.weight\n",
      "torch.Size([512])\n",
      "262 transformer.resblocks.8.ln_2.bias\n",
      "torch.Size([512])\n",
      "263 transformer.resblocks.9.attn.in_proj_weight\n",
      "torch.Size([1536, 512])\n",
      "264 transformer.resblocks.9.attn.in_proj_bias\n",
      "torch.Size([1536])\n",
      "265 transformer.resblocks.9.attn.out_proj.weight\n",
      "torch.Size([512, 512])\n",
      "266 transformer.resblocks.9.attn.out_proj.bias\n",
      "torch.Size([512])\n",
      "267 transformer.resblocks.9.ln_1.weight\n",
      "torch.Size([512])\n",
      "268 transformer.resblocks.9.ln_1.bias\n",
      "torch.Size([512])\n",
      "269 transformer.resblocks.9.mlp.c_fc.weight\n",
      "torch.Size([2048, 512])\n",
      "270 transformer.resblocks.9.mlp.c_fc.bias\n",
      "torch.Size([2048])\n",
      "271 transformer.resblocks.9.mlp.c_proj.weight\n",
      "torch.Size([512, 2048])\n",
      "272 transformer.resblocks.9.mlp.c_proj.bias\n",
      "torch.Size([512])\n",
      "273 transformer.resblocks.9.ln_2.weight\n",
      "torch.Size([512])\n",
      "274 transformer.resblocks.9.ln_2.bias\n",
      "torch.Size([512])\n",
      "275 transformer.resblocks.10.attn.in_proj_weight\n",
      "torch.Size([1536, 512])\n",
      "276 transformer.resblocks.10.attn.in_proj_bias\n",
      "torch.Size([1536])\n",
      "277 transformer.resblocks.10.attn.out_proj.weight\n",
      "torch.Size([512, 512])\n",
      "278 transformer.resblocks.10.attn.out_proj.bias\n",
      "torch.Size([512])\n",
      "279 transformer.resblocks.10.ln_1.weight\n",
      "torch.Size([512])\n",
      "280 transformer.resblocks.10.ln_1.bias\n",
      "torch.Size([512])\n",
      "281 transformer.resblocks.10.mlp.c_fc.weight\n",
      "torch.Size([2048, 512])\n",
      "282 transformer.resblocks.10.mlp.c_fc.bias\n",
      "torch.Size([2048])\n",
      "283 transformer.resblocks.10.mlp.c_proj.weight\n",
      "torch.Size([512, 2048])\n",
      "284 transformer.resblocks.10.mlp.c_proj.bias\n",
      "torch.Size([512])\n",
      "285 transformer.resblocks.10.ln_2.weight\n",
      "torch.Size([512])\n",
      "286 transformer.resblocks.10.ln_2.bias\n",
      "torch.Size([512])\n",
      "287 transformer.resblocks.11.attn.in_proj_weight\n",
      "torch.Size([1536, 512])\n",
      "288 transformer.resblocks.11.attn.in_proj_bias\n",
      "torch.Size([1536])\n",
      "289 transformer.resblocks.11.attn.out_proj.weight\n",
      "torch.Size([512, 512])\n",
      "290 transformer.resblocks.11.attn.out_proj.bias\n",
      "torch.Size([512])\n",
      "291 transformer.resblocks.11.ln_1.weight\n",
      "torch.Size([512])\n",
      "292 transformer.resblocks.11.ln_1.bias\n",
      "torch.Size([512])\n",
      "293 transformer.resblocks.11.mlp.c_fc.weight\n",
      "torch.Size([2048, 512])\n",
      "294 transformer.resblocks.11.mlp.c_fc.bias\n",
      "torch.Size([2048])\n",
      "295 transformer.resblocks.11.mlp.c_proj.weight\n",
      "torch.Size([512, 2048])\n",
      "296 transformer.resblocks.11.mlp.c_proj.bias\n",
      "torch.Size([512])\n",
      "297 transformer.resblocks.11.ln_2.weight\n",
      "torch.Size([512])\n",
      "298 transformer.resblocks.11.ln_2.bias\n",
      "torch.Size([512])\n",
      "299 token_embedding.weight\n",
      "torch.Size([49408, 512])\n",
      "300 ln_final.weight\n",
      "torch.Size([512])\n",
      "301 ln_final.bias\n",
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).to(device)\n",
    "# text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     image_features = model.encode_image(image)\n",
    "#     text_features = model.encode_text(text)\n",
    "    \n",
    "#     logits_per_image, logits_per_text = model(image, text)\n",
    "#     probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "# print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]\n",
    "\n",
    "i = 0\n",
    "for name, param in model.named_parameters():  # named_parameters 가중치 뽑아주는 함수\n",
    "    \n",
    "    print(i,name)\n",
    "    print(param.shape)\n",
    "    i+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param[0].dtype\n",
    "\n",
    "먼저 로라완성 시켜보자.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PEFT를 이용해서 로라를 완성시킨다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers accelerate evaluate datasets peft -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.33.0\n",
      "Accelerate version: 0.24.1\n",
      "PEFT version: 0.5.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import accelerate\n",
    "import peft\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Accelerate version: {accelerate.__version__}\")\n",
    "print(f\"PEFT version: {peft.__version__}\")\n",
    "\"Transformers version: 4.27.4\"\n",
    "\"Accelerate version: 0.18.0\"\n",
    "\"PEFT version: 0.2.0\"\n",
    "\n",
    "model_checkpoint = \"google/vit-base-patch16-224-in21k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juniverse/opt/anaconda3/envs/veda/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/juniverse/opt/anaconda3/envs/veda/lib/python3.8/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c106detail19maybe_wrap_dim_slowIxEET_S2_S2_b\n",
      "  Referenced from: <870081F6-12FD-3CEA-BC5C-30F4764F2A98> /Users/juniverse/opt/anaconda3/envs/veda/lib/python3.8/site-packages/torchvision/image.so\n",
      "  Expected in:     <F2FE5CF8-5B5B-3FAD-ADF8-C77D90F49FC9> /Users/juniverse/opt/anaconda3/envs/veda/lib/python3.8/site-packages/torch/lib/libc10.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "transform = transforms.Compose(\n",
    "    [transforms.RandomCrop(32, padding=4),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "test_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True) \n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=16,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0][0].shape # image\n",
    "trainset[0][1] # label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = trainset.targets\n",
    "\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label\n",
    "    break\n",
    "\n",
    "# id2label[2]\n",
    "\"baklava\"\n",
    "\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_processor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/juniverse/Desktop/pointcloud/VectorUniverse/project_code/youtube/LoRA/LoRA.ipynb 셀 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juniverse/Desktop/pointcloud/VectorUniverse/project_code/youtube/LoRA/LoRA.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juniverse/Desktop/pointcloud/VectorUniverse/project_code/youtube/LoRA/LoRA.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     CenterCrop,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juniverse/Desktop/pointcloud/VectorUniverse/project_code/youtube/LoRA/LoRA.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     Compose,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juniverse/Desktop/pointcloud/VectorUniverse/project_code/youtube/LoRA/LoRA.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     ToTensor,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juniverse/Desktop/pointcloud/VectorUniverse/project_code/youtube/LoRA/LoRA.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/juniverse/Desktop/pointcloud/VectorUniverse/project_code/youtube/LoRA/LoRA.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m normalize \u001b[39m=\u001b[39m Normalize(mean\u001b[39m=\u001b[39mimage_processor\u001b[39m.\u001b[39mimage_mean, std\u001b[39m=\u001b[39mimage_processor\u001b[39m.\u001b[39mimage_std)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/juniverse/Desktop/pointcloud/VectorUniverse/project_code/youtube/LoRA/LoRA.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m train_transforms \u001b[39m=\u001b[39m Compose(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/juniverse/Desktop/pointcloud/VectorUniverse/project_code/youtube/LoRA/LoRA.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     [\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/juniverse/Desktop/pointcloud/VectorUniverse/project_code/youtube/LoRA/LoRA.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         RandomResizedCrop(image_processor\u001b[39m.\u001b[39msize[\u001b[39m\"\u001b[39m\u001b[39mheight\u001b[39m\u001b[39m\"\u001b[39m]),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/juniverse/Desktop/pointcloud/VectorUniverse/project_code/youtube/LoRA/LoRA.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     ]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/juniverse/Desktop/pointcloud/VectorUniverse/project_code/youtube/LoRA/LoRA.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/juniverse/Desktop/pointcloud/VectorUniverse/project_code/youtube/LoRA/LoRA.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m val_transforms \u001b[39m=\u001b[39m Compose(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/juniverse/Desktop/pointcloud/VectorUniverse/project_code/youtube/LoRA/LoRA.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     [\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/juniverse/Desktop/pointcloud/VectorUniverse/project_code/youtube/LoRA/LoRA.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         Resize(image_processor\u001b[39m.\u001b[39msize[\u001b[39m\"\u001b[39m\u001b[39mheight\u001b[39m\u001b[39m\"\u001b[39m]),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/juniverse/Desktop/pointcloud/VectorUniverse/project_code/youtube/LoRA/LoRA.ipynb#X11sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     ]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/juniverse/Desktop/pointcloud/VectorUniverse/project_code/youtube/LoRA/LoRA.ipynb#X11sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_processor' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        RandomResizedCrop(image_processor.size[\"height\"]),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        Resize(image_processor.size[\"height\"]),\n",
    "        CenterCrop(image_processor.size[\"height\"]),\n",
    "        ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess_train(example_batch):\n",
    "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch\n",
    "\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.set_transform(preprocess_train)\n",
    "# val_ds.set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f175d355554892b99b0d8d5c3cc7da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 884736 || all params: 86684930 || trainable%: 1.02\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1,3,224,224)\n",
    "y = model(x)\n",
    "\n",
    "\n",
    "print_trainable_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 768])\n",
      "vit.embeddings.cls_token\n",
      "torch.Size([1, 197, 768])\n",
      "vit.embeddings.position_embeddings\n",
      "torch.Size([768, 3, 16, 16])\n",
      "vit.embeddings.patch_embeddings.projection.weight\n",
      "torch.Size([768])\n",
      "vit.embeddings.patch_embeddings.projection.bias\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.0.attention.attention.query.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.0.attention.attention.query.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.0.attention.attention.query.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.0.attention.attention.query.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.0.attention.attention.key.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.0.attention.attention.key.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.0.attention.attention.key.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.0.attention.attention.key.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.0.attention.attention.value.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.0.attention.attention.value.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.0.attention.attention.value.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.0.attention.attention.value.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.0.attention.output.dense.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.0.attention.output.dense.bias\n",
      "torch.Size([3072, 768])\n",
      "vit.encoder.layer.0.intermediate.dense.weight\n",
      "torch.Size([3072])\n",
      "vit.encoder.layer.0.intermediate.dense.bias\n",
      "torch.Size([768, 3072])\n",
      "vit.encoder.layer.0.output.dense.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.0.output.dense.bias\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.0.layernorm_before.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.0.layernorm_before.bias\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.0.layernorm_after.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.0.layernorm_after.bias\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.1.attention.attention.query.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.1.attention.attention.query.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.1.attention.attention.query.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.1.attention.attention.query.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.1.attention.attention.key.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.1.attention.attention.key.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.1.attention.attention.key.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.1.attention.attention.key.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.1.attention.attention.value.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.1.attention.attention.value.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.1.attention.attention.value.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.1.attention.attention.value.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.1.attention.output.dense.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.1.attention.output.dense.bias\n",
      "torch.Size([3072, 768])\n",
      "vit.encoder.layer.1.intermediate.dense.weight\n",
      "torch.Size([3072])\n",
      "vit.encoder.layer.1.intermediate.dense.bias\n",
      "torch.Size([768, 3072])\n",
      "vit.encoder.layer.1.output.dense.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.1.output.dense.bias\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.1.layernorm_before.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.1.layernorm_before.bias\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.1.layernorm_after.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.1.layernorm_after.bias\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.2.attention.attention.query.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.2.attention.attention.query.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.2.attention.attention.query.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.2.attention.attention.query.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.2.attention.attention.key.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.2.attention.attention.key.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.2.attention.attention.key.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.2.attention.attention.key.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.2.attention.attention.value.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.2.attention.attention.value.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.2.attention.attention.value.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.2.attention.attention.value.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.2.attention.output.dense.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.2.attention.output.dense.bias\n",
      "torch.Size([3072, 768])\n",
      "vit.encoder.layer.2.intermediate.dense.weight\n",
      "torch.Size([3072])\n",
      "vit.encoder.layer.2.intermediate.dense.bias\n",
      "torch.Size([768, 3072])\n",
      "vit.encoder.layer.2.output.dense.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.2.output.dense.bias\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.2.layernorm_before.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.2.layernorm_before.bias\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.2.layernorm_after.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.2.layernorm_after.bias\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.3.attention.attention.query.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.3.attention.attention.query.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.3.attention.attention.query.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.3.attention.attention.query.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.3.attention.attention.key.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.3.attention.attention.key.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.3.attention.attention.key.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.3.attention.attention.key.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.3.attention.attention.value.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.3.attention.attention.value.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.3.attention.attention.value.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.3.attention.attention.value.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.3.attention.output.dense.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.3.attention.output.dense.bias\n",
      "torch.Size([3072, 768])\n",
      "vit.encoder.layer.3.intermediate.dense.weight\n",
      "torch.Size([3072])\n",
      "vit.encoder.layer.3.intermediate.dense.bias\n",
      "torch.Size([768, 3072])\n",
      "vit.encoder.layer.3.output.dense.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.3.output.dense.bias\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.3.layernorm_before.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.3.layernorm_before.bias\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.3.layernorm_after.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.3.layernorm_after.bias\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.4.attention.attention.query.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.4.attention.attention.query.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.4.attention.attention.query.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.4.attention.attention.query.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.4.attention.attention.key.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.4.attention.attention.key.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.4.attention.attention.key.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.4.attention.attention.key.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.4.attention.attention.value.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.4.attention.attention.value.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.4.attention.attention.value.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.4.attention.attention.value.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.4.attention.output.dense.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.4.attention.output.dense.bias\n",
      "torch.Size([3072, 768])\n",
      "vit.encoder.layer.4.intermediate.dense.weight\n",
      "torch.Size([3072])\n",
      "vit.encoder.layer.4.intermediate.dense.bias\n",
      "torch.Size([768, 3072])\n",
      "vit.encoder.layer.4.output.dense.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.4.output.dense.bias\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.4.layernorm_before.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.4.layernorm_before.bias\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.4.layernorm_after.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.4.layernorm_after.bias\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.5.attention.attention.query.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.5.attention.attention.query.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.5.attention.attention.query.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.5.attention.attention.query.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.5.attention.attention.key.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.5.attention.attention.key.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.5.attention.attention.key.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.5.attention.attention.key.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.5.attention.attention.value.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.5.attention.attention.value.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.5.attention.attention.value.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.5.attention.attention.value.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.5.attention.output.dense.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.5.attention.output.dense.bias\n",
      "torch.Size([3072, 768])\n",
      "vit.encoder.layer.5.intermediate.dense.weight\n",
      "torch.Size([3072])\n",
      "vit.encoder.layer.5.intermediate.dense.bias\n",
      "torch.Size([768, 3072])\n",
      "vit.encoder.layer.5.output.dense.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.5.output.dense.bias\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.5.layernorm_before.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.5.layernorm_before.bias\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.5.layernorm_after.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.5.layernorm_after.bias\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.6.attention.attention.query.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.6.attention.attention.query.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.6.attention.attention.query.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.6.attention.attention.query.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.6.attention.attention.key.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.6.attention.attention.key.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.6.attention.attention.key.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.6.attention.attention.key.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.6.attention.attention.value.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.6.attention.attention.value.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.6.attention.attention.value.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.6.attention.attention.value.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.6.attention.output.dense.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.6.attention.output.dense.bias\n",
      "torch.Size([3072, 768])\n",
      "vit.encoder.layer.6.intermediate.dense.weight\n",
      "torch.Size([3072])\n",
      "vit.encoder.layer.6.intermediate.dense.bias\n",
      "torch.Size([768, 3072])\n",
      "vit.encoder.layer.6.output.dense.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.6.output.dense.bias\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.6.layernorm_before.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.6.layernorm_before.bias\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.6.layernorm_after.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.6.layernorm_after.bias\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.7.attention.attention.query.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.7.attention.attention.query.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.7.attention.attention.query.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.7.attention.attention.query.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.7.attention.attention.key.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.7.attention.attention.key.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.7.attention.attention.key.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.7.attention.attention.key.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.7.attention.attention.value.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.7.attention.attention.value.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.7.attention.attention.value.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.7.attention.attention.value.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.7.attention.output.dense.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.7.attention.output.dense.bias\n",
      "torch.Size([3072, 768])\n",
      "vit.encoder.layer.7.intermediate.dense.weight\n",
      "torch.Size([3072])\n",
      "vit.encoder.layer.7.intermediate.dense.bias\n",
      "torch.Size([768, 3072])\n",
      "vit.encoder.layer.7.output.dense.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.7.output.dense.bias\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.7.layernorm_before.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.7.layernorm_before.bias\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.7.layernorm_after.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.7.layernorm_after.bias\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.8.attention.attention.query.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.8.attention.attention.query.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.8.attention.attention.query.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.8.attention.attention.query.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.8.attention.attention.key.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.8.attention.attention.key.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.8.attention.attention.key.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.8.attention.attention.key.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.8.attention.attention.value.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.8.attention.attention.value.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.8.attention.attention.value.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.8.attention.attention.value.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.8.attention.output.dense.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.8.attention.output.dense.bias\n",
      "torch.Size([3072, 768])\n",
      "vit.encoder.layer.8.intermediate.dense.weight\n",
      "torch.Size([3072])\n",
      "vit.encoder.layer.8.intermediate.dense.bias\n",
      "torch.Size([768, 3072])\n",
      "vit.encoder.layer.8.output.dense.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.8.output.dense.bias\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.8.layernorm_before.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.8.layernorm_before.bias\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.8.layernorm_after.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.8.layernorm_after.bias\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.9.attention.attention.query.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.9.attention.attention.query.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.9.attention.attention.query.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.9.attention.attention.query.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.9.attention.attention.key.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.9.attention.attention.key.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.9.attention.attention.key.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.9.attention.attention.key.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.9.attention.attention.value.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.9.attention.attention.value.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.9.attention.attention.value.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.9.attention.attention.value.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.9.attention.output.dense.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.9.attention.output.dense.bias\n",
      "torch.Size([3072, 768])\n",
      "vit.encoder.layer.9.intermediate.dense.weight\n",
      "torch.Size([3072])\n",
      "vit.encoder.layer.9.intermediate.dense.bias\n",
      "torch.Size([768, 3072])\n",
      "vit.encoder.layer.9.output.dense.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.9.output.dense.bias\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.9.layernorm_before.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.9.layernorm_before.bias\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.9.layernorm_after.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.9.layernorm_after.bias\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.10.attention.attention.query.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.10.attention.attention.query.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.10.attention.attention.query.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.10.attention.attention.query.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.10.attention.attention.key.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.10.attention.attention.key.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.10.attention.attention.key.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.10.attention.attention.key.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.10.attention.attention.value.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.10.attention.attention.value.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.10.attention.attention.value.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.10.attention.attention.value.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.10.attention.output.dense.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.10.attention.output.dense.bias\n",
      "torch.Size([3072, 768])\n",
      "vit.encoder.layer.10.intermediate.dense.weight\n",
      "torch.Size([3072])\n",
      "vit.encoder.layer.10.intermediate.dense.bias\n",
      "torch.Size([768, 3072])\n",
      "vit.encoder.layer.10.output.dense.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.10.output.dense.bias\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.10.layernorm_before.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.10.layernorm_before.bias\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.10.layernorm_after.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.10.layernorm_after.bias\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.11.attention.attention.query.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.11.attention.attention.query.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.11.attention.attention.query.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.11.attention.attention.query.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.11.attention.attention.key.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.11.attention.attention.key.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.11.attention.attention.key.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.11.attention.attention.key.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.11.attention.attention.value.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.11.attention.attention.value.bias\n",
      "torch.Size([16, 768])\n",
      "vit.encoder.layer.11.attention.attention.value.lora_A.default.weight\n",
      "torch.Size([768, 16])\n",
      "vit.encoder.layer.11.attention.attention.value.lora_B.default.weight\n",
      "torch.Size([768, 768])\n",
      "vit.encoder.layer.11.attention.output.dense.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.11.attention.output.dense.bias\n",
      "torch.Size([3072, 768])\n",
      "vit.encoder.layer.11.intermediate.dense.weight\n",
      "torch.Size([3072])\n",
      "vit.encoder.layer.11.intermediate.dense.bias\n",
      "torch.Size([768, 3072])\n",
      "vit.encoder.layer.11.output.dense.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.11.output.dense.bias\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.11.layernorm_before.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.11.layernorm_before.bias\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.11.layernorm_after.weight\n",
      "torch.Size([768])\n",
      "vit.encoder.layer.11.layernorm_after.bias\n",
      "torch.Size([768])\n",
      "vit.layernorm.weight\n",
      "torch.Size([768])\n",
      "vit.layernorm.bias\n",
      "torch.Size([1, 768])\n",
      "classifier.original_module.weight\n",
      "torch.Size([1])\n",
      "classifier.original_module.bias\n",
      "torch.Size([1, 768])\n",
      "classifier.modules_to_save.default.weight\n",
      "torch.Size([1])\n",
      "classifier.modules_to_save.default.bias\n"
     ]
    }
   ],
   "source": [
    "for name,params in model.named_parameters():\n",
    "    print(params.shape)\n",
    "    print(name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 85799425 || all params: 85799425 || trainable%: 100.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'trainable params: 85876325 || all params: 85876325 || trainable%: 100.00'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_trainable_parameters(model)\n",
    "\"trainable params: 85876325 || all params: 85876325 || trainable%: 100.00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 884736 || all params: 86684930 || trainable%: 1.02\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'trainable params: 667493 || all params: 86466149 || trainable%: 0.77'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query\", \"value\",\"key\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    modules_to_save=[\"classifier\"],\n",
    ")\n",
    "lora_model = get_peft_model(model, config)\n",
    "\n",
    "print('query value key')\n",
    "print_trainable_parameters(lora_model)\n",
    "\n",
    "print('query value')\n",
    "\"trainable params: 667493 || all params: 86466149 || trainable%: 0.77\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.vit.encoder.layer.0.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.0.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.0.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.0.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.0.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.0.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.1.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.1.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.1.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.1.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.1.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.1.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.2.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.2.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.2.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.2.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.2.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.2.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.3.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.3.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.3.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.3.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.3.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.3.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.4.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.4.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.4.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.4.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.4.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.4.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.5.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.5.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.5.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.5.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.5.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.5.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.6.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.6.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.6.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.6.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.6.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.6.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.7.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.7.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.7.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.7.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.7.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.7.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.8.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.8.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.8.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.8.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.8.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.8.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.9.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.9.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.9.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.9.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.9.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.9.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.10.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.10.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.10.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.10.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.10.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.10.attention.attention.value.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.11.attention.attention.query.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.11.attention.attention.query.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.11.attention.attention.key.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.11.attention.attention.key.lora_B.default.weight\n",
      "base_model.model.vit.encoder.layer.11.attention.attention.value.lora_A.default.weight\n",
      "base_model.model.vit.encoder.layer.11.attention.attention.value.lora_B.default.weight\n"
     ]
    }
   ],
   "source": [
    "for name,params in lora_model.named_parameters():\n",
    "    if params.requires_grad==True:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "veda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
