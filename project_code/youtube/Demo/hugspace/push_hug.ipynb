{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import CLIPProcessor, CLIPModel,AutoProcessor, AutoTokenizer\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "config = LoraConfig(\n",
    "#     task_type=TaskType.FEATURE_EXTRACTION, \n",
    "#     inference_mode=False, \n",
    "    r=4, \n",
    "    lora_alpha=16, # alpha/r 이다. 가중치 조정하는 하이퍼 파라미터라고 생각하면된다. \n",
    "    # 내생각에는 rank가 2처럼 작은경우 조금더 높은값으로 조정하기 위해있는게 아닐까 추측한다.\n",
    "    lora_dropout=0.1,\n",
    "    # transformer의 clip에서는 아래와 같이 이름이 붙여져있다.\n",
    "    target_modules=[\"q_proj\", \"v_proj\",\"k_proj\",\"out_proj\"],\n",
    "    # PEFT를 적용할 모듈\n",
    "    bias=\"none\", # Bias usage\n",
    "    \n",
    "    # 여기를 붙이면 파라미터가 급격하게 늘어나는데 왜그런걸까? : encoder파라미터 저장.\n",
    "    # modules_to_save=[\"encoder\"], #save module list\n",
    ")\n",
    "\n",
    "# lora_model = get_peft_model(model, config)\n",
    "lora_model = get_peft_model(model, config)\n",
    "# _,lora_model_params_num = print_trainable_parameters(lora_model)\n",
    "# print('파라미터 늘어난 비율 : ',lora_model_params_num/basic_model_params_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'peft.tuners.lora.layer'; 'peft.tuners.lora' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/juniverse/Downloads/과제2.ipynb 셀 2\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juniverse/Downloads/%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A62.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juniverse/Downloads/%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A62.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m PATH \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./Youtube_CLIP_LoRA_SimCSE_15.pt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/juniverse/Downloads/%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A62.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(PATH,map_location\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juniverse/Downloads/%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A62.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# xmodel = model.load_state_dict(torch.load(PATH, map_location=torch.device('cpu')))\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/veda/lib/python3.8/site-packages/torch/serialization.py:789\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    788\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    790\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[1;32m    791\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/veda/lib/python3.8/site-packages/torch/serialization.py:1131\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1130\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1131\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1133\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1135\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/veda/lib/python3.8/site-packages/torch/serialization.py:1124\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m mod_name \u001b[39m=\u001b[39m load_module_mapping\u001b[39m.\u001b[39mget(mod_name, mod_name)\n\u001b[0;32m-> 1124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfind_class(mod_name, name)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'peft.tuners.lora.layer'; 'peft.tuners.lora' is not a package"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "PATH = './Youtube_CLIP_LoRA_SimCSE_15.pt'\n",
    "model = torch.load(PATH,map_location=torch.device('cpu'))\n",
    "# xmodel = model.load_state_dict(torch.load(PATH, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da73b968a67f4739be1ca16e81aac46e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/Soran/CLIP_LoRA_SimCSE', endpoint='https://huggingface.co', repo_type='model', repo_id='Soran/CLIP_LoRA_SimCSE')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import create_repo\n",
    " \n",
    "# Repository 생성\n",
    "REPO_NAME = 'CLIP_LoRA_SimCSE'\n",
    "create_repo('CLIP_LoRA_SimCSE')\n",
    "\n",
    "# , repo_type='model', repo_id='Soran/CLIP_LoRA_SimCSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/Soran/CLIP_LoRA_SimCSE into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import Repository\n",
    "\n",
    "repo = Repository('/Users/juniverse/Desktop/pointcloud/huggingface_space/model', clone_from='Soran/CLIP_LoRA_SimCSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r model/* /Users/juniverse/Desktop/pointcloud/huggingface_space/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# repo.git_add()\n",
    "# repo.git_commit('Initial commit')\n",
    "# repo.git_push()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "upload_file() takes 1 positional argument but 2 positional arguments (and 2 keyword-only arguments) were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/juniverse/Desktop/pointcloud/huggingface_space/Custom_Youtube_Algorithm/test.ipynb 셀 6\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juniverse/Desktop/pointcloud/huggingface_space/Custom_Youtube_Algorithm/test.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhuggingface_hub\u001b[39;00m \u001b[39mimport\u001b[39;00m upload_file, delete_file\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juniverse/Desktop/pointcloud/huggingface_space/Custom_Youtube_Algorithm/test.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# LOCAL_FILE_PATH =  '/Users/juniverse/Downloads/CLIP_LoRA_CSE_15.pt'\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juniverse/Desktop/pointcloud/huggingface_space/Custom_Youtube_Algorithm/test.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# 파일을 직접 업로드\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/juniverse/Desktop/pointcloud/huggingface_space/Custom_Youtube_Algorithm/test.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m upload_file(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juniverse/Desktop/pointcloud/huggingface_space/Custom_Youtube_Algorithm/test.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39m/Users/juniverse/Downloads/CLIP_LoRA_CSE_15.pt\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juniverse/Desktop/pointcloud/huggingface_space/Custom_Youtube_Algorithm/test.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     path_in_repo\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./model\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juniverse/Desktop/pointcloud/huggingface_space/Custom_Youtube_Algorithm/test.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     repo_id\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mSoran/CLIP_LoRA_SimCSE\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/juniverse/Desktop/pointcloud/huggingface_space/Custom_Youtube_Algorithm/test.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/veda/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/veda/lib/python3.8/site-packages/huggingface_hub/hf_api.py:849\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    846\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_as_future(fn, \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    848\u001b[0m \u001b[39m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m--> 849\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: upload_file() takes 1 positional argument but 2 positional arguments (and 2 keyword-only arguments) were given"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import upload_file, delete_file\n",
    "# LOCAL_FILE_PATH =  '/Users/juniverse/Downloads/CLIP_LoRA_CSE_15.pt'\n",
    "# 파일을 직접 업로드\n",
    "upload_file(\n",
    "    '/Users/juniverse/Downloads/CLIP_LoRA_CSE_15.pt',\n",
    "    path_in_repo='./model',\n",
    "    repo_id='Soran/CLIP_LoRA_SimCSE',\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "veda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
