# <center> Vector Universe </center>
---

<center>

![GitHub ë¡œê³ ](README_image/0u0.png)

</center>

#### <center> JoongHyun Shin </center>

<br>

##### <right>#CLIP #LoRA #Embeddings #Youtube Multimodal </right>

---


# Demo


**huggingface spaceì— ë°ëª¨ ë²„ì „ì„ ì˜¬ë ¤ë†“ì•˜ìŠµë‹ˆë‹¤.**

### [Click](https://huggingface.co/Soran/youtube_CLIP_LoRA_SimCSE)

![Alt text](beatles.png)



# 1. Datasets & DataLoader

<!-- ![Alt text](image-2.png) -->

#### youtube thumbnails data

$I^{(i)}$ : youtube thumbnail Image data $i$
$T^{(i)}$ : youtube Title data $i$

<!-- #### Example 

$I^{(i)}$ : ![Alt text](image-1.png)

$T^{(i)}$ : **Cutest Cats Compilation 2017 | Best Cute Cat Videos Ever** -->


---
# 2. Model & Loss Architecture

<!-- 
![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/lora_diagram.png) -->

<!-- ![Alt text](image-5.png) -->
![Alt text](image-7.png)

ìœ„ ëª¨ë¸ ì•„í‚¤í…ì³ë¥¼ ë³´ë©´ Latent space ìƒì—ì„œ ì´ë¯¸ì§€ ì„ë² ë”©ê³¼ í…ìŠ¤íŠ¸ ì„ë² ë”©ì˜ ê±°ë¦¬ë¥¼ ê°€ê¹ê²Œ í•˜ëŠ”ê²ƒì„ alignment, ë©€ê²Œí•˜ëŠ”ê²ƒì„ Uniformë¼ê³  ì •ì˜í•˜ì˜€ìŠµë‹ˆë‹¤.

### [model ì„¤ëª…](https://velog.io/@blackeyes0u0/youtube-CLIP-LoRA-SimCSE-%EA%B2%B0%EA%B3%BC)

### [LoRA ë…¼ë¬¸ ë¦¬ë·°](https://velog.io/@blackeyes0u0/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-LoRA-Low-Rank-Adaptation-of-Large-Language-Models)


---


# 3. Objective function

$$
h_i = f(x_i)
$$

$$
\mathcal{L}_i=-\log \frac{e^{\operatorname{sim}( {h}_i^{z_i}, {h}_i^{z_i^{\prime}}) / \tau }}{\sum_{j=1}^N e^{\operatorname{sim}({h}_i^{z_i}, {h}_j^{z_j^{\prime}}) / \tau}}
$$

$i$ ë²ˆì§¸ ë°ì´í„°ì™€ $N$ê°œì˜ batch_size pair ëŒ€í•´ì„œ ìœ„ì™€ ê°™ì´ í‘œí˜„ í•  ìˆ˜ìˆë‹¤. 

$h_i$ëŠ” ë°ì´í„°ì˜ ì„ë² ë”©ì— í•´ë‹¹í•˜ê³ , $z_i$ëŠ” ê° ë°ì´í„°ì— ê°€í•œ augmentationì— í•´ë‹¹í•œë‹¤. $\tau$ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° temperatureê°’ì´ë‹¤.


<!-- ![Alt text](image-3.png) -->

<!-- ![Alt text](image-6.png) -->

$$
\mathcal{L} = \sum_{i=1}^{N} log \exp^ {-\frac{1}{\tau}  sim(h_i,h_i^+)} (Alignment)
$$

$$
+\sum_{i=1}^{N} log \sum_{j=1 }^{N} \exp^{\frac{1}{\tau} sim(h_i,h_j)} (Uniform)
$$


ì—¬ê¸°ì„œ ë‚˜ì˜¤ëŠ” simì€ similarityì˜ ì•½ìì´ê³ , cosine similarityë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

### Notation

ië²ˆì§¸ image embedding : $I_i$ ëŠ” row vector

ië²ˆì§¸ text embedding : $T_i$

**(ë‹¨, $I_i,T_j,I_i^+,T_j^+$ëŠ” 1ë¡œ normalize)**

ì½”ë“œ ìƒì—ì„œëŠ” cosine similarityë¥¼ ì‚¬ìš©í•´ì„œ normalizeí•˜ì˜€ìŠµë‹ˆë‹¤.

$$
I_i = \mathbb M(batchsize,d=512)[i]
$$

## Image Text Alignment & Uniform

$$
alignment = -\sum_i tr(II^{+T}+I T^T+ I^+ T^{+T}+TT^{+T})
$$


ë¨¼ì € ìœ„ Object functionì—ì„œ Uniformì‹ì´ ì•„ë˜ì™€ ê°™ì´ ë˜ê¸° ìœ„í•´ì„œëŠ” convex functionë¼ê³  ê°€ì •í•˜ê³ , jensen's inequalityë¥¼ ì‚¬ìš©í•œ ê²°ê³¼ì…ë‹ˆë‹¤.

$$
Uniform = \sum_i \sum_{j } I_i \cdot T_j^T + \cdots \\
= sum(II^{+T}+I T^T+ I^+ T^{+T}+TT^{+T})
$$



ìœ„ ì‹ì„ ë¶„ì‚°ê³¼ í‰ê·  ê´€ì ì—ì„œ ë‹¤ì‹œ ë°”ë¼ë³´ì•˜ìŠµë‹ˆë‹¤.
$I_i$ê°€ í•œê°œì˜ ì„ë² ë”© ê°’ì´ë¼ê³  í•˜ê³ , ì´ ê°’ë“¤ì€ ê° í‰ê· ê³¼ ë¶„ì‚°ì„ ê°–ëŠ”ë‹¤ê³  í•˜ë©´, ì ì ˆí•œ ì„ë² ë”©ì€ ì–´ëŠ í•œ ì°¨ì›ìœ¼ë¡œ ì ë¦¬ì§€ ì•Šê³  ì ì ˆí•˜ê²Œ ë¶„ì‚°ë˜ì–´ì„œ í‘œí˜„ë˜ëŠ”ê²ƒ ì…ë‹ˆë‹¤.

ì´ê²ƒì— ëŒ€í•œ ì†”ë£¨ì…˜ìœ¼ë¡œëŠ” PCA whiteningê³¼ batch normalizationì´ê°€ ìƒê°ì´ ë‚©ë‹ˆë‹¤. ë¬´ì—‡ì„ ì‚¬ìš©í•´ì•¼í• ì§€ëŠ” ì•Œê¸° ìœ„í•´ ìˆ˜ì‹ì„ ì „ê°œí•´ ë³´ì•˜ìŠµë‹ˆë‹¤.

$$
I_i = \mu +\sigma_i \\
\mu = \frac{1}{N}\sum_{i \in \chi}^N I_i\\
\therefore \frac{1}{N}\sum_{i \in \chi}^N \sigma_i = 0
$$
$I$ë§Œ ìƒê°í•´ë³´ë©´,
$$
\frac{1}{N^2}\sum_{i \in \chi}^N \sum_{j \in \chi}^N I_i \cdot I_j^T 
$$

$$
= \frac{1}{N^2}\sum_{i \in \chi}^N \sum_{j \in \chi }^N (\mu +\sigma_i ) \cdot (\mu +\sigma_j )^T
$$

$$
=\mu \mu^T + \frac{1}{N^2}\sum_{i \in \chi }^N \sum_{j \in \chi}^N \sigma_i \cdot \sigma_j^T \\
=  I \cdot I^T = A
$$

ê°€ ë˜ì–´ì„œ ëœ»ì„ í•´ì„í•´ë³´ë©´ ì„ë² ë”©ì˜ í‰ê· ê°’ì„ ë‚®ì¶”ê³ , ë¶„ì‚°ì˜ ê³±ì„ ë‚®ì¶”ëŠ” ì‹ì´ë‹¤. ë˜í•œ, ìœ„ ì‹ì€ symmetric matrixì´ê¸° ë•Œë¬¸ì— í•­ìƒ diagonalizableí•˜ê³ , ê·¸ eigen vectorëŠ” orthogonal í•©ë‹ˆë‹¤.

ê·¸ëŸ¬í•œ ê²½ìš°ë¥¼ eigen decompositoiní•´ì„œ ìƒê°í•´ë³´ì.
$A$ë¼ê³  ë†“ì€ í–‰ë ¬ì„ $A P_i = \lambda_i P_i$ë¼ê³  ìƒê°í•´ë³´ì. ì´ë•Œ $P_i$ëŠ” $\lambda_i$ì— ëŒ€í•œ eigen vectorì…ë‹ˆë‹¤.

$$ 
A = P DP^T
$$

ì´ë•Œ, $PP^T = E$ ì¦‰, orthogonalí•˜ë¯€ë¡œ, 

$$
A = \sum_i \lambda_i P_i \cdot P_i^T
$$

$\lambda_i$ì˜ ì–´ëŠ í•œê°’ì´ í¬ë‹¤ëŠ” ê²ƒì€ ë°ì´í„°ê°€ ê³¨ê³ ë£¨ í¼ì ¸ìˆê¸°ë³´ë‹¨, í•œ ë°©í–¥ìœ¼ë¡œ ì¹˜ìš°ì³ì ¸ìˆëŠ”ê²ƒì´ë‹¤. ë”°ë¼ì„œ ìœ„ eigen valueê°’ì„ ê³¨ê³ ë£¨ ë§Œë“œëŠê²ƒì´ ì—¬ê¸°ì„œ ë‚˜ì˜¨ Uniformì˜ ëª©ì ì…ë‹ˆë‹¤. 

### Flatten Embedding

ìœ„ ëª©ì ì„ ì´ë£¨ê¸° ìœ„í•´ì„œ ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œìš”??

ë§Œì•½ì— $I_i$ê°€ normalize ë˜ì–´ìˆë‹¤ê³  í•œë‹¤ë©´, $tr(A)$ì˜ ê°’ì€ sum of eigen valueì´ê³ , constantí• ê²ƒì´ë‹¤. ì™œëƒí•˜ë©´ diagonal elementê°€ ëª¨ë‘ 1ì´ê¸° ë•Œë¬¸ì—.
ê·¸ë ‡ë‹¤ë©´ largest eigen valueì˜ ê°’ì„ ì¤„ì´ê³ , smallestí•œ eigen valueì˜ ê°’ì„ í‚¤ìš°ë©´ ë©ë‹ˆë‹¤. 

ë§Œì•½ì— , Aì˜ ê°’ë“¤ì´ ëª¨ë‘ ì–‘ìˆ˜ì´ê³ , $sum(P_i \cdot P_i^T)$ê°€ ì–‘ìˆ˜ë¼ë©´ sum($A$)ë¥¼ largest eigen valueì˜ upper boundì™€ ë¹„ë¡€í•œë‹¤ê³  ë†“ì„ ìˆ˜ ìˆë‹¤. ê·¸ë˜ì„œ ìœ„ sumì„ ì¤„ì´ëŠ”ê²ƒì´, flatten embeddingì„ í•˜ë©´ì„œ negative pairë¼ë¦¬ì˜ ì„ë² ë”©ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

SimCSE ë…¼ë¬¸ì˜ ì•„ì´ë””ì–´ë¥¼ ì¸ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.
https://arxiv.org/abs/2104.08821

[SimCSE ë…¼ë¬¸ë¦¬ë·°](https://github.com/Blackeyes0u0/Blackeyes0u0-paper-review/blob/master/papers/Language/simCSE/simcse.md)

---

## Flatten different Embeddings

í•˜ì§€ë§Œ ë‚˜ëŠ” ê·¸ë ‡ê²Œ ì¡°ê±´ì„ ì¤„ ìˆ˜ ì—†ê¸°ì—, ë‹¤ë¥¸ ë°©ì‹ì„ ìƒê°í•´ì•¼ í–ˆìŠµë‹ˆë‹¤. ì´ìœ , ë‹¤ë¥¸ ì„ë² ë”©ë¼ë¦¬ì˜ í‘œí˜„ì´ê¸° ë•Œë¬¸ì—..

ê·¸ë˜ì„œ ìœ„ì²˜ëŸ¼ negative pair lossì™€ Uniformë¥¼ í•˜ë‚˜ì˜ ì‹ìœ¼ë¡œ ë³´ì§€ì•Šê³ , ë”°ë¡œ ë³¼ ìƒê°ì…ë‹ˆë‹¤.
ì´ì œ Iì™€ Tì— ëŒ€í•´ì„œ ìƒê°í•´ ë´…ì‹œë‹¤.

### negative pair loss

$$
\frac{1}{N^2}\sum_{i \in \chi }^N \sum_{j  \in \Chi}^N I_i \cdot T_j^T 
$$

$$
= \frac{1}{N^2}\sum_{i \in \chi}^N \sum_{j  \in \Chi}^N (\mu^{(Image)} +\sigma_i^{(Image)} ) \cdot (\mu^{(Text)} +\sigma_j^{(Text)} )^T
$$

$$
=\mu^{(Image)} \mu^{(Text)T} + \frac{1}{N^2}\sum_{i \in \chi}^N \sum_{j  \in \Chi}^N \sigma_i \cdot \sigma_j^T
$$

ì§ê´€ì ì¸ ì˜ë¯¸ë¥¼ ë³´ìë©´, ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ì˜ í‰ê·  ê°’ì„ ì¤„ì´ê³ , ê° ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ì„ë² ë”©ì˜ ì„œë¡œ ë‹¤ë¥¸ ë¶„ì‚° ì„ë² ë”©ì„ ì¤„ì´ëŠ” ê²ƒì…ë‹ˆë‹¤. ë¨¼ì € ì´ê±¸ë¡œ, negative pairë¼ë¦¬ì˜ dot productê°’ì„ ì¤„ì—¬, cosine similarityë¥¼ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 


<!-- 
<br>

#### Preprocess & Tokenize
$I_{}^{\prime(i)} = Preprocesser(I^{(i)})$ : CLIP  Preprocess (parameter freeze $\Phi_0$)

$T_{}^{\prime(i)} = Tokenizer(T^{(i)})$ : CLIP Tokenizer (parameter freeze $\Phi_0$)

$Y_{}^{\prime(i)} = Tokenizer(Y^{(i)})$ : CLIP Tokenizer (parameter freeze $\Phi_0$)


<br>

#### Embeddings 
$E_I(I_{}^{\prime(i)};\theta_I)$ : CLIP Image Encoder + LoRA  (learnable params : $\theta_I$)
$E_T(T_{}^{\prime(i)};\theta_T)$ : CLIP Text Encoder + LoRA (learnable params : $\theta_T$)

$E_T(Y_{}^{\prime(i)};\theta_T)$ : CLIP Text Encoder + LoRA (learnable params : $\theta_T$)

#### Concat Image embeddings and Text embeddings

$X^{(i)} = f^{}(E_I(I_{}^{\prime(i)}),E_T(T_{}^{\prime(i)});\Psi)$

$f(\cdot)$ : Image + Text concat models (learnable params : $\Psi$)

<br>

#### Kernel method 

$\psi(X^{(i)})^T \cdot \phi(E_T(Y_{}^{\prime(i)}))$

<br>
miminize KL-divergence
-->







---

### Installation 

```bash
pip install transformers
pip install peft
pip install loralib
pip install wandb
```

#### file

<!-- ğŸ¤— Transformers is tested on Python 3.6+, PyTorch 1.1.0+, TensorFlow 2.0+, and Flax. Follow the installation instructions below for the deep learning library you are using:

PyTorch installation : [pytorch](https://pytorch.org/get-started/locally/)

TensorFlow 2.0 installation instructions : [tensorflow](https://www.tensorflow.org/install/pip)
Flax installation instructions.[Flax](https://flax.readthedocs.io/en/latest/)


```
python -m venv .env
```
Activate the virtual environment. On Linux and MacOs:

```
source .env/bin/activate
```
Activate Virtual environment on Windows
```
.env/Scripts/activate
```
Now youâ€™re ready to install ğŸ¤— Transformers with the following command:
```
pip install transformers
```
For CPU-support only, you can conveniently install ğŸ¤— Transformers and a deep learning library in one line. For example, install ğŸ¤— Transformers and PyTorch with: -->

```
/VectorUniverse Project X
â”œâ”€â”€ README.md
â”œâ”€â”€ Data
|  â”œâ”€â”€ VQA
|  â””â”€â”€ Youtube_thumbnails
|       â”œâ”€â”€ images
|       â””â”€â”€ metadata.csv
|
â”œâ”€â”€ node_modules
|  â”œâ”€â”€ bluebird
|  â”œâ”€â”€ chalk
|  â”œâ”€â”€ cli-spinner
|  â”œâ”€â”€ meow
|  â””â”€â”€ object-assign
â”œâ”€â”€ package.json
â””â”€â”€ tree.js
```
